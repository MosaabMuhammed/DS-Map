<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        Unit Testing
    </title>

    <link rel="stylesheet" href="../prism.css">
</head>

<body>
    <div style="width:1000px;margin:auto;">

        <h1 id="1startercode">
            Unit Testing
        </h1>
        <details>
            <summary><b>Commands</b></summary>
            <pre class="language-python"><code># run pytest
pytest
pytest test_file.py
pytest test/

# To see which files are collected to perform a test before applying the test.
pytest --collect-only

# Run the tests but stop on the first failure.
pytest -x

# Disable capture of stdout/stderr.
pytest -s

# Run tests whose names contain a keyword.
pytest -k

# Show locals in tracebacks.
pytest -l

# Give me more reporting on failures.
pytest -rf
# then you can copy the command to test a specific test,
# and run it until it passes.

# Control traceback generation.
pytest --tb/full-trace

# Invoke the python debugger for each failure
pytest test_file --pdb

# Debug first failure, then stop running tests
pytest test_file -x --pdb
</code></pre>
        </details>
        <details>
            <summary><b>Configuration</b></summary>
            <p>You can create a pytest.ini file:</p>
            <pre class="language-python"><code># Content of pytest.ini

[pytest]
adopts = -x -l # will be used in all test runs.
</code></pre>
        </details>
        <details>
            <summary><b>Test simple function</b></summary>
            <h3>Main Code</h3>
            <pre class="language-python"><code>class Calculator:
"""A terrible calculator."""

def add(self, a, b):
    """Add two numbers."""
    return a + b</code></pre>
            <h3>Test Function</h3>
            <pre class="language-python"><code>from calculator import Calculator

def test_add():
    calculator = Calculator()

    result = calculator.add(2, 3)

    assert result == 5</code></pre>

        </details>
        <details>
            <summary><b>Test Exception</b></summary>
            <h3>Main Function</h3>
            <pre class="language-python"><code>class CalculatorError(Exception):
    """For calculator errors"""

    class Calculator:
        """A terrible calculator."""

        def divide(self, a, b):
            """Divide two numbers."""
            try:
                return a / b
            except ZeroDivisionError as ex:
                raise CalculatorError("You can't divide by zero.") from ex</code></pre>
            <h3>Test File</h3>
            <pre class="language-python"><code>def test_divide_by_zero():
    calculator = Calculator()

    with pytest.raises(CalculatorError):
        calculator.divide(9, 0)</code></pre>
        </details>

        <details>
            <summary><b>Test Custom Expection</b></summary>
            <pre class="language-python"><code>import numbers

class CalculatorError(Exception):
    """For calculator errors"""

class Calculator:
    """A terrible calculator."""

    def add(self, a, b):
        """Add two numbers."""
        self._check_operand(a)
        self._check_operand(b)
        return a + b

    def _check_operand(self, operand):
        """Check that the operand is a number."""
        if not isinstance(operand, numbers.Number):
            raise CalculatorError(f'"{operand}" is not a number.')
</code></pre>
            <pre class="language-python"><code>import pytest
from calculator import Calculator, CalculatorError

def test_add_weird_stuff():
    calculator = Calculator()

    with pytest.raises(CalculatorError):
        result = calculator.add("two", 3)</code></pre>
        </details>
        <details>
            <summary><b>Mark some tests as a group [pytest.mark]</b></summary>
            <pre class="language-python"><code>import pytest
@pytest.mark.codec_x
def test_codec_x():
    pass

@pytest.mark.codec_y
def test_codec_y():
    pass

# You run it like the following:
pytest -m codec_x
pytest -m "not codec_x"
pytest -m "codec_x or codec_y"
</code></pre>
        </details>
        <details>
            <summary><b>Use Multiple Inputs for a single test function [paramatrize]</b></summary>
            <pre class="language-python"><code>import pytest

@pytest.mark.parametrize(
    "a, b, expected",
    [
        (1, 1, 2),
        (2, 1, 3),
        (3, 1, 3),
    ],
)

def  test_add_with_params(a, b, expected):
    assert calculator.add(a, b, expected)</code></pre>
        </details>
        <details>
            <summary><b>Grouping your tests inside a class</b></summary>
            <pre class="language-python"><code>class TestAdd:
    def test_add(self):
        assert calculator.add(1, 2) == 3

    def test_error(self):
        with pytest.raises(CalculatorError):
            calculator.add("two", 1)
            </code></pre>
        </details>
        <details>
            <summary><b>Skippping or "xfailing" tests</b></summary>
            <p><b>Skip a test if:</b>
                <ul>
                    <li>it can't run at all on a certain platform.</li>
                    <li>it can't run because a dependency is missing.</li>
                </ul>
                <b>"xfail" a test if:</b>
                <ul>
                    <li>The implementation is currently lacking.</li>
                    <li>It fails on a certain platform but should work.</li>
                </ul>
            </p>
            <pre class="language-python"><code># Imperative Skipping from inside test (or fixture) function:
pytest.skip("reason")
pytest.xfail("reason")

# Declarative versions on a test function/class:
skipif = pytest.mark.skipif
xfail = pytest.mark.xfail

@skipif(sys.platform == "win32",
        reason="POSIX only")
def test_posix_property(...):
    pass

@xfail(reason="not implemented")
class TestNewAPI:
    # test methods.
    pass
</code></pre>
        </details>
        <details>
            <summary><b>Fixtures</b></summary>
            <ul>
                <li>
                    <details>
                        <summary><b>Custom Fixture</b></summary>
                        <p>Fixtures used to be created to mimic a behaviour, so your tests won't depend on the real
                            behaviour to save time.</p>
                        <p>Create a file called conftest.py to write your fixtures, and your fixtures will be available for all tests in the folder.</p>
                        <pre class="language-python"><code># conftest.py
import pytest

@pytest.fixture
def my_fixture():
    return 42
</code></pre>
                        <pre class="language-python"><code># tests_file.py
def test_add(my_fixture):
    assert my_fixture == 42</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Default Fixtures [capsys]</b></summary>
                        <pre class="language-python"><code>def test_capsys(capsys):
    print("hello")
    out, err = capsys.readouterr()
    assert "hello\n" == out
                </code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Default Fixtures [MonkeyPatch]</b></summary>
                        <a href="https://docs.pytest.org/en/7.1.x/how-to/monkeypatch.html">docs</a>
                        <pre class="language-python"><code>def test_monkeypatch(monkeypatch):
    def fake_add(a, b):
        return 42

    monkeypatch.setattr(demo, "add", fake_add)
    assert demo.add(2, 3) == 42</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Default Fixtures [tmpdir]</b></summary>
                        <pre class="language-python"><code>def test_tmpdir(tmpdir):
    some_file = tmpdir.join("something.txt")
    some_file.write('{"hello": "world"}')

    result = demo.read_json(str(some_file))
    assert result["hello"] == "world"</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Autouse Fixtures (fixtures you don’t have to request)</b></summary>
                        <p>Sometimes you may want to have a fixture (or even several) that you know all your tests will depend on. “Autouse” fixtures are a convenient way to make all tests automatically request them. This can cut out a lot of redundant requests, and can even provide more advanced fixture usage (more on that further down).
                        </p>
                        <pre class="language-python"><code># contents of test_append.py
import pytest


@pytest.fixture
def first_entry():
    return "a"


@pytest.fixture
def order(first_entry):
    return []


@pytest.fixture(autouse=True)
def append_first(order, first_entry):
    return order.append(first_entry)


def test_string_only(order, first_entry):
    assert order == [first_entry]


def test_string_and_int(order, first_entry):
    order.append(2)
    assert order == [first_entry, 2]</code></pre>
                    </details>
                </li>
                <li><details>
                    <summary><b>Paramatrize a fixture</b></summary>
                    <pre class="language-python"><code># Class Developer

@pytest.fixture
def language():
    return 'python'

@pytest.fixture
def developer():
    return Developer(language)

# Old test
def test_brag(developer):
    assert developer.brag() == "python is the best!"

# paramtrizing a fixture
@pytest.mark.parametrize('language', ['ruby'])
def test_brag_works_for_ruby(developer):
    assert developer.brag() == "ruby is the best!"
                    </code></pre>
                </details></li>
            </ul>
        </details>
        <details>
            <summary><b>Mock</b></summary>
            <p>used to replicated third-party services and make the test independent of them.</p>
            <pre class="language-python"><code># forecaster.py

class Forecaster:
    def __init__(self, weather_service):
        self.weather_service = weather_service
    
    def forecast(self):
        reading = self.weather_service.barometer()
        forecasts = dict(
            rising="Going to rain",
            falling="Looks clear"
        )
        return forecasts[reading]
            </code></pre>
            <pre class="language-python"><code># test_forecaster.py
import pytest
from mock import Mock
from forecaster import Forecaster, WeatherService

@pytest.fixture
def mock_ws():
    return Mock(spec=WeatherService)

def test_rain_when_barometer_rising(mock_ws):
    forecaster = Forecaster(mock_ws)
    mock_ws.barometer.return_value == "rising"
    assert forecaster.forecast() == "Going to rain"</code></pre>
        </details>
    </div>
    <script src="../prism.js"></script>
</body>

</html>