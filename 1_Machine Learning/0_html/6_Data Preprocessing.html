<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Data Preprocessing</h1>
<div style='width:1000px;margin:auto'>

<details><summary> <b>Functions Data Processing</b> </summary>
<p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/5_Cargo%20Rican%20HouseHold/2_Featuretools%20for%20Good.html"><b>Notebook</b></a> </p>

<ol>
<li>Remove any <b>duplicated</b> columns.</li>

<li>Replace <b>infinite</b> values with <b>np.nan</b></li>

<li>Remove columns with a <b>missing percentage </b>above the <em>missing_threshold</em></li>

<li>Remove columns with only a <b>single unique value.</b></li>

<li>Remove one out of every pair of columns with a <b>correlation</b> threshold above the correlation_threshold</li>

<li>Extract the <b>training</b> and <b>testing</b> data along with <b>labels</b> and <b>ids</b> (needed for making submissions)</li>
</ol>


<pre><code class="python">def post_process(feature_matrix, missing_threshold=.95, correlation_threshold=.95):
    # Remove duplicated features.
    start_features  = feature_matrix.shape[1]
    feature_matrix = feature_matrix.loc[:, ~feature_matrix.columns.duplicated()]
    n_duplicated   = start_features - feature_matrix.shape[1]
    print(f'~&gt; There were {bg(n_duplicated)} duplicated features.')

    feature_matrix = feature_matrix.replace({np.inf: np.nan, -np.inf: np.nan}).reset_index()

    # Extract the Id and Target Columns.
    ids    = list(feature_matrix.pop('idhogar'))
    labels = list(feature_matrix.pop('Target'))

    # Drop any columns drived from column 'Target'
    drop_cols = []
    for col in feature_matrix:
        if col == 'Target':
            pass
        elif 'Target' in col:
            drop_cols.append(col)

    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]

    # One-Hot Encoding (if necessery)
    feature_matrix  = pd.get_dummies(feature_matrix, drop_first=True)
    n_start_feature = feature_matrix.shape[1]
    print(f'~&gt; Original Shape: {bg(n_start_feature)}.')


    # Find the missing values
    missing             = pd.DataFrame(feature_matrix.isnull().sum())
    missing['fraction'] = missing[0] / feature_matrix.shape[0]
    missing.sort_values('fraction', ascending=False, inplace=True)

    # Missing Above threshold
    missing_cols   = list(missing[missing['fraction'] &gt; missing_threshold].index)
    n_missing_cols = len(missing_cols)

    # Remove missing columns
    feature_matrix = feature_matrix[[col for col in feature_matrix if col not in missing_cols]]
    print(f'~&gt; There were {bg(n_missing_cols)} missing columns above threshold = {colored(missing_threshold, &quot;green&quot;)}.')

    unique_counts        = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending=False)
    zero_variance_cols   = list(unique_counts[unique_counts[0] == 1].index)
    n_zero_variance_cols = len(zero_variance_cols)

    # Remove zero variance columns
    feature_matrix = feature_matrix[[col for col in feature_matrix if col not in zero_variance_cols]]
    print(f'~&gt; There were {bg(n_zero_variance_cols)} columns with zero variance.')

    # Correlations.
    corr_matrix = feature_matrix.corr().abs()
    upper       = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
    to_drop     = [column for column in upper.columns if any(upper[column] &gt; correlation_threshold)]

    n_collinear = len(to_drop)

    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]
    print(f'~&gt; There were {bg(n_collinear)} highly correlated columns above threshold = {colored(correlation_threshold, &quot;green&quot;)}.')

    # Total number of columns removed.
    total_removed = n_collinear + n_duplicated + n_missing_cols + n_zero_variance_cols
    print(f'~&gt; Total Number columns removed = {bg(total_removed)}.')
    print(f'~&gt; Shape after removing: {bg(feature_matrix.shape, &quot;s&quot;)}.')

    # Add the ids and target
    feature_matrix['idhogar'] = ids
    feature_matrix['Target']  = labels

    # Extract traing and test datasets.
    train = feature_matrix[feature_matrix.Target.notnull()]
    test  = feature_matrix[feature_matrix.Target.isnull()]

    # Subset to houses with a head of household
    train = train[train['idhogar'].isin(list(train_valid['idhogar']))]
    test = test[test['idhogar'].isin(list(test_valid['idhogar']))]

    train_labels = np.array(train.pop('Target')).reshape((-1,))
    test_ids     = list(test.pop('idhogar'))

    train, test = train.align(test, join='inner', axis=1)

    assert (len(train_labels) == train.shape[0]), 'Labels must be the same length as number of training.'
    assert (len(test_ids) == test.shape[0]), 'Test ids must equal the number of test observations.'

    return train, train_labels, test, test_ids
</code></pre>


</p>
</details>

<hr>

<details><summary> <b>Train Test Split (Stratified)</b> [Categorical]</summary>
<p style="margin: 0">
<p>1) <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/10_%20K-Nearest%20Neighbors/1_step-by-step-diabetes-classification-knn-detailed.html#Test-Train-Split-and-Cross-Validation-methods">Explanation for <b>Train<em>Test</em>Split</b></a> <br>
2) <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/10_%20K-Nearest%20Neighbors/2_KNN%20-%20Full%20Pipeline.html#Train-Test-Split">Train Test Split notebook</a></p>

<pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    df_feat, y, test_size=0.4, stratify=y, random_state=42)
</code></pre>


<p>Another way to split, by hashing the unique identifier of each row, to make sure that at the next run, the training and test sets will be the same</p>

<pre><code>from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids         = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

housing_with_id = housing.reset_index()    # Since housing doesn't have identifier.
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;)

print(len(train_set)/housing.shape[0], len(test_set)/housing.shape[0])
</code></pre>


</p></details>

<details><summary> <b>Train Test Split (Stratified)</b> [Regression]</summary><p>
<h5>Descritize the target column into n bins.</h5>

<pre><code>housing['income_cat'] = pd.cut(housing['median_income'],
                               bins=[0., 1.5, 3., 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])
</code></pre>


<h5>Split based on the new categorical binned column</h5>

<pre><code>from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing['income_cat']):
    strat_train_set = housing.loc[train_index]
    strat_test_set  = housing.loc[test_index]
</code></pre>


<h5>Then, remove the binned column.</h5>

<pre><code># Remove the &quot;income_cat&quot;.
for set_ in (strat_train_set , strat_test_set):
    set_.drop(&quot;income_cat&quot;, axis=1, inplace=True)
</code></pre>

</p></details>

<details><summary> <b>Temporal Splitting (Time Based  Splitting)</b> </summary>
<p>

<pre><code class="python">split_train = int(len(data) * .8)
split_test  = int(len(data) * .2)

X_train = data[:split_train]
y_train = data[:split_train]['Target']

X_test = data[-split_test:]
y_test = data[-split_test:]['Target']
</code></pre>

</p>
</details>

<details><summary> <b>Make Scorer</b> </summary>
<p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Sklearn/sklearn.metrics.make_scorer.html#sklearn-metrics-make-scorer"><b>Sklearn Docs</b></a> </p>

<pre><code class="python">from sklearn.metrics import f1_score, make_scorer

scorer = make_scorer(f1_score, greater_is_better=True, average='macro')

</code></pre>

</p>
</details>

<details><summary> Create a <b>Pipeline</b> </summary>
<p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/5_Cargo%20Rican%20HouseHold/1_Costa%20Rican%20Household%20Poverty%20Level%20Prediction.html"><b>Notebook</b></a> </p>

<pre><code class="python">from sklearn.preprocessing import Imputer, MinMaxScaler
from sklearn.pipeline import Pipeline

pipeline = Pipeline([('imputer', Imputer(strategy='meadian')),
                     ('scaler', MinMaxScaler())])

# Fit and transform the training data
train_set = pipeline.fit_transform(train_set)
test_st   = pipeline.transform(test_set)
</code></pre>

</p>
</details>

<details><summary> <b>K-Fold CV</b> </summary><p>

<pre><code># Using Custom Stratified K-folds
def Stratified_kfolds(alg, X, y):
    score_valid = 0
    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=33)

    for train_idx, valid_idx in skf.split(X, y):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        # One-Hot Encoding
        ohe_enc_keyword  = CountVectorizer()
#         ohe_enc_location = CountVectorizer()
        ohe_enc_text     = CountVectorizer(max_df=.9, min_df=3)

        # Transform Train data
        keyword_train    = ohe_enc_keyword.fit_transform(X_train['keyword'])
#         location_train   = ohe_enc_location.fit_transform(X_train['location'])
        text_train       = ohe_enc_text.fit_transform(X_train['text'])

        # Transform Validation data.
        keyword_valid    = ohe_enc_keyword.transform(X_valid['keyword'])
#         location_valid   = ohe_enc_location.transform(X_valid['location'])
        text_valid       = ohe_enc_text.transform(X_valid['text'])

        ## Merge Training data.
        X_train = hstack((keyword_train, text_train)).tocsr()

        ## Merge Validation data
        X_valid = hstack((keyword_valid, text_valid)).tocsr()

        # ML Models.
        alg.fit(X_train.todense(), y_train)
        y_pred_valid = alg.predict(X_valid.todense())
        score_valid  += f1_score(y_valid, y_pred_valid)

    return score_valid/skf.n_splits
</code></pre>


</p></details>

<details><summary> <b>Cross_val_score</b> </summary><p>

<pre><code>model = RF(n_estimators=100, n_jobs=-1)

# 10 Fold cross validation
cv_results = cross_val_score(model, train_set, train_labels, cv=10, scoring=scorer)

print(f'~&gt; 10 Fold Cross Validation F1 Score = {bg(round(cv_results.mean(), 4), &quot;s&quot;)} with std = {bg(round(cv_results.std(), 4), &quot;s&quot;)}')
</code></pre>

</p></details>

<details><summary> <b>cross_val_predict</b> </summary><p>

<pre><code># Returns the predictions 
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, n_jobs=-1, verbose=1, method=&quot;predict&quot;)

# Predict Proba
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, n_jobs=-1, verbose=1, method=&quot;predict_proba&quot;)

# Predict decision function
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, n_jobs=-1, verbose=1, method=&quot;decision_function&quot;)
</code></pre>

</p></details>

<details><summary><b>Cross Validation</b></summary>
<p style="margin: 0">
<p>1) <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/10_%20K-Nearest%20Neighbors/1_step-by-step-diabetes-classification-knn-detailed.html#Test-Train-Split-and-Cross-Validation-methods">Explanation for <b>Cross Validation</b></a> </p>
</p>
</details>

<details><summary> <b>GridSearchCV</b> </summary>
<p style="margin: 0">
<p>1) <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/10_%20K-Nearest%20Neighbors/1_step-by-step-diabetes-classification-knn-detailed.html#Hyper-Parameter-optimization">Explanation of <b>GridSearch</b>.</a> <br>
2) <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/10_%20K-Nearest%20Neighbors/2_KNN%20-%20Full%20Pipeline.html#Hyperparameter-Tunning-&amp;-Cross-Validation">Hyperparameter Tunning &amp; Cross validation</a> </p>

<pre><code># Load the model
from sklearn.model_selection import GridSearchCV

# Select the range of parameters
param_grid = {'n_neighbors': np.arange(1, 50)}

# Activate the GridSearchCV
knn = KNeighborsClassifier()
knn_cv = GridSearchCV(knn, param_grid, cv=5)
knn_cv.fit(X, y)

# Calculate the Training score
print('The Best Score for training =', knn_cv.best_score_*100,'%')

# See the Best K value
print('The Best parameters (K) =', knn_cv.best_params_['n_neighbors'])

# Show all resutls.
resutls = pd.DataFrame(knn_cv.cv_results_)[['params', 'mean_test_score', rank_test_score']]

# Sort by test score
results.sort_values('rank_test_score')

# Get the best model.
knn_cv.best_estimator_
</code></pre>

</p>
</details>


<details><summary> <b>Custom Transformer</b> </summary><p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin

rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self # nothing else to do
    def transform(self, X, y=None):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household,
                       population_per_household,
                       bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]


attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)
</code></pre>

</p></details> 

<hr>

<details><summary><b style='font-size:20px'>Bayesian Optimization</b> </summary><p><ul>

<details><summary> <b>LinearSVC</b> </summary><p>

<pre><code>def svc_cv(C, data, targets):
    alg   = LinearSVC(C=C, random_state=33, penalty='l2')
    score = Stratified_kfolds(alg, data, targets)
    return score

def optimize_svc(data, targets):
    def svc_crossval(expC):
        C = 10 ** expC
        return svc_cv(C=C, data=data, targets=targets)

    optimizer = BayesianOptimization(
        f=svc_crossval,
        pbounds={'expC': (-6, 5)},
        random_state=33,
        verbose=2,

    )
    optimizer.maximize(n_iter=30, init_points=5)

    print(f&quot;~&gt; Final Result: {optimizer.max}&quot;)

# Optimize
optimize_svc(X, y)
</code></pre>

</p></details>

<details><summary> <b>RidgeClassifier</b> </summary><p>

<pre><code># Define Ridge CV
def ridge_cv(alpha, data, targets):
    alg = RidgeClassifier(alpha=alpha, random_state=33)
    score = Stratified_kfolds(alg, data, targets)
    return score

# Optimization Strategy
def optimize_ridge(data, targets):
    def ridge_crossval(expAlpha):
        alpha = 10 ** expAlpha
        return ridge_cv(alpha=alpha, data=data, targets=targets)

    optimizer = BayesianOptimization(
        f=ridge_crossval,
        pbounds={'expAlpha': (-7, 5)},
        random_state=33,
        verbose=2
    )
    optimizer.maximize(n_iter=20, init_points=5)

    print(f&quot;~&gt; Best parameters: {optimizer.max}&quot;)

# Run the optimization
optimize_ridge(X, y)
</code></pre>

</p></details>

<details><summary> <b>Multi-nomial Naive Bayes</b> </summary><p>

<pre><code># CV Strategy
def nb_cv(alpha, data, targets):
    alg = MultinomialNB(alpha=alpha)
    return Stratified_kfolds(alg, data, targets)

def nb_crossval(expAlpha):
    alpha = 10**expAlpha
    return nb_cv(alpha=alpha, data=X, targets=y)

optimizer = BayesianOptimization(
    f=nb_crossval,
    pbounds={'expAlpha': (-6, 5)},
    random_state=33,
    verbose=2
)

# Optimize
optimizer.maximize(
    n_iter=30,
    init_points=5
)

print(f&quot;~&gt; Best Result: {optimizer.max}&quot;)
</code></pre>

</p></details>

<details><summary> <b>Gaussian Naive Bayes</b> </summary><p>

<pre><code># CV Strategy
def nb2_cv(alpha, data, targets):
    alg = GaussianNB(var_smoothing=alpha)
    return Stratified_kfolds(alg, data, targets)

def nb2_crossval(expAlpha):
    alpha = 10**expAlpha
    return nb2_cv(alpha=alpha, data=X, targets=y)

optimizer = BayesianOptimization(
    f=nb2_crossval,
    pbounds={'expAlpha': (-9, 5)},
    random_state=33,
    verbose=2
)
# Optimize
optimizer.maximize(
    n_iter=30,
    init_points=5
)

print(f&quot;~&gt; Best Result: {optimizer.max}&quot;)
</code></pre>

</p></details>

<details><summary> <b>XGBoostClassifier</b> </summary><p>

<pre><code># Importing
from sklearn.model_selection import cross_val_score
from bayes_opt import BayesianOptimization

def xgboost_cv(max_depth,
               learning_rate,
               n_estimators,
               gamma,
               min_child_weight,
               max_delta_step,
               subsample,
               colsample_bytree,
               silent=True,
               nthread=-1):
    return cross_val_score(xgb.XGBClassifier(max_depth=int(max_depth),
                                             learning_rate=learning_rate,
                                             n_estimators=int(n_estimators),
                                             silent=silent,
                                             nthread=nthread,
                                             gamma=gamma,
                                             min_child_weight=min_child_weight,
                                             max_delta_step=max_delta_step,
                                             subsample=subsample,
                                             colsample_bytree=colsample_bytree),
                          X_train,
                          y_train,
                          scoring=&quot;f1&quot;,
                          cv=3).mean()

# Define Bayesian Optimization
optimizer = BayesianOptimization(xgboost_cv,
                                 {'max_depth': (5, 10),
                                  'learning_rate': (0.01, 0.3),
                                  'n_estimators': (50, 1000),
                                  'gamma': (1., 0.01),
                                  'min_child_weight': (2, 10),
                                  'max_delta_step': (0, 0.1),
                                  'subsample': (0.7, 0.8),
                                  'colsample_bytree' :(0.5, 0.99)
                                  })
# Run the optimization
optimizer.maximize(n_iter=20,
                   init_points=5)

print(optimizer.max)

# Check the score of the tunned model
params                 = optimizer.max['params']
params['max_depth']    = int(params['max_depth'])
params['n_estimators'] = int(params['n_estimators'])

alg = xgb.XGBClassifier(**params)
alg.fit(X_train, y_train)

y_pred_train = alg.predict(X_train)
y_pred_valid = alg.predict(X_valid)

print(f&quot;F1_score on train data: {bg(f1_score(y_train, y_pred_train))}&quot;)
print(f&quot;F1_score on Valid data: {bg(f1_score(y_valid, y_pred_valid))}&quot;)
</code></pre>

</p></details>

<details><summary> <b>Stacking</b> </summary><p>

<pre><code>from mlxtend.classifier import StackingClassifier

def stacking1_cv(C, data, targets):
    lr    = LogisticRegression(C=C, penalty='l2', n_jobs=-1)
    sclf  = StackingClassifier(classifiers=[alg1, alg2, alg3], meta_classifier=lr)
    return  Stratified_kfolds(sclf, data, targets)

def stacking1_crossval(expC):
    C = 10 ** expC
    return stacking1_cv(C=C, data=X, targets=y)

optimizer = BayesianOptimization(
    f=stacking1_crossval,
    pbounds={'expC': (-9, 5)},
    random_state=33,
    verbose=2,

)
optimizer.maximize(n_iter=10, init_points=10)

print(f&quot;~&gt; Final Result: {optimizer.max}&quot;)
</code></pre>

</p></details>

<details><summary> <b>LightGBM</b> </summary><p>
<ul>
<li><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Bayesian%20Optimization%20From%20Scratch/0_html/2_Bayesian%20Optimization%20for%20LightGBM.html">Using<b> Bayesian Optimization</b> library</a></li>

<li><a href="./6_data_processing/hyperopt_for_lightgbm_with_f1_score.html">Using<b> hyperopt</b> library</a></li>
</ul>
</p></details>

<details><summary> <b>Catboost</b> </summary><p>
<ul>

<li><a href="./6_data_processing/hyperopt_for_catboost.html">Using<b> hyperopt</b> library</a></li>
</ul>
</p></details>

<li><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Bayesian%20Optimization%20From%20Scratch/0_html/1_Bayesian%20Optimization.html">For <b>Sklearn Models</b></a></li>
</ul></p></details>

<details><summary> <b style='font-size:20px'>Randomized Search</b></summary><p>
<h4>NOTE:</h4>
<p><b>Reciprocal Distribution</b> is useful when you have no idea what the scale of the hyperparameter should be.</p>
<p><b>Exponential Distribution</b> is best when you know (more or less) what the scale of the hyperparameter should be.</p>
see more distributions from <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">here</a><br>

<ul><details><summary> <b>MultiNomial</b> </summary><p>

<pre><code># Import Libraries
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from scipy.stats import expon, reciprocal, geom, uniform

# Make the pipeline
pipe = make_pipeline(CounterVectorizer(), MultinomialNB())

# Cross-validate the pipeline using default parameters.
from sklearn.model_selection import cross_val_score
cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()

# Specifiy parameter values to search (use a distribution for any continous parameters)
import scipy as sp
params = {}
params['countvectorizer__min_df'] = [1, 2, 3, 4]
params['countvectorizer__lowercase'] = [True, False]
params['multinomialnb__alpha'] = sp.stats.uniform(scale=1)

# try &quot;n_iter&quot; random combinations of those parameter values.
from sklearn.model_selection import RandomizedSearchCV
rand = RandomizedSearchCV(pipe, params, n_iter=10, cv=5, scoring='accuracy', random_state=1)
rand.fit(X, y)

# What was the best score found during the search?
rand.best_score_

# Which combination of parameters produced the best score.
rand.best_params_
</code></pre>

</p></details>

<details><summary> <b>SVR</b> </summary><p>

<pre><code>from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import expon, reciprocal

# see https://docs.scipy.org/doc/scipy/reference/stats.html
# for `expon()` and `reciprocal()` documentation and more probability distribution functions.

# Note: gamma is ignored when kernel is &quot;linear&quot;
param_distribs = {
        'kernel': ['linear', 'rbf'],
        'C': reciprocal(20, 200000),
        'gamma': expon(scale=1.0),
    }

svm_reg = SVR()
rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,
                                n_iter=50, cv=5, scoring='neg_mean_squared_error',
                                verbose=2, random_state=42)
rnd_search.fit(housing_prepared, housing_labels)
</code></pre>

</p></details>

</ul></p></details>


</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>