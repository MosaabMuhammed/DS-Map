<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 style="color:darkcyan;text-decoration:underline">
   2. Data Problems
  </h1>
  <div style="width:1000px;margin:auto">
   <p>
    <a href="./1_starter/a-data-science-framework-to-achieve-99-accuracy.html#3.21-The-4-C's-of-Data-Cleaning:-Correcting,-Completing,-Creating,-and-Converting">
     Explanation of the 4'C of
     <b>
      Data Cleaning
     </b>
     .
    </a>
   </p>
   <details>
    <summary>
     <span style="color:#333;font-size:25px;font-weight:bold;text-decoration:underline">
      1. Related to Dataset:
     </span>
    </summary>
    <p>
     <details>
      <summary>
       <b>
        1. Imbalanced Data set:
       </b>
      </summary>
      <p>
      </p>
      <ul>
       <li>
        <p>
         <a href="./2_data_problems/1_Dealing%20with%20Imbalanced%20Datasets.html">
          <span style="color:#333333">
           <b>
            1. Imbalanced Data set:
           </b>
          </span>
         </a>
        </p>
       </li>
       <li>
        <p>
         <a href="https://imbalanced-learn.org/en/stable/over_sampling.html">
          <span style="color:#333333">
           <b>
            2. Another way
           </b>
           (better)
          </span>
         </a>
        </p>
       </li>
      </ul>
     </details>
    </p>
   </details>
   <details>
    <summary>
     <b>
      2. Large Dataset:
     </b>
    </summary>
    <p>
     <details>
      <summary>
       See the data before
       <b>
        READING
       </b>
       it
      </summary>
      <p>
       wzxhzdk:0
      </p>
     </details>
     <details>
      <summary>
       Using
       <b>
        Chunksize
       </b>
       in pd.read_csv()
      </summary>
      <p>
       wzxhzdk:1
      </p>
     </details>
     <details>
      <summary>
       From
       <b>
        CSV
       </b>
       To
       <b>
        HDF5
       </b>
      </summary>
      <p>
      </p>
      <ul>
       <li>
        <a href="https://stackoverflow.com/questions/27203161/convert-large-csv-to-hdf5">
         <b>
          After reading csv
         </b>
        </a>
       </li>
       <li>
        <a href="./2_data_problems/from_large_csv_to_small_HDFS.html">
         <b>
          When we can't read the csv
         </b>
        </a>
       </li>
      </ul>
     </details>
    </p>
   </details>
   <details>
    <summary>
     From
     <b>
      CSV
     </b>
     To
     <b>
      npz
     </b>
    </summary>
    <p>
    </p>
    <ul>
     <li>
      <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez_compressed.html#numpy.savez_compressed">
       <b>
        np.savez_compressed
       </b>
      </a>
     </li>
    </ul>
   </details>
   <details>
    <summary>
     From
     <b>
      CSV
     </b>
     To
     <b>
      pyarray
     </b>
    </summary>
    <p>
    </p>
    <ul>
     <li>
      <a href="https://github.com/Far0n/kaggletils/blob/7819a26973a25990f479e7b7f01f40de31a64b34/kaggletils/utils/data.py#L20">
       <b>
        Credits [Github]
       </b>
      </a>
     </li>
    </ul>
    wzxhzdk:2
   </details>
   <details>
    <summary>
     Best Type for
     <b>
      Save &amp; Read [Feather]
     </b>
    </summary>
    <p>
     wzxhzdk:3
    </p>
   </details>
   <details>
    <summary>
     <b>
      Reduced size of dataset
     </b>
    </summary>
    <p>
     wzxhzdk:4
    </p>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      2. Categorical ~&gt; Numerical:
     </b>
    </summary>
    <p>
    </p>
    <ul>
     <li>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.2-Cardinality.html#Cardinality">
       <b>
        Cardinality, How affects Model Performance
       </b>
      </a>
     </li>
     <li>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.3-Rare-Labels.html#Rare-Labels">
       <b>
        Rare Labels
       </b>
      </a>
     </li>
     <li>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.09-Comparison-categorical-encoding-techniques.html#Comparison-of-Categorical-Variable-Encodings">
       <b>
        Comparsion b/w different Encodings
       </b>
      </a>
     </li>
    </ul>
    <details>
     <summary>
      <b>
       Entity Embedding for Cat Features
      </b>
     </summary>
     <ul>
      <li>
       <a href="./2_data_problems/Entity_Embedding.html">
        <b>
         for ONLY Categorical Features
        </b>
       </a>
      </li>
      <li>
       <a href="./2_data_problems/entity_embedding_mixed_df.html">
        <b>
         for Mixed Features
        </b>
       </a>
      </li>
      <li>
       <a href="./2_data_problems/entity_embedding_num_converted_to_cat.html">
        <b>
         Num converted to Cat
        </b>
       </a>
      </li>
     </ul>
    </details>
    <details>
     <summary>
      <b>
       OneHotEncoding [Nominal]
      </b>
     </summary>
     <p>
     </p>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.01-One-hot-encoding.html#One-Hot-Encoding">
       <b>
        Pandas - Sklearn - Feature-Engine
       </b>
      </a>
     </p>
     <details>
      <summary>
       <b>
        Heuristics
       </b>
      </summary>
      <p>
       <details>
        <summary>
         <b>
          Dummy
         </b>
         Variables
        </summary>
        <p style="margin: 0">
        </p>
        <p>
         <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/11_Decision%20Tree,%20Random%20Forest/2_Decision%20Trees%20and%20Random%20Forest%20Project-Mosaab.html#Get-Dummy-Variable">
          Dummy Variable in Action
         </a>
        </p>
        wzxhzdk:5



wzxhzdk:6
        <h4>
         We can make the dummy variables sparse in order to make it fit into memory, then convert it back to form which suitable for models to handle.
        </h4>
        <h4>
         Note: (.sparse.to_coo().tocsr()) is responsible to make it suitable for models to handle.
        </h4>
        wzxhzdk:7
       </details>
      </p>
     </details>
     <details>
      <summary>
       From
       <b>
        One-Hot Encoding
       </b>
       To
       <b>
        Ordinal
       </b>
      </summary>
      <p>
       wzxhzdk:8
      </p>
     </details>
    </details>
   </details>
   <details>
    <summary>
     <b>
      OneHotEncoding for Top Categories
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.02-One-hot-encoding-frequent_categories.html#One-Hot-Encoding-of-Frequent-Categories">
      Using Manual &amp; Feature-Engine
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Binary Encoder
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="https://contrib.scikit-learn.org/categorical-encoding/binary.html">
      <b>
       category_encodors
      </b>
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Label Encoder
     </b>
    </summary>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.03-Integer-Encoding.html#Integer-Encoding">
      Using Manual &amp; Sklearn &amp; Feature-Engine
     </a>
    </p>
    <p style="margin: 0">
    </p>
    <h4>
     1. Convert each object feature to category feature
    </h4>
    wzxhzdk:9


wzxhzdk:10


wzxhzdk:11
    <h4>
     2. A Better Version
    </h4>
    wzxhzdk:12
   </details>
   <details>
    <summary>
     <b>
      Frequency Encoding
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.04_Count_or_frequency_encoding.html#Count-or-frequency-encoding">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
    wzxhzdk:13


wzxhzdk:14
   </details>
   <details>
    <summary>
     <b>
      Ordered Label Encoder
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.05-Ordered-Integer-Encoding.html#Target-guided-encodings">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Thermometer Encoding [Ordinal]
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="https://www.kaggle.com/superant/oh-my-cat">
      <b>
       Credits
      </b>
     </a>
    </p>
    <h4>
     Note:
    </h4>
    <p class="alert alert-info">
     You can encode ordinal data using the thermometer trick. If there are 𝑁 possible values for the variable, then you map each value to a 𝑁-vector, where you put a 1 in the position that matches the value of the variable and all subsequent position.
     <br/>
     <br/>
     For instance: first place ↦(1,1,1), second place ↦(0,1,1), third place ↦(0,0,1).
    </p>
    <h4>
     1. Build the Class.
    </h4>
    wzxhzdk:15
    <h4>
     2. How to use
    </h4>
    wzxhzdk:16



wzxhzdk:17
    <h4>
     3. Another trial: Returns dataframe
    </h4>
    <p>
     <a href="https://stackoverflow.com/questions/49080613/numpy-thermometer-encoding/49081131#49081131">
      <b>
       Credits
      </b>
     </a>
    </p>
    wzxhzdk:18
   </details>
   <details>
    <summary>
     <b>
      Mean/Target Encoding
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.06-Mean-Encoding.html#Target-guided-encodings">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
    <hr/>
    <ul>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/How%20to%20win%20a%20Data%20Science%20Competition/Week%203/mean_encoding_week_3.html#Mean-encodings-without-regularization">
        <b>
         1. Mean Encoding Without Regularization
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/How%20to%20win%20a%20Data%20Science%20Competition/Week%203/mean_encoding_week_3.html#1.-KFold-scheme">
        <b>
         2. Using KFold Scheme
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/How%20to%20win%20a%20Data%20Science%20Competition/Week%203/mean_encoding_week_3.html#2.-Leave-one-out-scheme">
        <b>
         3. Leave-One-Out Scheme
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/How%20to%20win%20a%20Data%20Science%20Competition/Week%203/mean_encoding_week_3.html#3.-Smoothing">
        <b>
         4. With Smoothing
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
      </p>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/How%20to%20win%20a%20Data%20Science%20Competition/Week%203/mean_encoding_week_3.html#4.-Expanding-mean-scheme">
        <b>
         5. Expanding Mean Scheme
        </b>
       </a>
      </p>
     </li>
    </ul>
    <details>
     <summary>
      <b>
       1 Target Encoding (similar to Response Coding)
      </b>
     </summary>
     <p>
     </p>
     <blockquote>
      <p>
       <b>
        NOTE
       </b>
       : Target-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.
      </p>
     </blockquote>
     wzxhzdk:19
     <h4>
      2. Another way of doing so
     </h4>
     wzxhzdk:20
    </details>
    <details>
     <summary>
      <b>
       2 Target Encoding with smoothing
      </b>
     </summary>
     <p>
     </p>
     <p>
      <a href="https://www.kaggle.com/delafields/a-thorough-guide-on-categorical-feature-encoding">
       <b>
        Credits
       </b>
      </a>
     </p>
     wzxhzdk:21



wzxhzdk:22
    </details>
   </details>
   <details>
    <summary>
     <b>
      Probability Ratio Encoding
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.07-Probability-Ratio-Encoding.html#Target-guided-encodings">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Weight of Evidence Ratio Encoding
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.08-Weight-of-Evidence.html#Weight--of-evidence">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Entity Embedding
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="./2_data_problems/Embedding for Cat.html">
      <b>
       Notebook using Keras
      </b>
     </a>
    </p>
   </details>
   <br/>
   <details>
    <summary>
     <b>
      Rare Labels Encoding
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-06-Categorical-Encoding/06.10-Engineering-Rare-Categories.html#Engineering-Rare-Categories">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Feature Hashing
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="https://contrib.scikit-learn.org/categorical-encoding/hashing.html#hashing">
      <b>
       Manual &amp; Feature-Engine
      </b>
     </a>
    </p>
    <h4>
     Using FeatureHasher (Multiple Columns)
    </h4>
    wzxhzdk:23
    <h4>
     Using Hash Function (One Column)
    </h4>
    wzxhzdk:24
   </details>
   <details>
    <summary>
     <b>
      Encoding Cyclic Features (Day, Month, ...)
     </b>
    </summary>
    <p>
     wzxhzdk:25
    </p>
   </details>
   <details>
    <summary>
     <b>
      Encoding
     </b>
     using
     <b>
      ASCII Code
     </b>
     (Ordinal Features)
    </summary>
    <p>
     wzxhzdk:26



wzxhzdk:27
    </p>
   </details>
   <details>
    <summary>
     <b>
      Combination of cat features
     </b>
    </summary>
    <p>
     wzxhzdk:28
    </p>
   </details>
   <details>
    <summary>
     <b>
      Encoding Librariy
     </b>
    </summary>
    <p>
     wzxhzdk:29
    </p>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      3. Outliers
     </b>
    </summary>
    <p>
    </p>
    <p>
     <img alt="" src="imgs/20191106-124543.png"/>
    </p>
    <p>
     <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html">
      <b>
       1. Local Outlier Factor
      </b>
     </a>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.6-Outliers.html#Outliers" style="font-weight:bold">
      2. Detecting Outliers for [Normal, Skewed, Extremely Skewed]
     </a>
    </p>
    <details>
     <summary>
      <b>
       Trimming
      </b>
     </summary>
     <p>
     </p>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-09-Outlier-Engineering/09.01-Outlier-Trimming.html#Outlier-Engineering" style="font-weight:bold">
       Trimming Outliers
      </a>
     </p>
    </details>
    <details>
     <summary>
      <b>
       Clipping
      </b>
     </summary>
     <p>
     </p>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-09-Outlier-Engineering/09.02-Capping-IQR-proximity-rule.html#Outlier-Engineering" style="font-weight:bold">
       Capping IQR Proximity Rule
      </a>
     </p>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.6-Outliers.html#Outliers" style="font-weight:bold">
       Capping Gaussian Approximiation
      </a>
     </p>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-09-Outlier-Engineering/09.04-Capping-Quantiles.html#Outlier-Engineering" style="font-weight:bold">
       Capping Quantiles
      </a>
     </p>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-09-Outlier-Engineering/09.05-Capping-Arbitrary.html#Outlier-Engineering" style="font-weight:bold">
       Capping Aribrary
      </a>
     </p>
    </details>
    <details>
     <summary>
      <b>
       Winsorization
      </b>
     </summary>
     <p>
      wzxhzdk:30
     </p>
    </details>
    <details>
     <summary>
      <b>
       RANSAC Algorithm
      </b>
     </summary>
     <p>
      wzxhzdk:31
     </p>
    </details>
    <details>
     <summary>
      <b>
       z-score
      </b>
     </summary>
     <p>
      The
      <b>
       z-score
      </b>
      of value x is a measure of how many standard deviations x is away from the mean.
      <br/>
      <b>
       z-score
      </b>
      is a normalization technique used in the preprocessing of features. It helps the ML model to learn better from data.
      <br/>
      <br/>
      High
      <b>
       z-score
      </b>
      values in a sample indicate that the sample value is far away from the mean and could be an outlier. Here's how we calculate zscore mathematically: z-score = (x - mean(x)) / std(x)
      <br/>
      <br/>
      Most used in Time-Series problem.


wzxhzdk:32



wzxhzdk:33



wzxhzdk:34
      <img src="./imgs/zscore.png"/>
     </p>
    </details>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      4. Data Scaling:
     </b>
    </summary>
    <p>
     <details>
      <summary>
       <b>
        How Scaling Affects Models Performance
       </b>
      </summary>
      <p>
      </p>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.7-Variable-magnitude.html#Variable-magnitude" style="font-weight:bold">
        Check this
       </a>
      </p>
      <p>
       <img alt="" src="imgs/20191030-134414.png"/>
      </p>
     </details>
    </p>
   </details>
   <br/>
   <details>
    <summary>
     <b>
      1. Normalization
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-10-Feature-Scaling/10.03-MinMaxScaling.html#Scaling-to-Minimum-and-Maximum-values---MinMaxScaling" style="font-weight:bold">
      Pandas - Sklearn
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      2. Standardization
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-10-Feature-Scaling/10.01-Standardisation.html#Feature-Scaling" style="font-weight:bold">
      Check this
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      3. Mean Normalization
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-10-Feature-Scaling/10.02-Mean-normalisation.html#Mean-Normalisation" style="font-weight:bold">
      Pandas - Sklearn
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      4. Max Absolute Scaling
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-10-Feature-Scaling/10.04-Maximum-Absolute-Scaling.html#Scaling-to-maximum-value---MaxAbsScaling" style="font-weight:bold">
      Sklearn
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      5. Robust Scaler
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-10-Feature-Scaling/10.05-Robust-Scaling.html#Scaling-to-quantiles-and-median---RobustScaling" style="font-weight:bold">
      Sklearn
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      6. Scaling to vector unit length / unit norm
     </b>
    </summary>
    <p>
     Read
     <b>
      mastering ml book
     </b>
     page: 12
     <br/>
     <br/>
     Scikit-learn recommends this scaling procedures for text classification or clustering. For example, they quote the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-10-Feature-Scaling/10.06-Scaling-to-unit-length.html#Scaling-to-vector-unit--length-/-unit-norm" style="font-weight:bold">
      Sklearn
     </a>
    </p>
   </details>
   <details>
    <summary>
     <b>
      7. Whitening
     </b>
    </summary>
    <p>
     <b>
      NOTE:
     </b>
     It is provided in PCA in sklearn
     <br/>
     Read
     <b>
      mastering ml book
     </b>
     page: 15
     <br/>
     <br/>
     wzxhzdk:35
    </p>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      5. Missing Values
     </b>
    </summary>
    <p>
    </p>
    <li>
     <p>
      <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.1-Missing-Data.html#Missing-Data-Mechanisms">
       <b style="color:#333">
        Missing Data Mechanisms
       </b>
      </a>
     </p>
    </li>
    <details>
     <summary>
      <b>
       Missing Data Imputing
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/24.%20Overview%20of%20missing%20value%20imputation%20methods.html">
        <b style="color:#333">
         Filling Missing Data Comparsion
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/25.%20Conclusion%20when%20to%20use%20each%20missing%20data%20imputation%20method.html">
        <b style="color:#333">
         Guidelines
        </b>
       </a>
      </p>
     </li>
     <details>
      <summary>
       <b>
        Categorical Variables
       </b>
      </summary>
      <p>
       <details>
        <summary>
         <b>
          1. Complete Case Analysis [CCA]
         </b>
        </summary>
        <p>
        </p>
        <li>
         <p>
          <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.01-Complete-Case-Analysis.html#Complete-Case-Analysis">
           <b style="color:#333">
            Manual
           </b>
          </a>
         </p>
        </li>
       </details>
      </p>
     </details>
     <details>
      <summary>
       <b>
        2. Arbitrary Value Imputation
       </b>
      </summary>
      <p>
      </p>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.03-Arbitrary-Value-Imputation.html#Arbitrary-value-imputation">
         <b style="color:#333">
          Manual
         </b>
         ['Missing']
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.12-Missing-Category-Imputation-Sklearn.html#Missing-Category-imputation-with-Scikit-learn:-SimpleImputer">
         <b style="color:#333">
          Sklearn
         </b>
         ['Missing']
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.20-Missing-Category-Imputation-Feature-Engine.html#Missing-Category-Imputation-==%3E-Feature-Engine">
         <b style="color:#333">
          Feature-Engine
         </b>
         ['Missing']
        </a>
       </p>
      </li>
     </details>
     <details>
      <summary>
       <b>
        3. Frequent category imputation | Mode imputation
       </b>
      </summary>
      <p>
      </p>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.05-Frequent-Category-Imputation.html#Frequent-category-imputation-|-Mode-imputation">
         <b style="color:#333">
          Manual
         </b>
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.11-Frequent-Category-Imputation-Sklearn.html#Frequent-category-imputation-with-Scikit-learn-==%3E-SimpleImputer">
         <b style="color:#333">
          Sklearn
         </b>
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.19-Frequent-Category-Imputation-Feature-Engine.html#Frequent-Category-Imputation-==%3E-Feature-Engine">
         <b style="color:#333">
          Feature-Engine
         </b>
        </a>
       </p>
      </li>
     </details>
     <details>
      <summary>
       <b>
        4. Random sample imputation
       </b>
      </summary>
      <p>
      </p>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.07-Random-Sample-Imputation.html#Random-Sampling-for-Categorical-Variables">
         <b style="color:#333">
          Manual
         </b>
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.20-Missing-Category-Imputation-Feature-Engine.html#Missing-Category-Imputation-==%3E-Feature-Engine">
         <b style="color:#333">
          Feature-Engine
         </b>
        </a>
       </p>
      </li>
     </details>
     <details>
      <summary>
       <b>
        5. Missing Binary Indicator
       </b>
      </summary>
      <p>
      </p>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.07-Random-Sample-Imputation.html#Random-Sampling-for-Categorical-Variables">
         <b style="color:#333">
          Manual
         </b>
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.13-MissingIndicator-Sklearn.html#Adding-a-Missing-Indicator-variable-with-Scikit-learn-==%3E-MissingIndicator">
         <b style="color:#333">
          Sklearn
         </b>
        </a>
       </p>
      </li>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.22-Missing-Indicator-Feature-Engine.html#Missing-Indicator-==%3E-Feature-Engine">
         <b style="color:#333">
          Feature-Engine
         </b>
        </a>
       </p>
      </li>
     </details>
     <details>
      <summary>
       <b>
        6. Automatic Imputing [GridSearch]
       </b>
      </summary>
      <p>
      </p>
      <li>
       <p>
        <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.14-Automatic-Imputation-Method-Detection-Sklearn.html#Automatic-selection-of-best-imputation-technique-with-Sklearn">
         <b style="color:#333">
          Sklearn
         </b>
        </a>
       </p>
      </li>
     </details>
    </details>
    <details>
     <summary>
      <b>
       Numerical Variables
      </b>
     </summary>
     <p>
      <details>
       <summary>
        <b>
         1. Complete Case Analysis [CCA]
        </b>
       </summary>
       <p>
       </p>
       <li>
        <p>
         <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.01-Complete-Case-Analysis.html#Complete-Case-Analysis">
          <b style="color:#333">
           1. Manual
          </b>
         </a>
        </p>
       </li>
      </details>
     </p>
    </details>
    <details>
     <summary>
      <b>
       2. Arbitrary Value Imputation
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.06-Missing-Category-Imputation.html#Arbitrary-value-imputation-for-categorical-variables">
        <b style="color:#333">
         1. Manual
        </b>
        [999/-1]
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.10-Arbitrary-Value-Imputation-Sklearn.html#Arbitrary-value-imputation-with-Scikit-learn-==%3E-SimpleImputer">
        <b style="color:#333">
         2. Sklearn
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.17-Arbitrary-Value-Imputation-Feature-Engine.html#Arbitrary-Imputation-==%3E-Feature-Engine">
        <b style="color:#333">
         Feature-Engine
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       3. Mean / Median Imputation
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.02-Mean-Median-Imputation.html#Mean-/-Median-imputation">
        <b style="color:#333">
         1. Manual
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.09-Mean-Median-Imputation-Sklearn.html#Mean-/-median-imputation-with-Scikit-learn-==%3E-SimpleImputer">
        <b style="color:#333">
         2. Sklearn
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.16-Mean-Median-Imputation-Feature-Engine.html#What-is-Feature-Engine">
        <b style="color:#333">
         Featuer-Engine
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       4. End of Distribution Imputation
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.04-End-Distribution-Imputation.html#End-of-distribution-imputation">
        <b style="color:#333">
         Manual
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.18-End-Tail-Imputation-Feature-Engine.html#End-of-distribution-Imputation-==%3E-Feature-Engine">
        <b style="color:#333">
         Feature-Engine
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       5. Frequent category imputation | Mode imputation
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.05-Frequent-Category-Imputation.html#Frequent-category-imputation-|-Mode-imputation">
        <b style="color:#333">
         Manual
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.11-Frequent-Category-Imputation-Sklearn.html#Frequent-category-imputation-with-Scikit-learn-==%3E-SimpleImputer">
        <b style="color:#333">
         Sklearn
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       6. Random sample imputation
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.07-Random-Sample-Imputation.html#Random-sample-imputation">
        <b style="color:#333">
        </b>
        Manual
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.20-Missing-Category-Imputation-Feature-Engine.html#Missing-Category-Imputation-==%3E-Feature-Engine">
        <b style="color:#333">
         Feature-Engine
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       7. Missing Binary Indicator
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.07-Random-Sample-Imputation.html#Random-Sampling-for-Categorical-Variables">
        <b style="color:#333">
         Manual
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.13-MissingIndicator-Sklearn.html#Adding-a-Missing-Indicator-variable-with-Scikit-learn-==%3E-MissingIndicator">
        <b style="color:#333">
         Sklearn
        </b>
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.22-Missing-Indicator-Feature-Engine.html#Missing-Indicator-==%3E-Feature-Engine">
        <b style="color:#333">
         Feature-Engine
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       8. Automatic Imputing [GridSearch]
      </b>
     </summary>
     <p>
     </p>
     <li>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-04-Missing-Data-Imputation/04.14-Automatic-Imputation-Method-Detection-Sklearn.html#Automatic-selection-of-best-imputation-technique-with-Sklearn">
        <b style="color:#333">
         Sklearn
        </b>
       </a>
      </p>
     </li>
    </details>
    <details>
     <summary>
      <b>
       9. KNN Imputer
      </b>
     </summary>
     <p>
      wzxhzdk:36
     </p>
    </details>
   </details>
   <br/>
   <details>
    <summary>
     <b>
      Misc Techniques
     </b>
    </summary>
    <p>
     <details>
      <summary>
       <b>
        Table contains # of Missing values &amp; its percentage
       </b>
      </summary>
      <p>
       wzxhzdk:37
      </p>
      <h4>
       Simple way
      </h4>
      wzxhzdk:38
     </details>
    </p>
   </details>
   <details>
    <summary>
     <b>
      Finding
     </b>
     NaN values
    </summary>
    <p>
     1)
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/9_Logistic%20Regression/1_Titanic-Logistic%20Regression.html#Missing-Data">
      See the
      <b>
       Result
      </b>
      .
     </a>
    </p>
    <p style="margin: 0">
     wzxhzdk:39


wzxhzdk:40
    </p>
   </details>
   <details>
    <summary>
     <b>
      Fill out
     </b>
     the *missing* data
    </summary>
    <p style="margin: 0">
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/10_%20K-Nearest%20Neighbors%20/1_step-by-step-diabetes-classification-knn-detailed.html#Skewness">
      Explanation for when to choose
      <b>
       Mean
      </b>
      ,
      <b>
       Median
      </b>
      , or
      <b>
       Mode
      </b>
      to fill out the missing data.
     </a>
    </p>
    wzxhzdk:41
   </details>
   <details>
    <summary>
     <b>
      Drop
     </b>
     the missing data
     <b>
      (skewness)
     </b>
    </summary>
    <p style="margin: 0">
     wzxhzdk:42
    </p>
   </details>
   <details>
    <summary>
     <b>
      Remove
     </b>
     missing values with
     <b>
      Threshold
     </b>
    </summary>
    <p style="margin: 0">
     wzxhzdk:43
    </p>
   </details>
   <details>
    <summary>
     <b>
      Add
     </b>
     a column containing the number of
     <b>
      NaN
     </b>
     s for a specific column
    </summary>
    <p style="margin: 0">
     wzxhzdk:44
    </p>
   </details>
   <details>
    <summary>
     <b>
      Sklearn Imputer
     </b>
    </summary>
    <p style="margin: 0">
     wzxhzdk:45
    </p>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      6. Multi-Collinearity
     </b>
    </summary>
    <p>
     <details>
      <summary>
       <b>
        1. Using Variance Inflation Fator
       </b>
      </summary>
      <p>
       <a href="file:///media/mosaab/Volume/Courses/Computer%20Science/Advanced/Machine%20Learning/Udacity/Udacity%20-%20Data%20Scientist%20Nanodegree%20nd025%20v1.0.0/Part%2012-Module%2001-Lesson%2015_Multiple%20Linear%20Regression/17.%20Screencast%20Multicollinearity%20%20VIFs.html">
        See the
        <b>
         video
        </b>
       </a>
      </p>
      <p>
       wzxhzdk:46
      </p>
     </details>
     <details>
      <summary>
       <b>
        2. Using Correlation matrix (pearson)
       </b>
      </summary>
      <p>
       <details>
        <summary>
         <b>
          2.1 Pearson Correlation
         </b>
        </summary>
        <p>
        </p>
        <p>
         <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/3_Home%20Credit%20Loans/1_Start%20Here:%20A%20Gentle%20Introduction.html#Exterior-Sources">
          See
          <b>
           Notebook
          </b>
         </a>
        </p>
        wzxhzdk:47



wzxhzdk:48



wzxhzdk:49
       </details>
      </p>
     </details>
     <details>
      <summary>
       <b>
        2.2 Spearman Correlation
       </b>
      </summary>
      <p>
      </p>
      <p>
       <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/5_Cargo%20Rican%20HouseHold/1_Costa%20Rican%20Household%20Poverty%20Level%20Prediction.html">
        <b>
         Notebook
        </b>
       </a>
      </p>
      <h4>
       1. Compute Spearman &amp; P-value
      </h4>
      wzxhzdk:50
      <h4>
       2. Calculate differences b/w spearman and pearson
      </h4>
      wzxhzdk:51
     </details>
    </p>
   </details>
   <details>
    <summary>
     2.2 Find
     <b>
      Correlation
     </b>
     b/w every 2 features
    </summary>
    <p>
     wzxhzdk:52
    </p>
   </details>
   <details>
    <summary>
     3. Correlation b/w
     <b>
      Nominal
     </b>
     features using [
     <b>
      Cramer’s V
     </b>
     ]
    </summary>
    <p>
     Cramer’s V is a measure of association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson's chi-squared statistic and was published by Harald Cramér in 1946.
     <br/>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/Misc/1_Cat%20Features%20Encoding%20Challenge.html">
      <b>
       Credits
      </b>
     </a>
    </p>
    wzxhzdk:53



wzxhzdk:54
    <h4>
     3. How to use
    </h4>
    wzxhzdk:55
    <h4>
     4. Get Correlation b/w every 2 features
    </h4>
    wzxhzdk:56
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      7. Linear Model Assumptions
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-03-Variable-Characteristics/03.4-Linear-Model-Assumptions.html#Linear-Model-Assumptions">
      <b>
       Notebook [MUST READ]
      </b>
     </a>
    </p>
    <ul>
     <li>
      There is a
      <b>
       linear
      </b>
      relationship between X and the outcome Y
     </li>
     <li>
      The independent variables X are
      <b>
       normally
      </b>
      distributed
     </li>
     <li>
      There is no or little
      <b>
       co-linearity
      </b>
      among the independent variables
     </li>
     <li>
      <b>
       Homoscedasticity
      </b>
      (homogeneity of variance)
     </li>
    </ul>
    <hr/>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-07-Variable-Transformation/07.01-Gaussian-transformation.html#Gaussian-Transformation">
      <b>
       Gaussian Transformation [Numpy]
      </b>
     </a>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-07-Variable-Transformation/07.01-Gaussian-transformation.html#Gaussian-Transformation">
      <b>
       Gaussian Transformation [Sklearn]
      </b>
     </a>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-07-Variable-Transformation/07.03-Gaussian-transformation-feature-engine.html#Gaussian-Transformation-with-Feature-Engine">
      <b>
       Gaussian Transformation [Feature-Engine]
      </b>
     </a>
    </p>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      8. Mixed Variables
     </b>
    </summary>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-11-Mixed-Variables/11.01-Engineering-mixed-variables.html#Engineering-mixed-variables">
      <b>
       The observations of the variable contain either numbers or strings
      </b>
     </a>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Feature%20Engineering%20for%20Machine%20Learning/0_code/Section-11-Mixed-Variables/11.01-Engineering-mixed-variables.html#Example-1:-the-observations-of-the-variable-contain-numbers-and-strings">
      <b>
       The observations of the variable contain numbers and strings
      </b>
     </a>
    </p>
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      9. Cold-Start Problem
     </b>
    </summary>
    <p>
    </p>
    <h4>
     1. Check the cold-start problem
    </h4>
    wzxhzdk:57
   </details>
   <hr/>
   <details>
    <summary style="font-size:23px;text-decoration:underline">
     <b>
      10. Underfitting &amp; Overfitting
     </b>
    </summary>
    <p>
    </p>
    <h4>
     1. Fix Overfitting
    </h4>
    <ul>
     <li>
      To simplify the model
      <br/>
      by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial
model),
      <br/>
      by reducing the number of attributes in the training data or
      <br/>
      by constraining the model (Adding Regularization)
      <br/>
      In DL, reduce the number of layers/neurons or add a drop-out layer.
     </li>
     <li>
      To gather more training data.
     </li>
     <li>
      To reduce the noise in the training data (e.g., fix data errors
and remove outliers)
     </li>
    </ul>
    <h4>
     2. Fix Underfitting
    </h4>
    <ul>
     <li>
      Selecting a more powerful model, with more parameters
     </li>
     <li>
      Feeding better features to the learning algorithm (feature engineering)
     </li>
     <li>
      Reducing the constraints on the model (e.g., reducing the regularization hyper‐
parameter)
     </li>
    </ul>
    <hr/>
    <h4>
     Learning Curves to check Overfitting or Underfitting
    </h4>
    wzxhzdk:58
    <h4>
     Using sklearn
    </h4>
    wzxhzdk:59



wzxhzdk:60



wzxhzdk:61
    <img height="200" src="imgs/20200916-161740.png" width="400"/>
   </details>
  </div>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>
