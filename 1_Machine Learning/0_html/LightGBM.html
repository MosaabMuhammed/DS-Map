<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 id="light-gbm">
   <mark>
    Light GBM
   </mark>
  </h1>
  <p>
   <details>
    <summary>
     <strong>
      1. Class Classifier
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <p>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Sklearn/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier">
      <strong>
       Class &amp; Parameters
      </strong>
     </a>
    </p>
    <pre><code class="python">class lightgbm.LGBMClassifier(
            boosting_type='gbdt', 
            num_leaves=31, 
            max_depth=-1, 
            learning_rate=0.1, 
            n_estimators=100, 
            subsample_for_bin=200000, 
            objective=None, 
            class_weight=None, 
            min_split_gain=0.0, 
            min_child_weight=0.001, 
            min_child_samples=20, 
            subsample=1.0, 
            subsample_freq=0, 
            colsample_bytree=1.0, 
            reg_alpha=0.0, 
            reg_lambda=0.0, 
            random_state=None, 
            n_jobs=-1, 
            silent=True, 
            importance_type='split', 
            **kwargs)
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <ul>
   <li>
    <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Sklearn/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier.n_features_">
     <strong>
      <span style="color:#333">
       2. Attributes
      </span>
     </strong>
    </a>
   </li>
  </ul>
  <p>
   <details>
    <summary>
     <strong>
      1. Model
     </strong>
    </summary>
    <br/>
    <p>
     <br/>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/3_Home%20Credit%20Loans/1_Start%20Here:%20A%20Gentle%20Introduction.html#Just-for-Fun:-Light-Gradient-Boosting-Machine">
      <strong>
       Notebook
      </strong>
     </a>
    </p>
    <pre><code class="python">from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
import lightgbm as lgb
import gc

def model(features, test_features, encoding = 'ohe', n_folds = 5):

    """Train and test a light gradient boosting model using
    cross validation. 

    Parameters
    --------
        features (pd.DataFrame): 
            dataframe of training features to use 
            for training a model. Must include the TARGET column.
        test_features (pd.DataFrame): 
            dataframe of testing features to use
            for making predictions with the model. 
        encoding (str, default = 'ohe'): 
            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding
            n_folds (int, default = 5): number of folds to use for cross validation

    Return
    --------
        submission (pd.DataFrame): 
            dataframe with `SK_ID_CURR` and `TARGET` probabilities
            predicted by the model.
        feature_importances (pd.DataFrame): 
            dataframe with the feature importances from the model.
        valid_metrics (pd.DataFrame): 
            dataframe with training and validation metrics (ROC AUC) for each fold and overall.

    """

    # Extract the ids
    train_ids = features['SK_ID_CURR']
    test_ids = test_features['SK_ID_CURR']

    # Extract the labels for training
    labels = features['TARGET']

    # Remove the ids and target
    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])
    test_features = test_features.drop(columns = ['SK_ID_CURR'])


    # One Hot Encoding
    if encoding == 'ohe':
        features = pd.get_dummies(features)
        test_features = pd.get_dummies(test_features)

        # Align the dataframes by the columns
        features, test_features = features.align(test_features, join = 'inner', axis = 1)

        # No categorical indices to record
        cat_indices = 'auto'

    # Integer label encoding
    elif encoding == 'le':

        # Create a label encoder
        label_encoder = LabelEncoder()

        # List for storing categorical indices
        cat_indices = []

        # Iterate through each column
        for i, col in enumerate(features):
            if features[col].dtype == 'object':
                # Map the categorical features to integers
                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))
                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))

                # Record the categorical indices
                cat_indices.append(i)

    # Catch error if label encoding scheme is not valid
    else:
        raise ValueError("Encoding must be either 'ohe' or 'le'")

    print('Training Data Shape: ', features.shape)
    print('Testing Data Shape: ', test_features.shape)

    # Extract feature names
    feature_names = list(features.columns)

    # Convert to np arrays
    features = np.array(features)
    test_features = np.array(test_features)

    # Create the kfold object
    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)

    # Empty array for feature importances
    feature_importance_values = np.zeros(len(feature_names))

    # Empty array for test predictions
    test_predictions = np.zeros(test_features.shape[0])

    # Empty array for out of fold validation predictions
    out_of_fold = np.zeros(features.shape[0])

    # Lists for recording validation and training scores
    valid_scores = []
    train_scores = []

    # Iterate through each fold
    for train_indices, valid_indices in k_fold.split(features):

        # Training data for the fold
        train_features, train_labels = features[train_indices], labels[train_indices]
        # Validation data for the fold
        valid_features, valid_labels = features[valid_indices], labels[valid_indices]

        # Create the model
        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', 
                                   class_weight = 'balanced', learning_rate = 0.05, 
                                   reg_alpha = 0.1, reg_lambda = 0.1, 
                                   subsample = 0.8, n_jobs = -1, random_state = 50)

        # Train the model
        model.fit(train_features, train_labels, eval_metric = 'auc',
                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],
                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,
                  early_stopping_rounds = 100, verbose = 200)

        # Record the best iteration
        best_iteration = model.best_iteration_

        # Record the feature importances
        feature_importance_values += model.feature_importances_ / k_fold.n_splits

        # Make predictions
        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits

        # Record the out of fold predictions
        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]

        # Record the best score
        valid_score = model.best_score_['valid']['auc']
        train_score = model.best_score_['train']['auc']

        valid_scores.append(valid_score)
        train_scores.append(train_score)

        # Clean up memory
        gc.enable()
        del model, train_features, valid_features
        gc.collect()

    # Make the submission dataframe
    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})

    # Make the feature importance dataframe
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})

    # Overall validation score
    valid_auc = roc_auc_score(labels, out_of_fold)

    # Add the overall scores to the metrics
    valid_scores.append(valid_auc)
    train_scores.append(np.mean(train_scores))

    # Needed for creating dataframe of validation scores
    fold_names = list(range(n_folds))
    fold_names.append('overall')

    # Dataframe of validation scores
    metrics = pd.DataFrame({'fold': fold_names,
                            'train': train_scores,
                            'valid': valid_scores}) 

    return submission, feature_importances, metrics
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      2. parameters
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python"># Create a default model
model = lgb.LGBMModel()
model.get_params()

########## Result ##########
{'boosting_type': 'gbdt',
 'class_weight': None,
 'colsample_bytree': 1.0,
 'learning_rate': 0.1,
 'max_depth': -1,
 'min_child_samples': 20,
 'min_child_weight': 0.001,
 'min_split_gain': 0.0,
 'n_estimators': 100,
 'n_jobs': -1,
 'num_leaves': 31,
 'objective': None,
 'random_state': None,
 'reg_alpha': 0.0,
 'reg_lambda': 0.0,
 'silent': True,
 'subsample': 1.0,
 'subsample_for_bin': 200000,
 'subsample_freq': 0}
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     2. Plot the
     <strong>
      Important Features
     </strong>
     and
     <strong>
      CDF
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">def plot_feature_importances(df, threshold = 0.9):
    """
    Plots 15 most important features and the cumulative importance of features.
    Prints the number of features needed to reach threshold cumulative importance.

    Parameters
    --------
    df : dataframe
        Dataframe of feature importances. Columns must be feature and importance
    threshold : float, default = 0.9
        Threshold for prining information about cumulative importances

    Return
    --------
    df : dataframe
        Dataframe ordered by feature importances with a normalized column (sums to 1)
        and a cumulative importance column

    """

    plt.rcParams['font.size'] = 18

    # Sort features according to importance
    df = df.sort_values('importance', ascending = False).reset_index()

    # Normalize the feature importances to add up to one
    df['importance_normalized'] = df['importance'] / df['importance'].sum()
    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])

    # Make a horizontal bar chart of feature importances
    plt.figure(figsize = (10, 6))
    ax = plt.subplot()

    # Need to reverse the index to plot most important on top
    ax.barh(list(reversed(list(df.index[:15]))), 
            df['importance_normalized'].head(15), 
            align = 'center', edgecolor = 'k')

    # Set the yticks and labels
    ax.set_yticks(list(reversed(list(df.index[:15]))))
    ax.set_yticklabels(df['feature'].head(15))

    # Plot labeling
    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')
    plt.show()

    # Cumulative importance plot
    plt.figure(figsize = (8, 6))
    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')
    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); 
    plt.title('Cumulative Feature Importance');
    plt.show();

    importance_index = np.min(np.where(df['cumulative_importance'] &gt; threshold))
    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))

    return df
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     3. Identify
     <strong>
      Zero Important Features
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">def identify_zero_importance_features(train, train_labels, iterations = 2):
    """
    Identify zero importance features in a training dataset based on the 
    feature importances from a gradient boosting model. 

    Parameters
    --------
    train : dataframe
        Training features

    train_labels : np.array
        Labels for training data

    iterations : integer, default = 2
        Number of cross validation splits to use for determining feature importances
    """

    # Initialize an empty array to hold feature importances
    feature_importances = np.zeros(train.shape[1])

    # Create the model with several hyperparameters
    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')

    # Fit the model multiple times to avoid overfitting
    for i in range(iterations):

        # Split into training and validation set
        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)

        # Train using early stopping
        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], 
                  eval_metric = 'auc', verbose = 200)

        # Record the feature importances
        feature_importances += model.feature_importances_ / iterations

    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)

    # Find the features with zero importance
    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])
    print('\nThere are %d features with 0.0 importance' % len(zero_features))

    return zero_features, feature_importances
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      4. CV
     </strong>
     with
     <strong>
      Early Stopping
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <h4 id="1-create-train-and-test-gbm-dataset">
     1. Create train and test gbm dataset
    </h4>
    <pre><code class="python"># Create a training and testing dataset
train_set = lgb.Dataset(data = train_features, label = train_labels)
test_set = lgb.Dataset(data = test_features, label = test_labels)
</code></pre>
    <h4 id="2-get-the-default-hyperparameters">
     2. Get the default hyperparameters
    </h4>
    <pre><code class="python"># Get default hyperparameters
model = lgb.LGBMClassifier()
default_params = model.get_params()

# Remove the number of estimators because we set this to 10000 in the cv call
del default_params['n_estimators']

# Cross validation with early stopping
cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, 
                    metrics = 'auc', nfold = N_FOLDS, seed = 42)
</code></pre>
    <blockquote>
     <p>
      The cv_results is a dictionary with lists for the metric mean and the metric standard deviation. The last entry (index of -1) contains the best performing score. The length of each list in the dictionary will be the “optimal” number of estimators to train.
     </p>
    </blockquote>
    <pre><code class="python">print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))
print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))
</code></pre>
    <h4 id="4-make-prediction-for-testing-data">
     4. Make prediction for testing data.
    </h4>
    <pre><code class="python">from sklearn.metrics import roc_auc_score
# Optimal number of esimators found in cv
model.n_estimators = len(cv_results['auc-mean'])

# Train and make predicions with model
model.fit(train_features, train_labels)
preds = model.predict_proba(test_features)[:, 1]
baseline_auc = roc_auc_score(test_labels, preds)

print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))
</code></pre>
    <h4 id="5-objective-function-contains-all-previous-steps">
     5. Objective function contains all previous steps
    </h4>
    <pre><code class="python">def objective(hyperparameters, iteration):
    """Objective function for grid and random search. Returns
       the cross validation score from a set of hyperparameters."""

    # Number of estimators will be found using early stopping
    if 'n_estimators' in hyperparameters.keys():
        del hyperparameters['n_estimators']

     # Perform n_folds cross validation
    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, 
                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)

    # results to retun
    score = cv_results['auc-mean'][-1]
    estimators = len(cv_results['auc-mean'])
    hyperparameters['n_estimators'] = estimators 

    return [score, hyperparameters, iteration]
score, params, iteration = objective(default_params, 1)

print('The cross-validation ROC AUC was {:.5f}.'.format(score))
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      Model (Multi-Class)
     </strong>
     &amp;
     <strong>
      Custom
     </strong>
     Scoring Function
    </summary>
    <br/>
    <p>
    </p>
    <h4 id="1-define-the-custom-function">
     1. Define the custom function.
    </h4>
    <pre><code class="python">def macro_f1_score(labels, predictions):
    # Reshape the predictions as needed
    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)

    metric_value = f1_score(labels, predictions, average = 'macro')

    # Return is name, value, is_higher_better
    return 'macro_f1', metric_value, True
</code></pre>
    <h4 id="2-function-model">
     2. Function Model
    </h4>
    <pre><code class="python">from sklearn.model_selection import StratifiedKFold
import lightgbm as lgb
from IPython.display import display

def model_gbm(features, labels, test_features, test_ids, 
              nfolds = 5, return_preds = False, hyp = None):
    """Model using the GBM and cross validation.
       Trains with early stopping on each fold.
       Hyperparameters probably need to be tuned."""

    feature_names = list(features.columns)

    # Option for user specified hyperparameters
    if hyp is not None:
        # Using early stopping so do not need number of esimators
        if 'n_estimators' in hyp:
            del hyp['n_estimators']
        params = hyp

    else:
        # Model hyperparameters
        params = {'boosting_type': 'dart', 
                  'colsample_bytree': 0.88, 
                  'learning_rate': 0.028, 
                   'min_child_samples': 10, 
                   'num_leaves': 36, 'reg_alpha': 0.76, 
                   'reg_lambda': 0.43, 
                   'subsample_for_bin': 40000, 
                   'subsample': 0.54, 
                   'class_weight': 'balanced'}

    # Build the model
    model = lgb.LGBMClassifier(**params, objective = 'multiclass', 
                               n_jobs = -1, n_estimators = 10000,
                               random_state = 10)

    # Using stratified kfold cross validation
    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)

    # Hold all the predictions from each fold
    predictions = pd.DataFrame()
    importances = np.zeros(len(feature_names))

    # Convert to arrays for indexing
    features = np.array(features)
    test_features = np.array(test_features)
    labels = np.array(labels).reshape((-1 ))

    valid_scores = []

    # Iterate through the folds
    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):

        # Dataframe for fold predictions
        fold_predictions = pd.DataFrame()

        # Training and validation data
        X_train = features[train_indices]
        X_valid = features[valid_indices]
        y_train = labels[train_indices]
        y_valid = labels[valid_indices]

        # Train with early stopping
        model.fit(X_train, y_train, early_stopping_rounds = 100, 
                  eval_metric = macro_f1_score,
                  eval_set = [(X_train, y_train), (X_valid, y_valid)],
                  eval_names = ['train', 'valid'],
                  verbose = 200)

        # Record the validation fold score
        valid_scores.append(model.best_score_['valid']['macro_f1'])

        # Make predictions from the fold as probabilities
        fold_probabilitites = model.predict_proba(test_features)

        # Record each prediction for each class as a separate column
        for j in range(4):
            fold_predictions[(j + 1)] = fold_probabilitites[:, j]

        # Add needed information for predictions 
        fold_predictions['idhogar'] = test_ids
        fold_predictions['fold'] = (i+1)

        # Add the predictions as new rows to the existing predictions
        predictions = predictions.append(fold_predictions)

        # Feature importances
        importances += model.feature_importances_ / nfolds   

        # Display fold information
        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')

    # Feature importances dataframe
    feature_importances = pd.DataFrame({'feature': feature_names,
                                        'importance': importances})

    valid_scores = np.array(valid_scores)
    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')

    # If we want to examine predictions don't average over folds
    if return_preds:
        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)
        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)
        return predictions, feature_importances

    # Average the predictions over folds
    predictions = predictions.groupby('idhogar', as_index = False).mean()

    # Find the class and associated probability
    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)
    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)
    predictions = predictions.drop(columns = ['fold'])

    # Merge with the base to have one prediction for each individual
    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])

    # Fill in the individuals that do not have a head of household with 4 since these will not be scored
    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)

    # return the submission and feature importances along with validation scores
    return submission, feature_importances, valid_scores
</code></pre>
    <h4 id="3-call-the-model-function">
     3. Call the model function.
    </h4>
    <pre><code class="python">%%capture --no-display
predictions, gbm_fi = model_gbm(train_set, train_labels, test_set, test_ids, return_preds=True)
</code></pre>
    <pre><code class="python">%%capture
submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, 
                                             test_set, test_ids, return_preds=False)

submission.to_csv('gbm_baseline.csv')
</code></pre>
    <h4 id="plot-feature-importance">
     Plot feature importance
    </h4>
    <pre><code class="python">_ = plot_feature_importances(gbm_fi, threshold=0.95)

</code></pre>
    <p>
    </p>
    <br/>
   </details>
   <br/>
   <details>
    <summary>
     <strong>
      6. GridSearch (manual)
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <h4 id="1-hyperparameters-to-tune">
     1. Hyperparameters to tune
    </h4>
    <blockquote>
     <p>
      <strong>
       Note:
      </strong>
      One aspect to note is that if boosting_type is goss, then we cannot use subsample
     </p>
    </blockquote>
    <pre><code class="python"># 1. Hyperparameter grid
param_grid = {
    'boosting_type': ['gbdt', 'goss', 'dart'],
    'num_leaves': list(range(20, 150)),
    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),
    'subsample_for_bin': list(range(20000, 300000, 20000)),
    'min_child_samples': list(range(20, 500, 5)),
    'reg_alpha': list(np.linspace(0, 1)),
    'reg_lambda': list(np.linspace(0, 1)),
    'colsample_bytree': list(np.linspace(0.6, 1, 10)),
    'subsample': list(np.linspace(0.5, 1, 100)),
    'is_unbalance': [True, False]
}
</code></pre>
    <h4 id="2-dataframe-to-store-the-score-of-each-evaluation">
     2. DataFrame to store the score of each evaluation.
    </h4>
    <pre><code class="python">grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],
                              index = list(range(MAX_EVALS)))
</code></pre>
    <h4 id="3-gridsearch-function">
     3. GridSearch Function
    </h4>
    <pre><code class="python">import itertools

def grid_search(param_grid, max_evals = MAX_EVALS):
    """Grid search algorithm (with limit on max evals)"""

    # Dataframe to store results
    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],
                              index = list(range(MAX_EVALS)))

    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists
    keys, values = zip(*param_grid.items())

    i = 0

    # Iterate through every possible combination of hyperparameters
    for v in itertools.product(*values):

        # Create a hyperparameter dictionary
        hyperparameters = dict(zip(keys, v))

        # Set the subsample ratio accounting for boosting type
        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']

        # Evalute the hyperparameters
        eval_results = objective(hyperparameters, i)

        results.loc[i, :] = eval_results

        i += 1

        # Normally would not limit iterations
        if i &gt; MAX_EVALS:
            break

    # Sort with best score on top
    results.sort_values('score', ascending = False, inplace = True)
    results.reset_index(inplace = True)

    return results 
</code></pre>
    <h4 id="4-print-the-best-hyperparameters">
     4. Print the best hyperparameters
    </h4>
    <pre><code class="python">grid_results = grid_search(param_grid)

print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))
print('\nThe best hyperparameters were:')

import pprint
pprint.pprint(grid_results.loc[0, 'params'])

</code></pre>
    <h4 id="5-test-the-best-model-on-test-set">
     5. Test the best model on [Test] set.
    </h4>
    <pre><code class="python"># Get the best parameters
grid_search_params = grid_results.loc[0, 'params']

# Create, train, test model
model = lgb.LGBMClassifier(**grid_search_params, random_state=42)
model.fit(train_features, train_labels)

preds = model.predict_proba(test_features)[:, 1]

print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      6.1 GridSearch
     </strong>
     (LongerTimeVersion) save to file
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">def grid_search(param_grid, out_file, max_evals = MAX_EVALS):
    """Grid search algorithm (with limit on max evals)
       Writes result of search to csv file every search iteration."""

    # Dataframe to store results
    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],
                              index = list(range(MAX_EVALS)))

    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists
    keys, values = zip(*param_grid.items())

    i = 0

    # Iterate through every possible combination of hyperparameters
    for v in itertools.product(*values):
        # Select the hyperparameters
        parameters = dict(zip(keys, v))

        # Set the subsample ratio accounting for boosting type
        parameters['subsample'] = 1.0 if parameters['boosting_type'] == 'goss' else parameters['subsample']

        # Evalute the hyperparameters
        eval_results = objective(parameters, i)

        results.loc[i, :] = eval_results

        i += 1

        # open connection (append option) and write results
        of_connection = open(out_file, 'a')
        writer = csv.writer(of_connection)
        writer.writerow(eval_results)

        # make sure to close connection
        of_connection.close()

        # Normally would not limit iterations
        if i &gt; MAX_EVALS:
            break

    # Sort with best score on top
    results.sort_values('score', ascending = False, inplace = True)
    results.reset_index(inplace = True)

    return results
</code></pre>
    <h4 id="2-run-the-function">
     2. Run the function
    </h4>
    <pre><code class="python"># MAX_EVALS = 1000

# Create file and open connection
out_file = 'grid_search_trials_1000.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write column names
headers = ['score', 'hyperparameters', 'iteration']
writer.writerow(headers)
of_connection.close()

grid_results = grid_search(param_grid, out_file)
</code></pre>
    <h4 id="covert-the-hyperparamters-from-string-to-dict">
     Covert the hyperparamters from string to dict
    </h4>
    <pre><code class="python">import ast

# Convert strings to dictionaries
grid_results['hyperparameters'] = grid_results['hyperparameters'].map(ast.literal_eval)
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      7. Randomized Search (manual)
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <h4 id="1-hyperparameters-to-tune_1">
     1. Hyperparameters to tune
    </h4>
    <blockquote>
     <p>
      <strong>
       Note:
      </strong>
      One aspect to note is that if boosting_type is goss, then we cannot use subsample
     </p>
    </blockquote>
    <pre><code class="python"># 1. Hyperparameter grid
param_grid = {
    'boosting_type': ['gbdt', 'goss', 'dart'],
    'num_leaves': list(range(20, 150)),
    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),
    'subsample_for_bin': list(range(20000, 300000, 20000)),
    'min_child_samples': list(range(20, 500, 5)),
    'reg_alpha': list(np.linspace(0, 1)),
    'reg_lambda': list(np.linspace(0, 1)),
    'colsample_bytree': list(np.linspace(0.6, 1, 10)),
    'subsample': list(np.linspace(0.5, 1, 100)),
    'is_unbalance': [True, False]
}
</code></pre>
    <h4 id="2-dataframe-to-store-the-score-of-each-evaluation_1">
     2. DataFrame to store the score of each evaluation.
    </h4>
    <pre><code class="python">random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],
                              index = list(range(MAX_EVALS)))
</code></pre>
    <h4 id="3-randomized-search-function">
     3. Randomized Search Function
    </h4>
    <pre><code class="python">def random_search(param_grid, max_evals = MAX_EVALS):
    """Random search for hyperparameter optimization"""

    # Dataframe for results
    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],
                                  index = list(range(MAX_EVALS)))

    # Keep searching until reach max evaluations
    for i in range(MAX_EVALS):

        # Choose random hyperparameters
        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']

        # Evaluate randomly selected hyperparameters
        eval_results = objective(hyperparameters, i)

        results.loc[i, :] = eval_results

    # Sort with best score on top
    results.sort_values('score', ascending = False, inplace = True)
    results.reset_index(inplace = True)
    return results
</code></pre>
    <h4 id="4-printing-the-best-hyperparameters">
     4. Printing the best hyperparameters
    </h4>
    <pre><code class="python">random_results = random_search(param_grid)

print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))
print('\nThe best hyperparameters were:')

import pprint
pprint.pprint(random_results.loc[0, 'params'])
</code></pre>
    <h4 id="5-trying-the-best-estimator-on-test-set">
     5. Trying the best estimator on [Test] set.
    </h4>
    <pre><code class="python"># Get the best parameters
random_search_params = random_results.loc[0, 'params']

# Create, train, test model
model = lgb.LGBMClassifier(**random_search_params, random_state = 42)
model.fit(train_features, train_labels)

preds = model.predict_proba(test_features)[:, 1]

print('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      7.1 Randomized Search
     </strong>
     (LongerTimeVersion) save to file
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">def random_search(param_grid, out_file, max_evals = MAX_EVALS):
    """Random search for hyperparameter optimization. 
       Writes result of search to csv file every search iteration."""


    # Dataframe for results
    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],
                                  index = list(range(MAX_EVALS)))
    for i in range(MAX_EVALS):

        # Choose random hyperparameters
        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}
        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']

        # Evaluate randomly selected hyperparameters
        eval_results = objective(random_params, i)
        results.loc[i, :] = eval_results

        # open connection (append option) and write results
        of_connection = open(out_file, 'a')
        writer = csv.writer(of_connection)
        writer.writerow(eval_results)

        # make sure to close connection
        of_connection.close()

    # Sort with best score on top
    results.sort_values('score', ascending = False, inplace = True)
    results.reset_index(inplace = True)

    return results 
</code></pre>
    <h4 id="2-run-the-function_1">
     2. Run the function
    </h4>
    <pre><code class="python"># Create file and open connection
out_file = 'random_search_trials_1000.csv'
of_connection = open(out_file, 'w')
writer = csv.writer(of_connection)

# Write column names
headers = ['score', 'hyperparameters', 'iteration']
writer.writerow(headers)
of_connection.close()

random_results = random_search(param_grid, out_file)
</code></pre>
    <h4 id="covert-the-parameters-from-string-to-dict">
     Covert the parameters from string to dict
    </h4>
    <pre><code class="python">import ast

# Convert strings to dictionaries
random_results['hyperparameters'] = random_results['hyperparameters'].map(ast.literal_eval)
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      8. Evaluate Results from (Grid/Random)
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">def evaluate(results, name):
    """Evaluate model on test data using hyperparameters in results
       Return dataframe of hyperparameters"""

    # Sort with best values on top
    results = results.sort_values('score', ascending = False).reset_index(drop = True)

    # Print out cross validation high score
    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, results.loc[0, 'score'], results.loc[0, 'iteration']))

    # Use best hyperparameters to create a model
    hyperparameters = results.loc[0, 'hyperparameters']
    model = lgb.LGBMClassifier(**hyperparameters)

    # Train and make predictions
    model.fit(train_features, train_labels)
    preds = model.predict_proba(test_features)[:, 1]

    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))

    # Create dataframe of hyperparameters
    hyp_df = pd.DataFrame(columns = list(results.loc[0, 'hyperparameters'].keys()))

    # Iterate through each set of hyperparameters that were evaluated
    for i, hyp in enumerate(results['hyperparameters']):
        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), 
                               ignore_index = True)

    # Put the iteration and score in the hyperparameter dataframe
    hyp_df['iteration'] = results['iteration']
    hyp_df['score'] = results['score']

    return hyp_df
</code></pre>
    <h4 id="2-using-the-function">
     2. Using the function
    </h4>
    <pre><code class="python">grid_hyp = evaluate(grid_results, name = 'grid search')

random_hyp = evaluate(random_results, name = 'random search')
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      9. Plot Distribution of Search
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python"># Iterate through each hyperparameter
for i, hyper in enumerate(random_hyp.columns):
    if hyper not in ['boosting_type', 'iteration', 'subsample', 'score', 'learning_rate', 'is_unbalance', 'metric', 'verbose', 'iteration', 'n_estimators', 'search']:
        plt.figure(figsize = (14, 6))

        # Plot the random search distribution and the sampling distribution
        if hyper != 'loss':
            sns.kdeplot(param_grid[hyper], label = 'Sampling Distribution', linewidth = 4)
        sns.kdeplot(random_hyp[hyper], label = 'Random Search', linewidth = 4)
        plt.vlines([best_random_hyp[hyper]],
                     ymin = 0.0, ymax = 10.0, linestyles = '--', linewidth = 4, colors = ['orange'])
        plt.legend(loc = 1)
        plt.title('{} Distribution'.format(hyper))
        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');
        plt.show();
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      10. Plot Squence of Search Value
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">fig, axs = plt.subplots(1, 4, figsize = (24, 6))
i = 0

# Plot of four hyperparameters
for i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):
        random_hyp[hyper] = random_hyp[hyper].astype(float)
        # Scatterplot
        sns.regplot('iteration', hyper, data = random_hyp, ax = axs[i])
        axs[i].scatter(best_random_hyp['iteration'], best_random_hyp[hyper], marker = '*', s = 200, c = 'k')
        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));

plt.tight_layout()
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      11. Plot Score Vs. Hyperparamter
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <pre><code class="python">fig, axs = plt.subplots(1, 4, figsize = (24, 6))
i = 0

# Plot of four hyperparameters
for i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):
        random_hyp[hyper] = random_hyp[hyper].astype(float)
        # Scatterplot
        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i])
        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'k')
        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper));

plt.tight_layout()
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      12. Baysian Optimization
     </strong>
    </summary>
    <br/>
    <p>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        1. Objective Function
       </strong>
      </summary>
      <br/>
      <p>
       <br/>
       <a href="https://www.kaggle.com/willkoehrsen/automated-model-tuning#Objective-Function">
        <strong>
         See the Code
        </strong>
       </a>
      </p>
      <h4 id="1-cv-with-early-stopping">
       1. CV with early stopping
      </h4>
      <pre><code class="python">import csv
from hyperopt import STATUS_OK
from timeit import default_timer as timer

def objective(hyperparameters):
    """Objective function for Gradient Boosting Machine Hyperparameter Optimization.
       Writes a new line to `outfile` on every iteration"""

    # Keep track of evals
    global ITERATION

    ITERATION += 1

    # Using early stopping to find number of trees trained
    if 'n_estimators' in hyperparameters:
        del hyperparameters['n_estimators']

    # Retrieve the subsample
    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)

    # Extract the boosting type and subsample to top level keys
    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']
    hyperparameters['subsample'] = subsample

    # Make sure parameters that need to be integers are integers
    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:
        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])

    start = timer()

    # Perform n_folds cross validation
    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, 
                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)

    run_time = timer() - start

    # Extract the best score
    best_score = cv_results['auc-mean'][-1]

    # Loss must be minimized
    loss = 1 - best_score

    # Boosting rounds that returned the highest cv score
    n_estimators = len(cv_results['auc-mean'])

    # Add the number of estimators to the hyperparameters
    hyperparameters['n_estimators'] = n_estimators

    # Write to the csv file ('a' means append)
    of_connection = open(OUT_FILE, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])
    of_connection.close()

    # Dictionary with information for evaluation
    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,
            'train_time': run_time, 'status': STATUS_OK}
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        2. Domin (Space)
       </strong>
      </summary>
      <br/>
      <p>
       <br/>
       <a href="https://www.kaggle.com/willkoehrsen/automated-model-tuning#Domain">
        <strong>
         Domin Code
        </strong>
       </a>
      </p>
      <h4 id="1-tutorials-on-how-to-use-the-diff-disb-for-each-hyperparameters">
       1. Tutorials on how to use the diff. disb for each hyperparameters
      </h4>
      <pre><code class="python">from hyperopt import hp
from hyperopt.pyll.stochastic import sample

# Create the learning rate
learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}

# Discrete uniform distribution
num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}

# boosting type domain 
boosting_type = {'boosting_type': hp.choice('boosting_type', 
                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, 
                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},
                                             {'boosting_type': 'goss', 'subsample': 1.0}])}

# Retrieve the subsample if present otherwise set to 1.0
subsample = hyperparams['boosting_type'].get('subsample', 1.0)

# Extract the boosting type
hyperparams['boosting_type'] = hyperparams['boosting_type']['boosting_type']
hyperparams['subsample'] = subsample
</code></pre>
      <h4 id="2-complete-space-for-lightgbm">
       2. Complete Space for LightGBM
      </h4>
      <pre><code class="python"># Define the search space
space = {
    'boosting_type': hp.choice('boosting_type', 
                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, 
                                             {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},
                                             {'boosting_type': 'goss', 'subsample': 1.0}]),
    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),
    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),
    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),
    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),
    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),
    'is_unbalance': hp.choice('is_unbalance', [True, False]),
}

# Sample from the full space
x = sample(space)

# Conditional logic to assign top-level keys
subsample = x['boosting_type'].get('subsample', 1.0)
x['boosting_type'] = x['boosting_type']['boosting_type']
x['subsample'] = subsample
</code></pre>
      <h4 id="example-of-using-objective-function-using-a-sample-from-hyperparameters">
       Example of using Objective function using a sample from hyperparameters
      </h4>
      <pre><code class="python"># Create a new file and open a connection
OUT_FILE = 'bayes_test.csv'
of_connection = open(OUT_FILE, 'w')
writer = csv.writer(of_connection)

ITERATION = 0

# Write column names
headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']
writer.writerow(headers)
of_connection.close()

# Test the objective function
results = objective(sample(space))
print('The cross validation loss = {:.5f}.'.format(results['loss']))
print('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        3. Optimization Algorithm
       </strong>
      </summary>
      <br/>
      <p>
      </p>
      <pre><code class="python">from hyperopt import tpe

# Create the algorithm
tpe_algorithm = tpe.suggest
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        4. Results Summary
       </strong>
      </summary>
      <br/>
      <p>
       <br/>
       <a href="https://www.kaggle.com/willkoehrsen/automated-model-tuning#Results-History">
        <strong>
         See the code
        </strong>
       </a>
      </p>
      <blockquote>
       <p>
        <strong>
         NOTE:
        </strong>
        Don’t open the file using
        <code>
         Excel
        </code>
        , open it using
        <code>
         Bash
        </code>
        by
        <code>
         tail results/out_file.csv
        </code>
        or using
        <code>
         Sublime
        </code>
        or
        <code>
         Notepad
        </code>
        .
       </p>
      </blockquote>
      <pre><code class="python">from hyperopt import Trials

# Record results
trials = Trials()
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        5. Automated Hyperparamter in Practice
       </strong>
      </summary>
      <br/>
      <p>
      </p>
      <blockquote>
       <p>
        <strong>
         NOTE:
        </strong>
        Don’t open the file using
        <code>
         Excel
        </code>
        , open it using
        <code>
         Bash
        </code>
        by
        <code>
         tail results/out_file.csv
        </code>
        or using
        <code>
         Sublime
        </code>
        or
        <code>
         Notepad
        </code>
        .
       </p>
      </blockquote>
      <pre><code class="python">from hyperopt import fmin
fmin takes the four parts defined above as well as the maximum number of iterations max_evals.

# Global variable
global  ITERATION

ITERATION = 0

# Run optimization
best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,
            max_evals = MAX_EVALS)
</code></pre>
      <h4 id="2-get-the-details-of-the-process-from-trials-csv-file">
       2. Get the details of the process. from (trials / csv file)
      </h4>
      <pre><code class="python"># Sort the trials with lowest loss (highest AUC) first
trials_dict = sorted(trials.results, key = lambda x: x['loss'])
trials_dict[:1]

# Or simply from the csv file
results = pd.read_csv(OUT_FILE)
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        6.Evaluate the results using best hyperparameters
       </strong>
      </summary>
      <br/>
      <p>
      </p>
      <pre><code class="python">import ast

def evaluate(results, name):
    """Evaluate model on test data using hyperparameters in results
       Return dataframe of hyperparameters"""

    new_results = results.copy()
    # String to dictionary
    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)

    # Sort with best values on top
    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)

    # Print out cross validation high score
    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))

    # Use best hyperparameters to create a model
    hyperparameters = new_results.loc[0, 'hyperparameters']
    model = lgb.LGBMClassifier(**hyperparameters)

    # Train and make predictions
    model.fit(train_features, train_labels)
    preds = model.predict_proba(test_features)[:, 1]

    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))

    # Create dataframe of hyperparameters
    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))

    # Iterate through each set of hyperparameters that were evaluated
    for i, hyp in enumerate(new_results['hyperparameters']):
        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), 
                               ignore_index = True)

    # Put the iteration and score in the hyperparameter dataframe
    hyp_df['iteration'] = new_results['iteration']
    hyp_df['score'] = new_results['score']

    return hyp_df
bayes_results = evaluate(results, name = 'Bayesian')
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
     <details>
      <summary>
       <strong>
        7.Continue Optimization
       </strong>
      </summary>
      <br/>
      <p>
       <br/>
       <strong>
        NOTE:
       </strong>
       we need to save the
       <code>
        Trials
       </code>
       , in order to let the algorithm learns from previous mistakes. Then we can use it later, to further optimization.
      </p>
      <pre><code class="python">MAX_EVALS = 10

# Continue training
best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,
            max_evals = MAX_EVALS)
To save the Trials object so it can be read in later for more training, we can use the json format.

import json

# Save the trial results
with open('trials.json', 'w') as f:
    f.write(json.dumps(trials_dict))
</code></pre>
      <p>
      </p>
      <br/>
     </details>
    </p>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      12.2 Baysian Optimization (Multi-Class)
     </strong>
    </summary>
    <br/>
    <p>
     <br/>
     <a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/5_Cargo%20Rican%20HouseHold/1_Costa%20Rican%20Household%20Poverty%20Level%20Prediction.html">
      <strong>
       Notebook
      </strong>
     </a>
    </p>
    <h4 id="1-objective-function">
     1. Objective Function:
    </h4>
    <pre><code class="python">from hyperopt import hp, tpe, Trials, fmin, STATUS_OK
from hyperopt.pyll.stochastic import sample
import csv
import ast
from timeit import default_timer as timer
</code></pre>
    <pre><code class="python">def objective(hyperparameters, nfolds=5):
    """Return validation score from hyperparameters for LightGBM"""

    # Keep track of evals
    global ITERATION
    ITERATION += 1

    # Retrieve the subsample
    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)
    subsample_freq = hyperparameters['boosting_type'].get('subsample_freq', 0)

    boosting_type = hyperparameters['boosting_type']['boosting_type']

    if boosting_type == 'dart':
        hyperparameters['drop_rate'] = hyperparameters['boosting_type']['drop_rate']

    # Subsample and subsample frequency to top level keys
    hyperparameters['subsample'] = subsample
    hyperparameters['subsample_freq'] = subsample_freq
    hyperparameters['boosting_type'] = boosting_type

    # Whether or not to use limit maximum depth
    if not hyperparameters['limit_max_depth']:
        hyperparameters['max_depth'] = -1

    # Make sure parameters that need to be integers are integers
    for parameter_name in ['max_depth', 'num_leaves', 'subsample_for_bin', 
                           'min_child_samples', 'subsample_freq']:
        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])

    if 'n_estimators' in hyperparameters:
        del hyperparameters['n_estimators']

    # Using stratified kfold cross validation
    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)

    # Convert to arrays for indexing
    features = np.array(train_selected)
    labels = np.array(train_labels).reshape((-1 ))

    valid_scores = []
    best_estimators = []
    run_times = []

    model = lgb.LGBMClassifier(**hyperparameters, class_weight = 'balanced',
                               n_jobs=-1, metric = 'None',
                               n_estimators=10000)

    # Iterate through the folds
    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):

        # Training and validation data
        X_train = features[train_indices]
        X_valid = features[valid_indices]
        y_train = labels[train_indices]
        y_valid = labels[valid_indices]

        start = timer()
        # Train with early stopping
        model.fit(X_train, y_train, early_stopping_rounds = 100, 
                  eval_metric = macro_f1_score, 
                  eval_set = [(X_train, y_train), (X_valid, y_valid)],
                  eval_names = ['train', 'valid'],
                  verbose = 400)
        end = timer()
        # Record the validation fold score
        valid_scores.append(model.best_score_['valid']['macro_f1'])
        best_estimators.append(model.best_iteration_)

        run_times.append(end - start)

    score = np.mean(valid_scores)
    score_std = np.std(valid_scores)
    loss = 1 - score

    run_time = np.mean(run_times)
    run_time_std = np.std(run_times)

    estimators = int(np.mean(best_estimators))
    hyperparameters['n_estimators'] = estimators

    # Write to the csv file ('a' means append)
    of_connection = open(OUT_FILE, 'a')
    writer = csv.writer(of_connection)
    writer.writerow([loss, hyperparameters, ITERATION, run_time, score, score_std])
    of_connection.close()

    # Display progress
    if ITERATION % PROGRESS == 0:
        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')

    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,
            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, 
            'score': score, 'score_std': score_std}
</code></pre>
    <h4 id="2-space">
     2. Space:
    </h4>
    <pre><code class="python"># Define the search space
space = {
    'boosting_type': hp.choice('boosting_type', 
                              [{'boosting_type': 'gbdt', 
                                'subsample': hp.uniform('gdbt_subsample', 0.5, 1),
                                'subsample_freq': hp.quniform('gbdt_subsample_freq', 1, 10, 1)}, 
                               {'boosting_type': 'dart', 
                                 'subsample': hp.uniform('dart_subsample', 0.5, 1),
                                 'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1),
                                 'drop_rate': hp.uniform('dart_drop_rate', 0.1, 0.5)},
                                {'boosting_type': 'goss',
                                 'subsample': 1.0,
                                 'subsample_freq': 0}]),
    'limit_max_depth': hp.choice('limit_max_depth', [True, False]),
    'max_depth': hp.quniform('max_depth', 1, 40, 1),
    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),
    'learning_rate': hp.loguniform('learning_rate', 
                                   np.log(0.025), 
                                   np.log(0.25)),
    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),
    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),
    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),
    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),
    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)
}
sample(space)
</code></pre>
    <h4 id="3-algorithm">
     3. Algorithm:
    </h4>
    <pre><code class="python">algo = tpe.suggest
</code></pre>
    <pre><code class="python"># Record results
trials = Trials()

# Create a file and open a connection
OUT_FILE = 'optimization.csv'
of_connection = open(OUT_FILE, 'w')
writer = csv.writer(of_connection)

MAX_EVALS = 100
PROGRESS = 10
N_FOLDS = 5
ITERATION = 0

# Write column names
headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']
writer.writerow(headers)
of_connection.close()
%%capture --no-display
display("Running Optimization for {} Trials.".format(MAX_EVALS))

# Run optimization
best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,
            max_evals = MAX_EVALS)
</code></pre>
    <h4 id="5-saving-trils-for-later-optimization">
     5. Saving trils for later optimization
    </h4>
    <pre><code class="python">import json

# Save the trial results
with open('trials.json', 'w') as f:
    f.write(json.dumps(str(trials)))
</code></pre>
    <h4 id="6-using-the-optimized-model">
     6. Using the optimized model.
    </h4>
    <pre><code class="python">results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()
results.head()


### Plot the optimization process
plt.figure(figsize = (8, 6))
sns.regplot('iteration', 'score', data = results);
plt.title("Optimization Scores");
plt.xticks(list(range(1, results['iteration'].max() + 1, 3)));

### Convert the best hyperparameter to dictionary to be used.
best_hyp = ast.literal_eval(results.loc[0, 'hyperparameters'])


%%capture
submission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, 
                                             test_selected, test_ids, 
                                             nfolds = 10, return_preds=False)
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>