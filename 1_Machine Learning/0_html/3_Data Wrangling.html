<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>3. Data Wrangling</h1>
<div style='width:1000px;margin:auto'>

<details><summary><b>Python</b></summary><p>

<details><summary> From <b>list of lists</b> to <b>list of items</b></summary><p>

<pre><code>list_of_lists = [[1, 2, 3, 4], [5, 6, 76], [123, 12, 123, 123,124123,123,123]]
list_elements = sum(terms, [])
</code></pre>

</p></details>

<details><summary> <b>See Files in Current Directory</b> </summary><p>

<pre><code>import os
print(os.listdir(&quot;../input&quot;))
</code></pre>

</p></details>

<details><summary> <b>Difference</b> & <b>Intersection</b> & <b>Combination</b> b/w lists</summary><p>

<pre><code>a = [1, 2, 3, 4, 5]
b = [4, 5, 5, 6, 5, 6]

### 1. Existing in (a) but NOT in (b)
# Returns a set.
set(a).difference(b) # {1, 2, 3}
set(a) - set(b)          # {1, 2, 3}
# Returns array
np.setdiff1d(a, b)     # array([1, 2, 3])

### 2. Intersection b/w (a) and (b)
# Returns a set.
set(a).intersection(b)     # {4, 5}

# Returns an array.
np.intersect1d(a, b)       # array([4, 5])

### 3. All of them together.
a + b           
</code></pre>

</p></details>
<details><summary> <b>Partial Function</b> [python 3] </summary><p>

<pre><code>from functools import partial 

# A normal function 
def add(a, b, c): 
    return 100 * a + 10 * b + c 

# A partial function with b = 1 and c = 2 
add_part = partial(add, c = 2, b = 1) 

# Calling partial function 
print(add_part(3)) 
</code></pre>

</p></details>
<details><summary> <b>f-Literal</b> </summary><p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/NLP%20with%20Python%20-%20Udemy/00-Python-Text-Basics/00-Working-with-Text-Files.html#Formatted-String-Literals-(f-strings)">Formatted String Literal</a> </p>
</p></details>


</p></details><hr>

<details><summary><b>Numpy</b> </summary><p>

<details><summary><b>Concatenate [c_]</b> in Numpy </summary><p>

<pre><code># Example 1.
np.c_[[1, 2, 3], [5, 6, 7]]
# array([[1, 4],
#          [2, 5],
#              [3, 6]])

# Example 2.
np.c_[[[1, 2, 3]], 0, 1, [[4, 5, 6]]]
# array([[1, 2, 3, 0, 1, 4, 5, 6]])
</code></pre>

</p></details>
<details><summary>Add <b>new dimension</b> to vector/matrix [np.array] </summary><p>

<pre><code>#### Trick number #1:
# Add a second dimension
# This is like x[:, np.newaxis] == x[:, None]
x[np.newaxis].shape, x[None].shape

#### Trick number #2:
# Add a new dimension at last.
# This is much better/safe approch
x[..., None].shape, x[..., np.newaxis].shape


#### Trick number #3:
# First parameter is the array/tensor
# Second parameter is the position where you want to add.
# ex:  BEFORE: x.shape --&gt; (4,)
#        AFTER:    x.shape --&gt; (1, 4)
np.expand_dims(x, 0)
</code></pre>

</p></details>

</p></details><hr>

<details><summary><b>DataFrame</b></summary><p>

<details><summary><b>Better Visualization for Sparse Matrix/Dataframe</b></summary><p>

<pre><code># By simpling replacing 0 with ''
df[df==0] = 0
df
</code></pre>

</p></details>

<details><summary>From <b>Normal Dataframe</b> to <b> Similarity Matrix</b></summary><p>

<h4>1. Create graph dataframe</h4>

<pre><code>col_index = &quot;person&quot;
col_value = &quot;docs&quot;

index1, index2, n_values = [], [], []
index_value = df.groupby(col_index)[col_value].apply(pd.Series.unique).to_dict()

for p1, p2 in itertools.permutations(np.unique(df[col_index].values), 2):
    index1.append(p1)
    index2.append(p2)
    n_values.append(len(set(index_value[p1]).intersection(index_value[p2])))

# Create a dataframe has columns [&quot;index1&quot;, &quot;index2&quot;, &quot;common_values_b/w_them&quot;]
index_df = pd.DataFrame({'index1': index1, 'index2': index2, 'n_values': n_values})
</code></pre>


<h4>2. Create the Similarity Matrix</h4>

<pre><code>index_df = pd.pivot(index_df, index='index1', columns='index2', values='n_values')
</code></pre>


<h4>3. Visualize it with Heatmap (if possible)</h4>

<pre><code>plt.figure(figsize=(10, 8))
sns.heatmap(person_df, cmap='viridis')
plt.title(&quot;People Correlation&quot;, size=30, y=1.05)
plt.xticks(size=16)
plt.yticks(size=16);
</code></pre>

</p></details>

<details><summary>From <b>One-Hot Encoding</b> to <b> Unpiovt Table</b></summary><p>
<h4>1. Convert array of labels in a raw to One-Hot Encoding</h4>

<pre><code>df  = df_eng
col = &quot;MoreSamples&quot;

from sklearn.preprocessing import MultiLabelBinarizer

binarizer = MultiLabelBinarizer()
samples = binarizer.fit_transform(df[col].values)
</code></pre>


<h4>2. Create the One-Hot encoding dataframe</h4>

<pre><code>samples_df = pd.DataFrame(samples, columns=binarizer.classes_)
df = pd.concat([df.reset_index(), samples_df.reset_index()], axis=1).drop([&quot;index&quot;, col], axis=1)
</code></pre>


<h4>3. Unpivot the One-Hot encoding dataframe</h4>

<pre><code># Change &quot;DisplayName&quot; to your columns to be used as index.
df = pd.melt(df, id_vars=[&quot;DisplayName&quot;])
df = df_eng[(df.value == 1)]
df.drop(&quot;value&quot;, axis=1, inplace=True)
</code></pre>

</p></details>

<details><summary>Show <b>Thousands comma seperator</b> in dataframe </summary><p>

<pre><code>df = pd.read_csv(&quot;file.csv&quot;, thousands=&quot;,&quot;)
</code></pre>

</p></details>

<details><summary> <b>Change value of cell in dataframe</b> </summary><p>

<pre><code># using .at
news_df.at[idx, 'word'] = operations[operation_idx](random_row.word)
</code></pre>

</p></details>
<details><summary> Rename<b> Repeated</b> columns </summary>

<pre><code class="python">cols = []
col_name = &quot;Grill&quot;
count = 1
for column in X.columns:
    if column == col_name:
        cols.append(f'{col_name}_{count}')
        count+=1
        continue
    cols.append(column)
X.columns = cols
</code></pre>

</details>
<details><summary> <b>Display</b> Multiple dataframes</summary><p>

<pre><code>import IPython

def display(*dfs, head=True):
    for df in dfs:
        IPython.display.display(df.head() if head else df)
</code></pre>

</p></details>
<details><summary> <b>Chain</b> of <b>Functions [pipe()]</b></summary><p>

<pre><code class="python"># using pipe, we can chain functions on dataframe or series.
prices = pd.read_csv(f&quot;{INPUT_DIR}/sell_prices.csv&quot;).pipe(reduce_mem_usage)
</code></pre>

</p></details>
<details><summary> Create <b>DataFrame for Testing</b> </summary><p>

<pre><code># Import pandas
import pandas as pd

# Create the testing dataframe.
test_df = pd.util.testing.makeMixedDataFrame()
test_df = pd.util.testing.makeDataFrame()
test_df = pd.util.testing.makeMissingDataframe()
test_df = pd.util.testing.makeTimeDataFrame()
test_df = pd.util.testing.makePeriodFrame()
</code></pre>

</p></details>
<details><summary> <b>Relationship Table</b> b/w <b> 2 categorical features</b></summary><p>

<pre><code>table = pd.crosstab(df.label, df.flow_id, normalize='columns'); table
</code></pre>

</p></details>
<details><summary> Return columns have <b>NaNs or Infinite</b> values</summary><p>

<pre><code>def return_cols_have_inf(df):
    return [col for col in df if np.isfinite(df[col]).sum() != df.shape[0]]

def return_cols_have_nan(df):
    return [col for col in df if np.isnan(df[col]).sum()]
</code></pre>

</p></details>
<details><summary><b>A series of arrays</b> to <b>DataFrame</b> </summary><p>

<pre><code class="python">X_train = X_train.apply(pd.Series)
</code></pre>

</p></details>
<details><summary> <b>Progress Bar</b> for <b>Pandas Operations</b> </summary><p>

<pre><code class="python">from tqdm import tqdm
tqdm.pandas()

temp = tweet.text.progress_apply(len)
</code></pre>

</p></details>
<details><summary> <b>Select All Columns EXCEPT specific columns</b> </summary>
<p>

<pre><code class="python"># Option 1
df.loc[:, df.columns != 'b']

# Option 2
df.drop('b', axis=1)

# Option 3
df[df.columns.difference(['b'])]

# Option 4
df.loc[:, ~df.columns.isin(['col1', 'col2'])]

# Option 5
df[map(lambda x :x not in ['b'], list(df.columns))]
</code></pre>

</p></details>

<details><summary> <b>Save & Remove label at the same time</b> </summary><p>

<pre><code>label = iris.pop('species')
</code></pre>

</p></details>
<details><summary> <b>Add Prefix or Suffix to all columns name</b> </summary><p>

<pre><code class="python"># Add Prefix
df.add_prefix('X_')

# Add Suffix
df.add_suffix('_Y')
</code></pre>

</p></details>
<details><summary> <b>Create Rare Category</b> </summary><p>

<pre><code class="python"># See the value counts for each category
genre.value_counts()

# Select the top n categories
top_four = genre.value_counts().nlargest(4).index
top_four

# Add Rare category
genre_updated = genre.where(genre.isin(top_four), other='Rare')

# See the changes
genre_updated.value_counts()
</code></pre>

</p></details>
<details><summary> <b>Select Multiple Slices of Columns from a DataFrame</b> </summary><p>

<pre><code class="python"># DataFrame
df = pd.DataFrame(np.random.rand(3, 11), columns=list('ABCDEFGHIJk'))

# Option 1
pd.concat([df.loc[:, 'A', 'C'], df.loc[:, 'F'], df.loc[:, 'J':'K']], axis='columns')

# Option 2
df[list(df.columns[0:3]) + list(df.columns[5]) + list(df.columns[9:11])]

# Option 3
df.iloc[:, np.r_[0:3, 5, 9:11]]

</code></pre>

</p></details>
<details><summary> <b>Remove Duplicated cat/num features</b> </summary><p>

<pre><code class="python">train_enc = pd.DataFrame(index=train_reduced.index)

for col in tqdm_notebook(traintest.columns):
    train_enc[col] = train_reduced[col].factorize()[0]
</code></pre>



<pre><code class="python">dup_cols = {}

for i, c1 in enumerate(tqdm_notebook(train_enc.columns)):
    for c2 in train_enc.columns[i+1:]:
        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):
            dup_cols[c2] = c1
</code></pre>


<h4> Drop them </h4>

<pre><code class="python">traintest.drop(dup_cols.keys(), axis=1, inplace=True)
</code></pre>

</p></details>
<details><summary>See If which features have <b>Differencet Distrubtion</b> in <b>traing</b> and <b>test</b> datasets <b>(KS Test)</b> </summary>
<p>

<p><a href="https://www.kaggle.com/alexpengxiao/preprocessing-model-averaging-by-xgb-lgb-1-39"><b>Credits</b></a> </p>

<pre><code class="python">from scipy.stats import ks_2samp
THRESHOLD_P_VALUE = 0.01 #need tuned
THRESHOLD_STATISTIC = 0.3 #need tuned
diff_cols = []
for col in train.columns:
    statistic, pvalue = ks_2samp(train[col].values, test[col].values)
    if pvalue &lt;= THRESHOLD_P_VALUE and np.abs(statistic) &gt; THRESHOLD_STATISTIC:
        diff_cols.append(col)
for col in diff_cols:
    if col in train.columns:
        train.drop(col, axis=1, inplace=True)
        test.drop(col, axis=1, inplace=True)
train.shape
</code></pre>

</p></details>
<details><summary> Read <b>specific</b> # rows <b>(if data is large)</b> </summary><p>

<pre><code class="python">features_sample = pd.read_csv('../input/home-credit-default-risk-feature-tools/feature_matrix.csv', nrows = 20000)
</code></pre>

</p></details>
<details><summary> Show <b>a specific number of columns in (df.head())</b> </summary><p>

<pre><code class="python">pd.options.display.max_columns = 1700
</code></pre>

</p></details>
<details><summary> <b>De-Ananomitizing</b> </summary><p>
[<b>Example</b>](file:///media/mosaab/Volume/Courses/Computer%20Science/Advanced/Machine%20Learning/[FreeCoursesOnline.Me]%20Coursera%20-%20How%20to%20Win%20a%20Data%20Science%20Competition%20%20Learn%20from%20Top%20Kagglers/008.Exploratory%20data%20analysis/Ananomized%20Data%20&%20Visualization.html#Importing,-Importing,-Importing:) 
</p></details>
<details><summary> Show <b># of Unique Values</b> for each <b>Column</b> </summary><p>

<pre><code class="python"># Number of unique classes in each object column
app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)

#### Result #####
NAME_CONTRACT_TYPE             2
CODE_GENDER                    3
FLAG_OWN_CAR                   2
FLAG_OWN_REALTY                2
NAME_TYPE_SUITE                7
NAME_INCOME_TYPE               8
NAME_EDUCATION_TYPE            5
NAME_FAMILY_STATUS             6
</code></pre>

</p></details>
<details><summary> Rename <b>Columns Name</b> </summary><p>

<pre><code>rename = {'Column Name 1':'New Name 1', 
    'Column Name 2': 'New Name 2'}
data.rename(index=str, columns=rename, inplace=True)
</code></pre>

</p></details>
<details><summary> Show a <b>Beautiful</b> Statistical Result </summary>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/1_Titanic%20Survival/EDA%20To%20Prediction(DieTanic).html#Embarked--%3E-Categorical-Value">See the <b>Result</b></a> <br>
<a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html">See the <b>Doc</b></a> </p>
<p>

<pre><code>data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')

data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band

pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')
</code></pre>

</p></details>
<details><summary> <b>Word Cloud</b></summary>
<p style="margin: 0">

<pre><code class="python">from wordcloud import WordCloud, STOPWORDS

# textn_w is your list of words.
wc = WordCloud(width=1440, height=1080, background_color='black',
               max_words=len(textn_w), stopwords=set(STOPWORDS)
wc.generate(textn_w)
print(bg('Word Cloud for non_duplicate Questions Pairs:', 'str', 'green'))
plt.figure(figsize=(20, 15))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off');
</code></pre>


<h4> 2. Generate from Dictionary (tag, number of occurances)</h4>
[<b>Notebook</b>](file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Applied%20ML%20Course/0_Code/0_Case%20Studies/4_Stackoverflow%20Tag%20Predictor/1_Course%20Code/SO_Tag_Predictor.html) 

<pre><code class="python"># Lets first convert the 'result' dictionary to 'list of tuples'
tup = dict(result.items())
#Initializing WordCloud using frequencies of tags.
wordcloud = WordCloud(    background_color='black',
                          width=1600,
                          height=800,
                          stopwords=set(STOPWORDS),
                    ).generate_from_frequencies(tup)

fig = plt.figure(figsize=(30,20))
plt.imshow(wordcloud)
plt.axis('off')
plt.tight_layout(pad=0)
fig.savefig(&quot;tag.png&quot;)
plt.show()
</code></pre>

</p>
</details>
<details><summary>From <b>String</b> to <b>Date</b></summary>
<p style="margin: 0">
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Data%20Science/8_First%20Capstone%20Project/911%20Calls%20Data%20Capstone%20Project-Mosaab.html#From-String-to-Datetime">See <b>Code</b> in notebook</a> </p>

<pre><code class="python">df['timeStamp'] = pd.to_datetime(df['timeStamp'])
</code></pre>

</p>
</details>


<details><summary>Convert a Column to Type (<b>Int</b>)</summary>
<p style="margin: 0">
[See <b>Code</b> in Kaggle](https://www.kaggle.com/jemseow/machine-learning-to-predict-app-ratings) 

<pre><code class="python"># convert reviews to numeric
df['Reviews'] = df['Reviews'].astype(int)
</code></pre>

</p>
</details>

<details><summary><b>Align Training & Testing data with same columns</b></summary>
<p>
[see <b>results</b>](file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/3_Home%20Credit%20Loans/1_Start%20Here:%20A%20Gentle%20Introduction.html#Aligning-Training-and-Testing-Data) 

<pre><code class="python">train_labels = app_train['TARGET']

# Align the training and testing data, keep only columns present in both dataframes
app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)

# Add the target back in
app_train['TARGET'] = train_labels

print('Training Features shape: ', app_train.shape)
print('Testing Features shape: ', app_test.shape)
</code></pre>

</p>
</details>


<details><summary>Show <b>Top Correlated Features</b> with <b>TARGET</b></summary>
<p>

<pre><code class="python"># Function to calculate correlations with the target for a dataframe
def target_corrs(df, target):

    # List of correlations
    corrs = []

    # Iterate through the columns 
    for col in df.columns:
        print(col)
        # Skip the target column
        if col != target:
            # Calculate correlation with the target
            corr = df[target].corr(df[col])

            # Append the list as a tuple
            corrs.append((col, corr))

    # Sort by absolute magnitude of correlations
    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)

    return corrs
</code></pre>

</p></details>
<details><summary><b>Add a record to a DataFrame</b></summary><p>

<pre><code>df = train.append(test, ignore_index=True)
</code></pre>

</p></details>

</p></details><hr>

<details><summary><b>Misc</b></summary><p>

<details><summary> Upload <b> files</b> into colab </summary>

<pre><code class="python">from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file &quot;{name}&quot; with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
</code></pre>

</details>
<details><summary> <b>Sparse Matrix</b> </summary><p>
<p><a href="./3_data_wrangling/3-logreg-nb-imdb.html#5.-Sparse-Matrix-Representation">FastAI tutorials on Sparse Matrix</a> </p>
</p></details>

<details><summary><b>Settings for Plotting</b> </summary><p>

<pre><code>import matplotlib.pyplot as plt

def set_plot_sizes(sml, med, big):
    plt.rc('font', size=sml)          # controls default text sizes
    plt.rc('axes', titlesize=sml)     # fontsize of the axes title
    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels
    plt.rc('legend', fontsize=sml)    # legend fontsize
    plt.rc('figure', titlesize=big)  # fontsize of the figure title

# Usage
set_plot_sizes(12, 14, 16)
</code></pre>

</p></details>
<details><summary> <b>[Profiling]</b> See which command takes the most in a function </summary><p>

<pre><code># Let's run it in RandomForest.
m = RandomForestRegressor(n_jobs=-1)
%prun m.fit(X, y)

# After that you can notice that the following command takes the most time in running m.fit()
# So we make once and use it multiple times.
%time X = np.array(X, dtype=np.float32)
</code></pre>

</p></details>
<details><summary>Save List using <b>Pickle</b> and <b>joblib</b></summary><p>
<h4>1. Save list</h4>

<pre><code class="python"># Pickle
# Save the onehot columns to later use.
with open('onehot_cols.pkl', 'wb') as f:
    pickle.dump(onehot_cols, f)

# Joblib
from sklearn.externals import joblib

joblib.dump(my_model, &quot;my_model.pkl&quot;)
</code></pre>


<h4>2. Read List</h4>

<pre><code class="python"># Pickle
with open('onehot_cols.pkl', 'rb') as f:
    myList = pickle.load(f)

# Joblib
from sklearn.externals import joblib

my_model_loaded = joblib.load(&quot;my_model.pkl&quot;)
</code></pre>


<h4>3. Read or Save (Compact Version) (best)</h4>

<pre><code class="python"># See if frequency encoded colums is there or not.
if os.path.isfile('./freq_cols.pkl'):
    with open('freq_cols.pkl', 'rb') as f: 
        freq_cols = pickle.load(f)
else:
    # Select only the dummy variables.
    freq_cols = [col for col in train.columns if col.startswith('freq')]

    # Save the onehot columns to later use.
    with open('freq_cols.pkl', 'wb') as f:
        pickle.dump(freq_cols, f)
</code></pre>

</p></details>
<details><summary><b>Download & Extract tgz file</b> with Python</summary><p>

<pre><code>import os, tarfile
from six.moves import urllib

# Constants.
DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot;
HOUSING_PATH  = os.path.join(&quot;datasets&quot;, &quot;housing&quot;)
HOUSING_URL   = os.path.join(DOWNLOAD_ROOT, HOUSING_PATH, &quot;housing.tgz&quot;)

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;)
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
</code></pre>

</p></details>


</p></details>
 </div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>