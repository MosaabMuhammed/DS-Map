<!doctype HTML>
<html>

<head>
    <meta charset="utf-8">
    <title>Made with Remarkable!</title>
    <link rel="stylesheet" href="../../prism.css">
</head>

<body>
    <h1>7. Feature Reduction</h1>
    <div style='width:1000px;margin:auto'>

        <details>
            <summary style='font-size:23px;text-decoration:underline'><b>Dimensionality Reduction:</b></summary>
            <p>

                <ul>
                    <li><a href="https://scikit-learn.org/stable/modules/decomposition.html"><b>Linear Methods for Dimensionality Reduction</b></a></li>

                    <li><a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html"><b>Non-Linear Methods for Dimensionality Reduction</b></a>
                        <br></li>

                    <li><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Applied%20ML%20Course/0_Code/8_Dimensionality%20Reduction/mnist_loadData_pca_tsne.html">PCA vs. t-SNE</a></li>
                </ul>

                <details>
                    <summary><b>PCA</b> & <b>ICA</b> & <b>t-SNE</b> & <b>UMAP</b></summary>
                    <p>
                        <p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Kaggle's%20Notebooks/5_Cargo%20Rican%20HouseHold/1_Costa%20Rican%20Household%20Poverty%20Level%20Prediction.html"><b>Notebook</b></a></p>
                        <ul>
                            <li><b>PCA:</b> Principal Components Analysis. Finds the dimensions of greatest variation in the data</li>

                            <li><b>ICA:</b> Independent Components Analysis. Attempts to separate a mutltivariate signal into independent signals.</li>

                            <li><b>TSNE:</b> T-distributed Stochastic Neighbor Embedding. Maps high-dimensional data to a low-dimensional manifold attempting to maintain the local structure within the data. It is a non-linear technique and generally only
                                used for visualization.</li>

                            <li><b>UMAP:</b> Uniform Manifold Approximation and Projection: A relatively new technique that also maps data to a low-dimensional manifold but tries to preserve more global structure than TSNE.</li>
                        </ul>

                        <h4>1. Importing Libraries</h4>

                        <pre class="language-python"><code class="python">from umap import UMAP
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE

n_components = 3

umap = UMAP(n_components=n_components)
pca = PCA(n_components=n_components)
ica = FastICA(n_components=n_components)
tsne = TSNE(n_components=n_components)
</code></pre>


                        <h4> 2. Fitting and Transforming</h4>

                        <pre class="language-python"><code class="python">train_df = train_selected.copy()
test_df = test_selected.copy()

for method, name in zip([umap, pca, ica, tsne], 
                        ['umap', 'pca', 'ica', 'tsne']):

    # TSNE has no transform method
    if name == 'tsne':
        start = timer()
        reduction = method.fit_transform(train_selected)
        end = timer()

    else:
        start = timer()
        reduction = method.fit_transform(train_selected)
        end = timer()

        test_reduction = method.transform(test_selected)

        # Add components to test data
        test_df['%s_c1' % name] = test_reduction[:, 0]
        test_df['%s_c2' % name] = test_reduction[:, 1]
        test_df['%s_c3' % name] = test_reduction[:, 2]

    # Add components to training data for visualization and modeling
    train_df['%s_c1' % name] = reduction[:, 0]
    train_df['%s_c2' % name] = reduction[:, 1]
    train_df['%s_c3' % name] = reduction[:, 2]

    print(f'Method: {name} {round(end - start, 2)} seconds elapsed.')
</code></pre>


                        <h4> 3. Plot it 3D</h4>

                        <pre class="language-python"><code class="python">from mpl_toolkits.mplot3d import Axes3D

def discrete_cmap(N, base_cmap=None):
    &quot;&quot;&quot;Create an N-bin discrete colormap from the specified input map
    Source: https://gist.github.com/jakevdp/91077b0cae40f8f8244a&quot;&quot;&quot;

    base = plt.cm.get_cmap(base_cmap)
    color_list = base(np.linspace(0, 1, N))
    cmap_name = base.name + str(N)
    return base.from_list(cmap_name, color_list, N)

cmap = discrete_cmap(4, base_cmap = plt.cm.RdYlBu)

train_df['label'] = train_labels
</code></pre>



                        <pre class="language-python"><code class="python"># Plot each method
for method, name in zip([umap, pca, ica, tsne], 
                        ['umap', 'pca', 'ica', 'tsne']):

    fig = plt.figure(figsize = (8, 8))
    ax = fig.add_subplot(111, projection='3d')

    p = ax.scatter(train_df['%s_c1' % name], train_df['%s_c2'  % name], train_df['%s_c3'  % name], 
                   c = train_df['label'].astype(int), cmap = cmap)

    plt.title(f'{name.capitalize()}', size = 22)
    fig.colorbar(p, aspect = 4, ticks = [1, 2, 3, 4])
</code></pre>

                    </p>
                </details>

                <details>
                    <summary><b>Factor Analysis</b></summary>

                    <pre class="language-python"><code># Import required libraries
import pandas as pd
from sklearn.datasets import load_iris
from factor_analyzer import FactorAnalyzer
import matplotlib.pyplot as plt
                </code></pre>
                    <h3>Adequacy Test:</h3>
                    <p>
                        Adequacy Test Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means "can we found the factors in the dataset?". There are two methods to check the factorability or sampling adequacy: Bartlett’s
                        Test Kaiser-Meyer-Olkin Test Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant,
                        you should not employ a factor analysis.
                    </p>
                    <pre class="language-python"><code>from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(df)
chi_square_value, p_value
# (18146.065577234807, 0.0)
# In this Bartlett ’s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.
                    </code></pre>
                    <p>
                        Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. Lower proportion
                        id more suitable for factor analysis. KMO values range between 0 and 1. Value of KMO less than 0.6 is considered inadequate. </p>
                    <pre class="language-python"><code>from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(df)
kmo_model
# 0.8486452309468382
# The overall KMO for our data is 0.84, which is excellent. This value indicates that you can proceed with your planned factor analysis.
                    </code></pre>
                    <h3>Choose the number of Factors:</h3>
                    <pre class="language-python"><code># For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues.
# Create factor analysis object and perform factor analysis
fa = FactorAnalyzer()
fa.analyze(df, 25, rotation=None)
# Check Eigenvalues
ev, v = fa.get_eigenvalues()
ev
# 0	5.134311
# 1	2.751887
# 2	2.142702
# 3	1.852328
# 4	1.548163
# 5	1.073582</code></pre>
                    <p>
                        Here, you can see only for 6-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables).
                    </p>
                    <pre class="language-python"><code># Create scree plot using matplotlib
plt.scatter(range(1,df.shape[1]+1),ev)
plt.plot(range(1,df.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()</code></pre>
                    <h3>Performing Factor Analysis</h3>
                    <pre class="language-python"><code># Create factor analysis object and perform factor analysis
fa = FactorAnalyzer()
fa.analyze(df, 6, rotation="varimax")
fa.loadings
Factor1	Factor2	Factor3	Factor4	Factor5	Factor6
A1	0.040783	0.095220	0.048734	-0.113057	-0.530987	0.161216
A2	0.235538	0.033131	0.133714	0.063734	0.661141	-0.006244
A3	0.343008	-0.009621	0.121353	0.033990	0.605933	0.160106
</code></pre>
                    <p>
                        Factor 1 has high factor loadings for E1,E2,E3,E4, and E5 (Extraversion) <br> Factor 2 has high factor loadings for N1,N2,N3,N4, and N5 (Neuroticism) <br> Factor 3 has high factor loadings for C1,C2,C3,C4, and C5 (Conscientiousness)
                        <br> Factor 4 has high factor loadings for O1,O2,O3,O4, and O5 (Opennness) <br> Factor 5 has high factor loadings for A1,A2,A3,A4, and A5 (Agreeableness) <br> Factor 6 has none of the high loagings for any variable and is not easily
                        interpretable. Its good if we take only five factors.
                    </p>
                    <h3>Get variance for each factor:</h3>
                    <pre class="language-python"><code># Get variance of each factors
fa.get_factor_variance()

Factor1	Factor2	Factor3	Factor4	Factor5
SS Loadings	2.473090	2.709633	2.041106	1.522153	1.844498
Proportion Var	0.098924	0.108385	0.081644	0.060886	0.073780
Cumulative Var	0.098924	0.207309	0.288953	0.349839	0.423619

# Total 42% cumulative Variance explained by the 5 factors.</code></pre>
                </details>
                <h4>Linear Projection</h4>

                <details>
                    <summary><b>PCA</b></summary>
                    <details>
                        <summary>Code 1</summary>

                        <h4> PCA inside a pipeline</h4>

                        <pre class="language-python"><code class="python">from sklearn.decomposition import PCA
from sklearn.preprocessing import Imputer
from sklearn.pipeline import Pipeline

# Make sure to drop the ids and target
train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])
test = test.drop(columns = ['SK_ID_CURR'])

# Make a pipeline with imputation and pca
pipeline = Pipeline(steps = [('imputer', Imputer(strategy = 'median')),
     ('pca', PCA())])

# Fit and transform on the training data
train_pca = pipeline.fit_transform(train)


# See how each axis preserve variance.
pca.explained_variance_ratio_


# transform the testing data
test_pca = pipeline.transform(test)
</code></pre>


                        <h4> CDF for # of principle componets</h4>

                        <pre class="language-python"><code class="python"># Extract the pca object
pca = pipeline.named_steps['pca']

# Plot the cumulative variance explained

plt.figure(figsize = (10, 8))
plt.plot(list(range(train.shape[1])), np.cumsum(pca.explained_variance_ratio_), 'r-')
plt.xlabel('Number of PC'); plt.ylabel('Cumulative Variance Explained');
plt.title('Cumulative Variance Explained with PCA');
</code></pre>


                        <h4> Visualizing the 2 components</h4>

                        <pre class="language-python"><code class="python"># Dataframe of pca results
pca_df = pd.DataFrame({'pc_1': train_pca[:, 0], 'pc_2': train_pca[:, 1], 'target': train_labels})

# Plot pc2 vs pc1 colored by target
sns.lmplot('pc_1', 'pc_2', data = pca_df, hue = 'target', fit_reg=False, size = 10)
plt.title('PC2 vs PC1 by Target');
</code></pre>


                        <h4> How much those components preserve from the data</h4>

                        <pre class="language-python"><code class="python">print('2 principal components account for {:.4f}% of the variance.'.format(100 * np.sum(pca.explained_variance_ratio_[:2])))
</code></pre>

                    </details>
                    <details>
                        <summary>Code 2</summary>
                        <pre class="language-python"><code>from sklearn.decomposition import PCA

n_components = 784
whiten       = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, \
                    random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)

# Percentage of Variance Captured by 784 principal components
print("Variance Explained by all 784 principal components: ", \
                sum(pca.explained_variance_ratio_))
# Variance Explained by all 784 principal components: 0.9999999999999997</code></pre>
                        <pre class="language-python"><code># Percentage of Variance Captured by X principal components
importanceOfPrincipalComponents = \
                    pd.DataFrame(data=pca.explained_variance_ratio_)
importanceOfPrincipalComponents = importanceOfPrincipalComponents.T

print('Variance Captured by First 10 Principal Components: ',
                importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)
print('Variance Captured by First 20 Principal Components: ',
                importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)
print('Variance Captured by First 50 Principal Components: ',
                importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)
print('Variance Captured by First 100 Principal Components: ',
                importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)
print('Variance Captured by First 200 Principal Components: ',
                importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)
print('Variance Captured by First 300 Principal Components: ',
                importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)

#  Variance Captured by First 10 Principal Components: [0.48876238]
# Variance Captured by First 20 Principal Components: [0.64398025]
# Variance Captured by First 50 Principal Components: [0.8248609]
# Variance Captured by First 100 Principal Components: [0.91465857]
# Variance Captured by First 200 Principal Components: [0.96650076]
# Variance Captured by First 300 Principal Components: [0.9862489]
                        </code></pre>
                        <pre class="language-python"><code># Define scatterplot function 
def scatterPlot(xDF, yDF, algoName):
    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)
    tempDF = pd.concat((tempDF,yDF), axis=1, join="inner")
    tempDF.columns = ["First Vector", "Second Vector", "Label"]
    sns.lmplot(x="First Vector", y="Second Vector", hue="Label", \
                data=tempDF, fit_reg=False)
    ax = plt.gca()
    ax.set_title("Separation of Observations using "+algoName)</code></pre>
                        <img src="./imgs/pca.png" alt="">
                    </details>
                </details>

                <details>
                    <summary><b>Multiple Correspondence Analysis (MCA)</b></summary>
                    <p>
                        <h4>It's like PCA for categorical features</h4>

                        <pre class="language-python"><code>!pip install prince
import prince

# Let's try drawing first by extracting only 2 components
mca = prince.MCA(n_components=2)
mca.fit(X_train_org[:1000])

# Transform the data
X_train_mca = mca.transform(X_train_org[:1000])
</code></pre>


                        <pre class="language-python"><code># Let's plot the generated data
ax = mca.plot_coordinates(
    X=X_train_org[:100],
    ax=None,
    figsize=(6, 6),
    show_row_points=True,
    row_points_size=10,
    show_row_labels=False,
    show_column_points=True,
    column_points_size=30,
    show_column_labels=False,
    legend_n_cols=1
)

# To relocate the legend
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);

</code></pre>

                    </p>
                </details>

                <details>
                    <summary><b>Incremental PCA</b></summary>
                    <ul>
                        <li>
                            <details>
                                <summary><b>1. Using np.array_split()</b></summary>
                                <pre class="language-python"><code>from sklearn.decomposition import IncrementalPCA

n_batches = 100
inc_pca = IncrementalPCA(n_componenets=154)

for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)

X_reduced = inc_pca.transform(X_train)
                                </code></pre>
                            </details>
                        </li>


                        <li>
                            <details>
                                <summary><b>2. Using np.memmap()</b></summary>
                                <pre class="language-python"><code># np.memmap allows you to manipulate a large
# array store in a binary file on disk as if it were entirely in memory;
# The class loads only the data it needs in memory, when it needs it.
# 1. Let's create the memmap() structure and copy MNIST data into it. This would typically be done by a first program.
filename = &quot;mnist.data&quot;
m, n = X_train.shape

X_mm = np.memmap(filename, dtype=&quot;float32&quot;, mode=&quot;write&quot;, shape=(m, n))
X_mm[:] = X_train

# Now deleting the memmap() object will trigger its Python finalizer, which ensures that the data is saved to disk.
del X_mm

# Another program would load the data and use it for training.
X_mm = np.memmap(filename, dtype=&quot;float32&quot;, mode=&quot;readonly&quot;, shape=(m, n))

batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)
                                    </code></pre>
                            </details>
                        </li>
                        <li>
                            <details>
                                <summary><b>3. Using Normal fit_transform</b></summary>
                                <pre class="language-python"><code># Incremental PCA
from sklearn.decomposition import IncrementalPCA

n_components = 784
batch_size = None

incrementalPCA = IncrementalPCA(n_components=n_components, \
                                batch_size=batch_size)

X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)
X_train_incrementalPCA = \
    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)

X_validation_incrementalPCA = incrementalPCA.transform(X_validation)
X_validation_incrementalPCA = \
    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)

scatterPlot(X_train_incrementalPCA, y_train, "Incremental PCA")
# check PCA section for scatterPlot implementation
                                    </code></pre>
                            </details>
                        </li>

                    </ul>

                </details>
                <details>
                    <summary><b>Sparse PCA</b></summary>
                    <p>The normal **PCA** algorithm searches for linear combinations in all the input variables, reducing the original feature space as densely as possible. <br> But for some machine learning problems, some degree of sparsity may be preferred.
                        A version of **PCA** that retains some degree of sparsity—controlled by a hyperparameter called alpha—is known as **sparse PCA**. The **sparse** **PCA** algorithm searches for linear combinations in just some of the input variables,
                        reducing the original feature space to some degree but not as compactly as **normal PCA**. this algorithm trains a bit more slowly than normal PCA</p>

                    <pre class="language-python"><code># Sparse PCA
from sklearn.decomposition import SparsePCA

n_components = 100
alpha = 0.0001
random_state = 2018
n_jobs = -1

sparsePCA = SparsePCA(n_components=n_components, \
                alpha=alpha, random_state=random_state, n_jobs=n_jobs)

sparsePCA.fit(X_train.loc[:10000,:])
X_train_sparsePCA = sparsePCA.transform(X_train)
X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)

X_validation_sparsePCA = sparsePCA.transform(X_validation)
X_validation_sparsePCA = \
    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)

scatterPlot(X_train_sparsePCA, y_train, "Sparse PCA")
# check PCA section for scatterPlot implementation
</code></pre>
                </details>

                <details>
                    <summary><b>Kernel PCA</b></summary>
                    <p>

                        <pre class="language-python"><code>from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

clf = Pipeline([
    (&quot;kpca&quot;, KernelPCA(n_components=2)),
    (&quot;log_reg&quot;, LogisticRegression())
])

param_grid = [{
    &quot;kpca__gamma&quot;: np.linspace(0.03, .05, 10),
    &quot;kpca__kernel&quot;: [&quot;rbf&quot;, &quot;sigmoid&quot;]
}]

grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)

# Print the best hyperparameters.
print(grid_search.best_params_)
</code></pre>

                    </p>
                </details>
                <details>
                    <summary><b>Trucated SVD</b></summary>

                    <pre class="language-python"><code># Singular Value Decomposition
from sklearn.decomposition import TruncatedSVD

n_components = 200
algorithm = 'randomized'
n_iter = 5
random_state = 2018

svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \
                    n_iter=n_iter, random_state=random_state)

X_train_svd = svd.fit_transform(X_train)
X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)

X_validation_svd = svd.transform(X_validation)
X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)

scatterPlot(X_train_svd, y_train, "Singular Value Decomposition")
# check PCA section for scatterPlot implementation
</code></pre>
                </details>

                <details>
                    <summary><b>Gaussian Random Projection</b></summary>
                    <p>
                        <p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.GaussianRandomProjection.html#sklearn-random-projection-gaussianrandomprojection"><b>Gaussian Random Projection</b></a> </p>
                        <p>For Gaussian random projection, we can either specify the number of components we would like to have in the reduced feature space, or we can set the hyperparameter eps. The eps controls the quality of the embedding according to
                            the Johnson–Lindenstrauss lemma, where smaller values generate a higher number of dimensions.</p>

                        <pre class="language-python"><code class="python"># Gaussian Random Projection
from sklearn.random_projection import GaussianRandomProjection

n_components = 'auto'
eps = 0.5
random_state = 2018

GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \
                                random_state=random_state)

X_train_GRP = GRP.fit_transform(X_train)
X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)

X_validation_GRP = GRP.transform(X_validation)
X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)

scatterPlot(X_train_GRP, y_train, "Gaussian Random Projection")
</code></pre>

                    </p>
                </details>

                <details>
                    <summary><b>Sparse Random Projection</b></summary>
                    <p>
                        <p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"><b>SparseRandomProjection</b></a> </p>

                        <pre class="language-python"><code class="python"># Sparse Random Projection
from sklearn.random_projection import SparseRandomProjection

n_components = 'auto'
density = 'auto'
eps = 0.5
dense_output = False
random_state = 2018

SRP = SparseRandomProjection(n_components=n_components, \
        density=density, eps=eps, dense_output=dense_output, \
        random_state=random_state)

X_train_SRP = SRP.fit_transform(X_train)
X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)

X_validation_SRP = SRP.transform(X_validation)
X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)

scatterPlot(X_train_SRP, y_train, "Sparse Random Projection")
</code></pre>
                    </p>
                </details>
                <h4>Manifold Learning</h4>

                <details>
                    <summary><b>Isomap</b></summary>
                    <p>Isomap is one type of manifold learning approach. This algorithm learns the intrinsic geometry of the data manifold by estimating the geodesic or curved distance between each point and its neighbors rather than the Euclidean distance.
                        Isomap uses this to then embed the original high-dimensional space into a low- dimensional one.</p>

                    <pre class="language-python"><code>from sklearn.manifold import Isomap

n_neighbors = 5
n_components = 10
n_jobs = 4

isomap = Isomap(n_neighbors=n_neighbors, \
                n_components=n_components, n_jobs=n_jobs)

isomap.fit(X_train.loc[0:5000,:])
X_train_isomap = isomap.transform(X_train)
X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)

X_validation_isomap = isomap.transform(X_validation)
X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \
                                    index=validation_index)

scatterPlot(X_train_isomap, y_train, "Isomap")
# check this function from PCA section.
</code></pre>
                </details>

                <details>
                    <summary><b>Multi-Dimensional Scaling (MDS)</b></summary>
                    <p>Multidimensional scaling (MDS) is a form of nonlinear dimensionality reduction that learns the similarity of points in the original dataset and, using this similarity learning, models this in a lower dimensional space</p>
                    <pre class="language-python"><code>from sklearn.manifold import MDS

n_components = 2
n_init = 12
max_iter = 1200
metric = True
n_jobs = 4
random_state = 2018

mds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \
            metric=metric, n_jobs=n_jobs, random_state=random_state)

X_train_mds = mds.fit_transform(X_train.loc[0:1000,:])
X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])

scatterPlot(X_train_mds, y_train, "Multidimensional Scaling")
# check this function from PCA section.
</code></pre>
                </details>

                <details>
                    <summary><b>Fact ICA</b></summary>
                    <p>

                        <pre class="language-python"><code>from sklearn.decomposition import FastICA
fastica = FastICA(n_components=64,
                    max_iter=5000,
                    random_state=1000)
fastica.fit(X)
</code></pre>

                    </p>
                </details>


                <details>
                    <summary><b>LLE</b> [LocallyLinearEmbedding]</summary>
                    <p>This method preserves distances within local neighborhoods as it projects the data from the original feature space to a reduced space. LLE discovers the nonlinear structure in the original, high-dimensional data by segmenting the data
                        into smaller components (i.e., into neighborhoods of points) and modeling each component as a linear embedding.</p>

                    <pre class="language-python"><code>from sklearn.manifold import LocallyLinearEmbedding

# For this algorithm, we set the number of components we desire and the number of points to consider in a given neighborhood
n_neighbors = 10
n_components = 2
method = 'modified'
n_jobs = 4
random_state = 2018

lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \
        n_components=n_components, method=method, \
        random_state=random_state, n_jobs=n_jobs)

lle.fit(X_train.loc[0:5000,:])
X_train_lle = lle.transform(X_train)
X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)

X_validation_lle = lle.transform(X_validation)
X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)

scatterPlot(X_train_lle, y_train, "Locally Linear Embedding")
</code></pre>
                </details>

                <details>
                    <summary><b>UMAP</b></summary>
                    <p> Uniform Manifold Approximation and Projection: A relatively new technique that also maps data to a low-dimensional manifold but tries to preserve more global structure than TSNE.</p>

                    <pre class="language-python"><code>rom umap import UMAP
n_components = 3

umap = UMAP(n_components=n_components)
</code></pre>
                </details>

                <details>
                    <summary><b>t-SNE</b></summary>
                    <p>
                        <h4>1. Faster Wrapper for t-SNE</h4>

                        <pre class="language-python"><code># !pip install tsne
from tsne import bh_sne

X_2d = bh_sne(train.drop(['subject', 'Activity', 'ActivityName'], axis=1))

# Plot the 2D reduced data
plt.figure(figsize=(12, 8))
sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=train['ActivityName'], alpha=0.5)
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0);
</code></pre>


                        <h4>2. Sklearn Wrapper</h4>

                        <pre class="language-python"><code># Performing t-SNE with sklearn-wrapper
from sklearn.manifold import TSNE

def perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):
    for idx, perplexity in enumerate(perplexities):
        print(f'\nPerforming t-SNE with perplexity {perplexity} and with {n_iter} iterations.')
        X_reduced = TSNE(verbose=2, perplexity=perplexity, n_iter=n_iter, n_jobs=-1).fit_transform(X_data)
        print('Done...')

        # Prepare the data for seaborn
        print('Plot the reduced data...')
        df = pd.DataFrame({'x': X_reduced[:, 0], 'y': X_reduced[:, 1], 'label': y_data})

        # Draw the plot in appropriate place in the grid
        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8, palette='Set1')
        plt.title(f'Perplexity: {perplexity} and max_iterations: {n_iter}')
        plt.show()
</code></pre>


                        <pre class="language-python"><code># Perform the t-SNE function
X_pre_tsne = train.drop(['subject', 'Activity', 'ActivityName'], axis=1)
y_pre_tsne = train['ActivityName']
perform_tsne(X_pre_tsne, y_pre_tsne, perplexities=[2, 5, 10, 10, 50])
</code></pre>

                    </p>
                </details>

                <h4>Others</h4>
                <details>
                    <summary><b>Dictionary Learning</b></summary>
                    <p>An approach known as dictionary learning involves learning the sparse representation of the underlying data. These representative elements are simple, binary vectors (zeros and ones), and each instance in the dataset can be reconstructed
                        as a weighted sum of the representative elements. The matrix (known as the dictionary) that this unsupervised learning generates is mostly populated by zeros with only a few nonzero weights. By creating such a dictionary, this
                        algorithm is able to efficiently identify the most salient representative elements of the original feature space—these are the ones that have the most nonzero weights. The representative elements that are less important will have
                        few nonzero weights. As with **PCA**, dictionary learning is excellent for learning the underlying structure of the data, which will be helpful in separating the data and in identifying interesting patterns.</p>
                    <pre class="language-python"><code>from sklearn.decomposition import MiniBatchDictionaryLearning

n_components = 50
alpha = 1
batch_size = 200
n_iter = 25
random_state = 2018

miniBatchDictLearning = MiniBatchDictionaryLearning( \
                        n_components=n_components, alpha=alpha, \
                        batch_size=batch_size, n_iter=n_iter, \
                        random_state=random_state)

miniBatchDictLearning.fit(X_train.loc[:,:10000])
X_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)
X_train_miniBatchDictLearning = pd.DataFrame( \
    data=X_train_miniBatchDictLearning, index=train_index)

X_validation_miniBatchDictLearning = \
    miniBatchDictLearning.transform(X_validation)
X_validation_miniBatchDictLearning = \
    pd.DataFrame(data=X_validation_miniBatchDictLearning, \
    index=validation_index)

scatterPlot(X_train_miniBatchDictLearning, y_train, \
            "Mini-batch Dictionary Learning")
# check this function from PCA section.
</code></pre>
                </details>

                <details>
                    <summary><b>Dictionary Learning</b></summary>
                    <p> Uniform Manifold Approximation and Projection: A relatively new technique that also maps data to a low-dimensional manifold but tries to preserve more global structure than TSNE.</p>

                    <pre class="language-python"><code>rom umap import UMAP
n_components = 3

umap = UMAP(n_components=n_components)
</code></pre>
                </details>


            </p>
        </details>


        <details>
            <summary style='font-size:23px;text-decoration:underline'><b>Feature Selection:</b></summary>
            <p>

                <details>
                    <summary><b>Filter Methods</b></summary>
                    <p>
                        <ul>
                            <li>
                                <details>
                                    <summary>A/B Testing on one-hot encoded columns with numerical target</summary>
                                    <pre class="language-python"><code>def AB_Test(dataframe, group, target):
    # Packages
    from scipy.stats import shapiro
    import scipy.stats as stats

    # Split A/B
    groupA = dataframe[dataframe[group] == 1][target]
    groupB = dataframe[dataframe[group] == 0][target]

    # Assumption: Normality
    ntA = shapiro(groupA)[1] < 0.05
    ntB = shapiro(groupB)[1] < 0.05
    # H0: Distribution is Normal! - False
    # H1: Distribution is not Normal! - True

    if (ntA == False) & (ntB == False): # "H0: Normal Distribution"
        # Parametric Test
        # Assumption: Homogeneity of variances
        leveneTest = stats.levene(groupA, groupB)[1] < 0.05
        # H0: Homogeneity: False
        # H1: Heterogeneous: True
        
        if leveneTest == False:
            # Homogeneity
            ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]
            # H0: M1 == M2 - False
            # H1: M1 != M2 - True
        else:
            # Heterogeneous
            ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]
            # H0: M1 == M2 - False
            # H1: M1 != M2 - True
    else:
        # Non-Parametric Test
        ttest = stats.mannwhitneyu(groupA, groupB)[1] 
        # H0: M1 == M2 - False
        # H1: M1 != M2 - True
        
    # Result
    temp = pd.DataFrame({
        "AB Hypothesis":[ttest < 0.05], 
        "p-value":[ttest]
    })
    temp["Test Type"] = np.where((ntA == False) & (ntB == False), "Parametric", "Non-Parametric")
    temp["AB Hypothesis"] = np.where(temp["AB Hypothesis"] == False, "Fail to Reject H0", "Reject H0")
    temp["Comment"] = np.where(temp["AB Hypothesis"] == "Fail to Reject H0", "A/B groups are similar!", "A/B groups are not similar!")
    temp["Feature"] = group
    temp["GroupA_mean"] = groupA.mean()
    temp["GroupB_mean"] = groupB.mean()
    temp["GroupA_median"] = groupA.median()
    temp["GroupB_median"] = groupB.median()

    # Columns
    if (ntA == False) & (ntB == False):
        temp["Homogeneity"] = np.where(leveneTest == False, "Yes", "No")
        temp = temp[["Feature","Test Type", "Homogeneity","AB Hypothesis", "p-value", "Comment", "GroupA_mean", "GroupB_mean", "GroupA_median", "GroupB_median"]]
    else:
        temp = temp[["Feature","Test Type","AB Hypothesis", "p-value", "Comment", "GroupA_mean", "GroupB_mean", "GroupA_median", "GroupB_median"]]

    # Print Hypothesis
    # print("# A/B Testing Hypothesis")
    # print("H0: A == B")
    # print("H1: A != B", "\n")

    return temp

# Apply A/B Testing
he_cols = d.columns[d.columns.str.startswith("events")].tolist() + d.columns[d.columns.str.startswith("holiday")].tolist() + d.columns[d.columns.str.startswith("national")].tolist()+ d.columns[d.columns.str.startswith("local")].tolist()
ab = []
for i in he_cols:
ab.append(AB_Test(dataframe=d[d.sales.notnull()], group = i, target = "sales"))
ab = pd.concat(ab)
ab
</code></pre>
                                </details>
                            </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/03.2_Constant_features.html#Constant-features"><b>Constant Features</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/03.3_Quasi-constant_features.html#Quasi-constant-features"><b>Quasi-Constant Features</b></a> </li>

                            <li>
                                <details>
                                    <summary>Duplicate Features</summary>
                                    <p>
                                        <a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/03.4_Duplicated_features.html#Duplicated-features"><b>Duplicated Features</b></a>

                                        <pre class="language-python"><code># For big data
from itertools import combinations

duplicated_feats = []
for idx, (col_1, col_2) in enumerate(combinations(train.columns, 2)):
    if train[col_1].equals(train[col_2]):
        duplicated_feats.append(col_2)
</code></pre>

                                    </p>
                                </details>
                            </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/03.5_Basic_methods_review.html#Filter-Methods---Basics"><b>Basic Methods Pipeline</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/04.2_Correlation.html#Correlation"><b>Correlation</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/04.3_Basic_methods_plus_correlation_pipeline.html#Basic-methods-plus-correlation-pipeline"><b>Basic Methods + Correlation Pipeline</b></a>                                </li>


                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/05.2_Information_gain.html#Information-gain---mutual-information"><b>Mutual Information</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/05.3_Fisher_score.html#Fisher-Score---chi-square-implementation-in-sklearn"><b>Fisher Score - Chi-Square</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/05.4_Univariate_selection.html#Univariate-feature-selection"><b>Univariate Feature Selection (ANOVA)</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/05.5_Univariate_roc_auc.html#Univariate-roc-auc-or-mse"><b>Univariate ROC-AUC or MSE</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/05.6_Basic_methods_correlation_univariate_rocauc_review.html#Filter-Methods---Basics---Correlations---Univariate-ROC-AUC"><b>Basic Methods + Correlation + others Pipeline</b></a>                                </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/05.7_Bonus_method_used_in_KDD_competition.html#Bonus:-Method-used-in-KDD-2009-competition"><b>Method used in KDD 2009 competition</b></a>                                </li>

                        </ul>
                    </p>
                </details>

                <details>
                    <summary><b>Wrapper Methods</b></summary>
                    <p>
                        <ul>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/06.1_Step_forward.html#Step-forward-feature-selection"><b>Step Forward Feature Selection</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/06.2_Step_backward.html#Step-backward-feature-selection"><b>Step Backward Feature Selection</b></a> </li>

                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/06.3_Exhaustive_feature_selection.html#Exhaustive-feature-selection"><b>Exhaustive Feature Selection</b></a> </li>

                        </ul>
                    </p>
                </details>

                <details>
                    <summary><b>Embedded Methods</b></summary>
                    <p>

                        <details>
                            <summary><b>Linear Models</b></summary>
                            <p>
                                <ul>
                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/07.3_Lasso.html#Lasso-regularisation"><b>LASSO Regularization</b></a> </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/08.2_Logistic_regression_coefficients.html#Logistic-Regression-Coefficients"><b>Logistic Regression Coefficients</b></a>                                        </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/08.3_Regression_coefficients_and_regularisation.html#Regression-Coefficients-are-affected-by-regularisation"><b>Regression Coefficients are affected by regularisation</b></a>                                        </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/08.4_Linear_Regression_coefficients.html#Linear-Regression-Coefficients"><b>Linear Regression Coefficients</b></a> </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/08.5_Feature_selection_with_linear_models_review.html#Feature-selection-with-linear-models,-review"><b>Linear Model Pipeline</b></a>                                        </li>
                                </ul>
                            </p>
                        </details>

                        <details>
                            <summary><b>Tree-based Models</b></summary>
                            <p>
                                <ul>
                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/09.2_Random_forest_importance.html#Random-Forest-importance"><b>Random Forest importance</b></a> </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/09.3_Random_Forest_recursive_feature_selection.html#Recursive-feature-selection-using-random-forests-importance"><b>Recursive feature selection using random forests importance</b></a>                                        </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/09.4_GradientBoosting_importance.html#Gradient-Boosted-trees-importance"><b>Gradient Boosted trees importance</b></a>                                        </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/09.5_Feature_selection_with_decision_trees_review.html#Feature-selection-with-decision-trees,-review"><b>Tree Models - Pipeline/b></a>                                        </li>

                                    <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/09.3_Random_Forest_recursive_feature_selection.html#Recursive-feature-selection-using-random-forests-importance"><b>Recursive feature selection using random forests importance</b></a>                                        </li>
                                </ul>
                            </p>
                        </details>

                    </p>
                </details>
                <details>
                    <summary><b>Hybrid Methods</b></summary>
                    <p>
                        <ul>
                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/11.01_Feature_shuffling.html#Feature-selection-by-random-shuffling"><b>Random Shuffling</b></a> </li>
                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/11.02_Hybrid_Recursive_feature_elimination.html#Hybrid-method:-Recursive-feature-elimination"><b>Recursive Feature Elimination</b></a>                                </li>
                            <li><a href="file:///D:/DS-Map/1_Machine%20Learning/0_html/7_dim_reduction/Feature-selection-notebooks/11.03_Hybrid_Recursive_feature_addition.html#Hybrid-method:-Recursive-feature-addition"><b>Recursive Feature Addition</b></a>                                </li>
                        </ul>
                    </p>
                </details>

                <hr>
                <details>
                    <summary>1. Remove <b>Highly Correlated</b> Features</summary>
                    <p>
                        <h4> Identify Highly Correlated Features</h4>

                        <pre class="language-python"><code class="python"># Threshold for removing correlated variables
threshold = 0.9

# Absolute value correlation matrix
corr_matrix = train.corr().abs()
corr_matrix.head()
</code></pre>

                        <h4> Drop the columns</h4>

                        <pre class="language-python"><code class="python"># Create correlation matrix
corr_matrix = df.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] &gt; threshold)]

print('There are %d columns to remove.' % (len(to_drop)))

# Drop features 
df.drop(df[to_drop], axis=1)
</code></pre>


                    </p>
                </details>


                <details>
                    <summary><b>Recursive Feature Elimination CV</b></summary>
                    <p>
                        <p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Sklearn/sklearn.feature_selection.RFECV.html#sklearn-feature-selection-rfecv"><span style='color:#333'><b> 2. Recursive Feature Elimination method</b></span></a></p>


                        <pre class="language-python"><code class="python">from sklearn.feature_selection import RFECV

# Create a model for feature selection
estimator = RandomForestClassifier(random_state = 10, n_estimators = 100,  n_jobs = -1)

# Create the object
selector = RFECV(estimator, step = 1, cv = 3, scoring= scorer, n_jobs = -1)
</code></pre>



                        <pre class="language-python"><code class="python">selector.fit(train_set, train_labels)
</code></pre>



                        <pre class="language-python"><code class="python">plt.plot(selector.grid_scores_);

plt.xlabel('Number of Features'); plt.ylabel('Macro F1 Score'); plt.title('Feature Selection Scores');
selector.n_features_
</code></pre>



                        <pre class="language-python"><code class="python">rankings = pd.DataFrame({'feature': list(train_set.columns), 'rank': list(selector.ranking_)}).sort_values('rank')
rankings.head(10)
</code></pre>



                        <pre class="language-python"><code class="python">train_selected = selector.transform(train_set)
test_selected = selector.transform(test_set)
# Convert back to dataframe
selected_features = train_set.columns[np.where(selector.ranking_==1)]
train_selected = pd.DataFrame(train_selected, columns = selected_features)
test_selected = pd.DataFrame(test_selected, columns = selected_features)
</code></pre>

                    </p>
                </details>

                <ul>
                    <li>
                        <p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Sklearn/Step%20Forward%20Feature%20Selection_%20A%20Practical%20Example%20in%20Python.html"><span style='color:#333'><b>3. Forward Feature  Selection<b></span></a>                            </p>
                    </li>

                    <li>
                        <p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/How%20to%20win%20a%20Data%20Science%20Competition/compute_KNN_features.html#Load-data"><span style='color:#333'><b>4. Nearest Neighbors for Feature Extraction<b></span></a></p>
                    </li>
                </ul>


            </p>
        </details>

    </div>
    <script src="../../prism.js"></script>

</body>

</html>