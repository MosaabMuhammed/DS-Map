<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>8. Metrices</h1>
<details><summary style='font-size:18px;color:red'> <b>1. Important Functions</b></summary>
<p>

<pre><code># This function plots the confusion matrices given y_i, y_i_hat.
# NOTE: make sure the predicted labels are NOT probabilities.
# predicted_y =np.argmax(test_predicted_y, axis=1)
# plot_confusion_matrix(y_test, predicted_y+1)
def plot_confusion_matrix(y_test, y_pred):
  C = confusion_matrix(y_test, y_pred)
  A = (((C.T)/(C.sum(axis=1))).T)
  B = (C/C.sum(axis=0))

  liOfMat    = [C, B, A]
  liOfTitles = ['Confusion Matrix', 'Precision Matrix (Column Sum = 1)', 'Recall Matrix (Row sum = 1)']
  labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # Change this based on problem.

  # Representing matrices in heatmap format.  
  for i, mat in enumerate(liOfMat):
    plt.figure(figsize=(20, 7))
    sns.heatmap(mat, annot=True, cmap='viridis', fmt='.3f', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title(liOfTitles[i])
    plt.show(
</code></pre>

</p></details>

<details><summary> <b>Accuracy Score</b></summary><p>

<pre><code>accuracy = model.score(y_test, y_pred)
</code></pre>

<h4>Manual</h4>

<pre><code>def accuracy(y_true, y_pred):
    correct_counter = 0

    for yt, yp in zip(y_true, y_pred):
        if yt == yp:
            correct_counter += 1
    return correct_counter / len(y_true)
</code></pre>

</p></details>

<details><summary> <b>TP, TN, FP, FN</b></summary><p>

<pre><code># This can be true_positive or true_negative 
# based on the class_label!
def true_positive(y_true, y_pred, class_label):
    tp = 0
    for yt, yp in zip(y_true, y_pred):
        if yt == class_label and yp == class_label:
            tp += 1
    return tp

def false_positive(y_true, y_pred, class_label):
    fp = 0
    for yt, yp in zip(y_true, y_pred):
        if yt != class_label and yp == class_label:
            fp += 1
    return fp

def false_negative(y_true, y_pred, class_label):
    fn = 0
    for yt, yp in zip(y_true, y_pred):
        if yt == class_label and yp != class_label:
            fn += 1
    return fn
</code></pre>

</p></details>

<details><summary> <b>Confusion Matrix</b> </summary><p>

<pre><code>pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

# Another way with Background
pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')
</code></pre>


<pre><code>from sklearn.metrics import confusion_matrix

sns.heatmap(confusion_matrix(y_test, y_pred), cmap='viridis', annot=True)
</code></pre>


<h4>See the errors in each class (see the confusion of the model)</h4>

<pre><code>row_sums     = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums

np.fill_diagonal(norm_conf_mx, 0)
sns.heatmap(norm_conf_mx, cmap=&quot;viridis&quot;)
plt.xlabel(&quot;Predicted Labels&quot;)
plt.ylabel(&quot;True Labels&quot;);
</code></pre>

</p></details>

<details><summary> <b>Classification Report</b> </summary><p>

<pre><code>from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
</code></pre>

</p></details>

<details><summary> <b>Ploting the ROC Curve</b> </summary><p>

<pre><code># Extract the prediction probabilities
y_pred_proba = knn.predict_proba(X_test)[:, 1]

# Calculate the roc_curve
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Generate the plot
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label='knn')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('KNN (n_neighbors = 16) ROC Curve')
</code></pre>

</p></details>

<details><summary> <b>ROC Area Under Curve (AUC)</b> </summary><p>

<pre><code>from sklearn.metrics import roc_auc_score

print('{:.2f}'.format(roc_auc_score(y_test, y_pred_proba)*100))
</code></pre>

</p>
</details>

<details><summary> <b>Confidance Interval</b> </summary><p>

<pre><code>from scipy import stats

confidence = 0.95

squared_errors = (final_predictions - y_test) ** 2

np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))

### Returns
#array([45685.10470776, 49691.25001878])
</code></pre>

</p></details>

<details><summary> <b>Precision</b> </summary><p>
<p><b>Precision</b> is simply the accuracy of the positive predictions.</p>

<pre><code>from sklearn.metrics import precision_score

precision_score(y_train_5, y_train_pred)
</code></pre>


<h4>Manual</h4>

<pre><code>def precision(y_true, y_pred, class_label=0):
    tp = true_positive(y_true, y_pred, class_label)
    fp = false_positive(y_true, y_pred, class_label)
    precision = tp / (tp + fp)
    return precision
</code></pre>

</p></details>

<details><summary> <b>Recall</b> aka <b>True Positive Rate (TPR)</b> aka <b>Sensitivity</b></summary><p>

<pre><code>from sklearn.metrics import recall_score

recall_score(y_train_5, y_train_pred)
</code></pre>


<h4>Manual</h4>

<pre><code>def recall(y_true, y_pred, class_label=0):
    tp = true_positive(y_true, y_pred, class_label)
    fn = false_negative(y_true, y_pred, class_label)
    recall = tp / (tp + fn)
    return recall
</code></pre>

</p></details>

<details><summary> <b>False Positive Rate (FPR)</b></summary><p>

<pre><code>def FPR(y_true, y_pred, class_label=0):
    fp = false_positive(y_true, y_pred, class_label)
    tn = true_negative(y_true, y_pred, class_label)
    return fp / (tn + fp)
</code></pre>

</p></details>

<details><summary> <b>True Negative Rate (TNR)</b> aka <b>Specifity</b></summary><p>

<pre><code>def TNR(y_true, y_pred, class_label=0):
    return 1 - FPR(y_true, y_pred, class_label)
</code></pre>

</p></details>

<details><summary> <b>F1 Score</b> </summary><p>

<pre><code>from sklearn.metrics import f1_score

f1_score(y_train_5, y_train_pred)
</code></pre>

<h4>Manual</h4>

<pre><code>def f1(y_true, y_pred, class_label=0):
    p = precision(y_true, y_pred, class_label)
    r = recall(y_true, y_pred, class_label)
    score = 2 * p * r / (p + r)
    return score
</code></pre>

</p></details>

<details><summary> Plot <b>Precision and Recall </b> Vs <b>Thresholds</b></summary><p>
<h4>1. Calculate the decision function for the dataset.</h4>

<pre><code>y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=&quot;decision_function&quot;)
</code></pre>

<h4>2. Get the precisions, recalls and thresholds</h4>

<pre><code>from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
</code></pre>


<h4>3. Plot it, and take the best threshold</h4>

<pre><code>def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.figure(figsize=(10, 6))
    plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;)
    plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;)
    plt.legend(); plt.grid()
    plt.xlabel(&quot;Thresholds&quot;)

plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
</code></pre>

</p></details>

<details><summary> Plot <b>Precision</b> Vs. <b>Recal</b> </summary><p>
<p><b>NOTE:</b> We prefer PR curve whenever the positive class is rare or when you care more about the <b>false positive</b> than the <b>false negative</b>, otherwise use ROC curve.</p>
<h4>1. Calculate the decision function for the dataset.</h4>

<pre><code>y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=&quot;decision_function&quot;)
</code></pre>

<h4>2. Get the precisions, recalls and thresholds</h4>

<pre><code>from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
</code></pre>


<h4>3. Plot the precisions vs. recall</h4>

<pre><code>plt.figure(figsize=(10, 6))
plt.plot(recalls, precisions)
plt.grid()
plt.xlabel(&quot;Recall&quot;)
plt.ylabel(&quot;Precision&quot;)
plt.title(&quot;Precision vs. Recall&quot;, size=20, y=1.05)
</code></pre>


<h4>4. Choose the threshold based on your business case.</h4>

<pre><code>threshold_90_precision = thresholds[np.argmax(precisions &gt;= .9)]
y_train_pred_90 = (y_scores &gt;= threshold_90_precision)

precision_score(y_train_5, y_train_pred_90),\
recall_score(y_train_5, y_train_pred_90)

# (0.9000345901072293, 0.4799852425751706)
</code></pre>

</p></details>

<details><summary> <b>Log Loss</b></summary><p>
<h4>Manual</h4>

<pre><code>def log_loss(y_true, y_proba):
    # Define an epsilon value.
    # this can also be an input.
    # this value is used to clip probabilities.
    epsilon = 1e-15

    # Iniitialize empty list to store.
    # individual losses.
    loss = []

    # Loop over all true and predicted probabitlity values
    for yt, yp in zip(y_true, y_proba):
        # Adjust probability.
        # 0 gets converted to 1e-15
        # 1 gets converted to 1 - 1e15
        yp = np.clip(yp, epsilon, 1 - epsilon)

        # Calculate loss for one sample
        temp_loss = - 1. * (yt * np.log(yp) + (1 - yt) * np.log(1 - yp))

        # Add to loss list.
        loss.append(temp_loss)
    return np.mean(loss)
</code></pre>


<h4>Using Sklearn</h4>

<pre><code>from sklearn import metrics
metrics.log_loss(y_true, y_proba)
</code></pre>

</p></details>

<ul>
<li>Mean Absolute Error (Regression).</li>
<li>Mean Squared Error (Regression).</li>
<li>Square Root Mean Square Error (Regression).</li>
</ul><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>