<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>EDA</h1>
<div style='width:1000px;margin:auto'>

<details><summary><b style="font-size:20px">n-gram Viewer with Time</b></summary>


<pre><code>fig, ax = plt.subplots(figsize=(9,6))

for term in terms:
    data[term].plot(ax=ax)

ax.set_title(&quot;Token Frequency over Time&quot;)
ax.set_ylabel(&quot;word count&quot;)
ax.set_xlabel(&quot;publication date&quot;)
ax.set_xlim((&quot;2016-02-29&quot;,&quot;2016-05-25&quot;))
ax.legend()
plt.show()

# NOTE: If you don't have time column, you can display it with index, but convert it to bag-of-words first.
</code></pre>

<p><img src="imgs/20200606-153159.png" alt="" /></p>
</details>

<details><summary><b style="font-size:20px">Network Visualization</b></summary>

Visualize the relationship between each pair of words.
<h4>1. Create the corpus</h4>

<pre><code>import itertools

corpus = []
for sentence in df['sample'].values:
    corpus.extend(sentence.split())

def cooccurrence(corpus):
    possible_pairs = list(itertools.combinations(corpus, 2))
    cooccurring    = dict.fromkeys(possible_pairs, 0)
    for idx, current_token in enumerate(corpus):
        if (idx+1 &lt; len(corpus)) and (tuple((current_token, corpus[idx+1])) in possible_pairs):
            cooccurring[(current_token, corpus[idx+1])] += 1
    return cooccurring

pairs = cooccurrence(corpus)
</code></pre>

<h4>2. Create the network</h4>

<pre><code>import networkx as nx

G = nx.Graph()
G.name = &quot;The Social Network of tokens&quot;

# pairs = cooccurrence(corpus)
for pair, wgt in pairs.items():
    if wgt &gt; 0:
        G.add_edge(pair[0], pair[1], weight=wgt)

# Make some the center!
# TODO: WRITE YOUR WORD OF INTEREST HERE!!
D = nx.ego_graph(G, &quot;corona&quot;)
edges, weights = zip(*nx.get_edge_attributes(D, &quot;weight&quot;).items())

# Push nodes away that are less related to that specific word.
pos = nx.spring_layout(D, k=.5, iterations=40)
nx.draw(D, pos, node_color=&quot;gold&quot;, node_size=50, edgelist=edges,
        width=.5, edge_color=&quot;orange&quot;, with_labels=True, font_size=12)
plt.show()
</code></pre>

</details>

<details><summary><b style="font-size:20px">Heatmap</b> b/w <b style="font-size:20px">Tokens</b></summary>

<h4>1. Create the Matrix</h4>

<pre><code>specific_tokens = ['kill', 'corona', 'viris', 'hi', 'symptoms', 'the', 'treatment']

corpus = []
for sentence in df['sample'].values:
    corpus.append(sentence.split())

def cooccurrence_mtx(corpus, specific_tokens=None, sort_alpha=False):
    if sort_alpha: specific_tokens = sorted(specific_tokens)
    possible_pairs = list(itertools.permutations(specific_tokens, 2))
    cooccurring    = dict.fromkeys(possible_pairs, 0)

    for idx, current_sample in enumerate(corpus):
        for pair in possible_pairs:
            if pair[0] in current_sample and pair[1] in current_sample:
                cooccurring[pair] += 1

    pairs = cooccurring.copy()
    mtx = pd.DataFrame(columns=[&quot;p1&quot;, &quot;p2&quot;, &quot;count&quot;])
    for pair, weight in pairs.items():
        mtx = mtx.append({&quot;p1&quot;:    pair[0],
                        &quot;p2&quot;:    pair[1],
                        &quot;count&quot;: float(weight)}, ignore_index=True)

    mtx = pd.pivot(mtx, index='p1', columns='p2', values='count')
    mtx.fillna(.0, inplace=True)
    return mtx

mtx = cooccurrence_mtx(corpus, specific_tokens, True)


mtx.sample(5)
</code></pre>


<h4>2. Show Heatmap</h4>

<pre><code>import matplotlib

fig, ax = plt.subplots()
fig.suptitle(&quot;Co-occurrence of Tokens&quot;, fontsize=12)
fig.subplots_adjust(wspace=.75)

n = len(specific_tokens)
x_tick_marks = np.arange(n)
y_tick_marks = np.arange(n)

ax1 = plt.subplot(121)
ax1.set_xticks(x_tick_marks)
ax1.set_yticks(y_tick_marks)
ax1.set_xticklabels(specific_tokens, fontsize=8, rotation=90)
ax1.set_yticklabels(specific_tokens, fontsize=8)
ax1.xaxis.tick_top()
ax1.set_xlabel(&quot;By Frequency&quot;)
sns.heatmap(mtx, cmap=&quot;viridis&quot;)

# And alphabetically
alpha_cast = sorted(specific_tokens)
alpha_mtx  = 
</code></pre>

</details>

<details><summary><b style="font-size:20px">POS Tagging Coloring</b></summary>

<pre><code># Import required libraries.
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import pos_tag, word_tokenize
from yellowbrick.text.postag import PosTagVisualizer
</code></pre>



<pre><code>pie = &quot;&quot;&quot;
In a small saucepan, combine sugar and eggs
until well blended. Cook over low heat, stirring
constantly, until mixture reaches 160° and coats
the back of a metal spoon. Remove from the heat.
Stir in chocolate and vanilla until smooth. Cool
to lukewarm (90°), stirring occasionally. In a small
bowl, cream butter until light and fluffy. Add cooled
chocolate mixture; beat on high speed for 5 minutes
or until light and fluffy. In another large bowl,
beat cream until it begins to thicken. Add
confectioners' sugar; beat until stiff peaks form.
Fold into chocolate mixture. Pour into crust. Chill
for at least 6 hours before serving. Garnish with
whipped cream and chocolate curls if desired.
&quot;&quot;&quot;

tokens = word_tokenize(pie)
tagged = pos_tag(tokens)

visualizer = PosTagVisualizer()
visualizer.transform(tagged)

print(&quot; &quot;.join((visualizer.colorize(token, color)
                for color, token in visualizer.tagged)))
print(&quot;\n&quot;)
</code></pre>

</details>

<details><summary><b style="font-size:20px">Frequency Distribution</b></summary>

<pre><code>from yellowbrick.text.freqdist import FreqDistVisualizer
from sklearn.feature_extraction.text import CountVectorizer

# &quot;stop_words&quot; parameter removes stopwords.
# Remove the parameter, if you prefer another thing.
vectorizer = CountVectorizer(stop_words=&quot;english&quot;)
docs       = vectorizer.fit_transform(df[&quot;sample&quot;].values)
features   = vectorizer.get_feature_names()

plt.figure(figsize=(15, 8))
visualizer = FreqDistVisualizer(features=features)
visualizer.fit(docs)
visualizer.poof()
</code></pre>

</details>

<details><summary><b style="font-size:20px">t-SNE Visualizer</b></summary>
<p><b>yellowbrick</b> applies a decomosition first (SVD with 50 components by defaults), then performs the t-SNE embedding</p>

<pre><code>from yellowbrick.text import TSNEVisualizer
from sklearn.feature_extraction.text import TfidfVectorizer

# We could use any vectorization technique and not specifically TF-IDF.
tfidf = TfidfVectorizer()
docs  = tfidf.fit_transform(df[&quot;sample&quot;].values)

# We could try PCA instead of SVD, by passing &quot;decompose=&quot;pca&quot;&quot; into TSNEVisualizer().
tsne = TSNEVisualizer()
tsne.fit(docs, y=df[&quot;intent&quot;].values)
tsne.poof()
</code></pre>



<pre><code># Apply clustering instead of class names.
from sklearn.cluster import KMeans
clusters = KMeans(n_clusters=5)
clusters.fit(docs)

tsne = TSNEVisualizer()
tsne.fit(docs, [&quot;c{}&quot;.format(c) for c in clusters.labels_])
tsne.poof()

</code></pre>

</details>
</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>