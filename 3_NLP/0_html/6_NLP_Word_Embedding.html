<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Word Embedding</h1>
<div style='width:1000px;margin:auto'>
<details><summary><b style='font-size:20px'>1. GloVe</b></summary><p>

<details><summary><b style='font-size:20px'>Read Glove</b></summary><p>
<h4>1. Read GloVe Vectors</h4>

<pre><code>embedding_dict = {}
with open('../../../Personal/Development/Courses Docs/GloVe - Pretrained Word Representation/glove.6B.100d.txt') as f:
    for line in f:
        values  = line.split()
        word    = values[0]
        vectors = np.asarray(values[1:], 'float32')
        embedding_dict[word] = vectors

f.close()
</code></pre>


<h4>2. Tokenize & Add Padding</h4>

<pre><code>def create_corpus(df):
    corpus = []

    for text in tqdm(df.text):
        words = [word.lower() for word in word_tokenize(text) if word.isalpha() and word not in stop]
        corpus.append(words)
    return corpus

corpus = create_corpus(tweet)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>Deep Learning for GloVe</b></summary><p>


<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

MAX_LEN = 50
tokensizer = Tokenizer()
tokensizer.fit_on_texts(corpus)
sequences  = tokensizer.texts_to_sequences(corpus)

tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')

# Print number of words
word_index = tokensizer.word_index
print(f'~&gt; Number of Unique words: {bg(len(word_index))}')
</code></pre>


<h4>3. Create Embedding Matrix</h4>

<pre><code>num_words = len(word_index)+1
embedding_matrix = np.zeros((num_words, 100))

for word, i in tqdm(word_index.items()):
    if i &gt; num_words:
        continue

    emb_vec = embedding_dict.get(word)
    if emb_vec is not None:
        embedding_matrix[i] = emb_vec
</code></pre>


<h4>4. LSTM Model with Embedding</h4>

<pre><code>from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from keras.initializers import Constant
from keras.optimizers import Adam

model = Sequential([
    Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN),
    SpatialDropout1D(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='relu')
])
model.summary()
</code></pre>


<pre><code># Compile the model
model.compile(loss='binary_crossentropy',
              optimizer=Adam(learning_rate=1e-5),
              metrics=['acc', f1_m])
</code></pre>


<pre><code># Train Test Split
train = tweet_pad[:tweet.shape[0]]
test  = tweet_pad[tweet.shape[0]:]

X_train, X_test, y_train, y_test = train_test_split(train,
                                                    tweet['target'].values,
                                                    test_size=0.15)

shape(X_train, X_test)
</code></pre>


<pre><code># Fit the model.
history = model.fit(X_train, y_train,
                    batch_size=4,
                    epochs=20,
                    validation_data=(X_test, y_test),
                    verbose=2)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>ML Models for GloVe</b></summary><p>

<h4>5. Make GloVe suitable for ML Models</h4>

<pre><code># Convert each text in a row to a vector.
def sent2vec(s):
    words = str(s).lower()
    words = word_tokenize(words)
    words = [w for w in words if not w in stop and w.isalpha()]

    M = []
    for w in words:
        try:
            M.append(embedding_dict[w])
        except:
            continue

    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray: return np.zeros(100)
    return v / np.sqrt((v**2).sum())

# Apply the function
df['text'] = [sent2vec(x) for x in tqdm(df['text'], position=0)]
</code></pre>


<pre><code># Train Test Split the data
X_train, X_valid, y_train, y_valid = train_test_split(df[df.target.notnull()]['text'],
                                                      df[df.target.notnull()]['target'],
                                                      test_size=.2,
                                                      random_state=33)
</code></pre>


<pre><code># Convert the data into arrays
X_train, X_valid = X_train.apply(pd.Series), X_valid.apply(pd.Series)
</code></pre>

</p></details>
</p></details>

<details><summary><b style='font-size:20px'>2. Universal Sentence Encoding</b></summary><p>
<h4>1. Load the embeddings</h4>

<pre><code># Import the hubber
import tensorflow_hub as hub

# Load the embbedding
embed = hub.load('../../../Personal/Development/Courses Docs/Word Embeddings/Universal Sentence Encoder')
X_train_embedding = embed(train.text.values)
X_test_embedding  = embed(test.text.values)
</code></pre>


<h4>2. Concatenate with other features</h4>

<pre><code># Merge with TF-IDF
train_df    = np.concatenate([X_train_embedding['outputs'], tf_train], axis=1)
test_df     = np.concatenate([X_test_embedding['outputs'], tf_test], axis=1)
</code></pre>

</p></details>
</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>