<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Word Embedding</h1>
<div style='width:1000px;margin:auto'>
<details><summary><b style='font-size:20px'>GloVe</b></summary><p>

<details><summary><b style='font-size:20px'>Read Glove</b></summary><p>
<h4>1. Read GloVe Vectors</h4>

<pre><code>embedding_dict = {}
with open('../../../Personal/Development/Courses Docs/GloVe - Pretrained Word Representation/glove.6B.100d.txt') as f:
    for line in f:
        values  = line.split()
        word    = values[0]
        vectors = np.asarray(values[1:], 'float32')
        embedding_dict[word] = vectors

f.close()
</code></pre>


<h4>2. Tokenize & Add Padding</h4>

<pre><code>def create_corpus(df):
    corpus = []

    for text in tqdm(df.text):
        words = [word.lower() for word in word_tokenize(text) if word.isalpha() and word not in stop]
        corpus.append(words)
    return corpus

corpus = create_corpus(tweet)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>Deep Learning for GloVe</b></summary><p>


<pre><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

MAX_LEN = 50
tokensizer = Tokenizer()
tokensizer.fit_on_texts(corpus)
sequences  = tokensizer.texts_to_sequences(corpus)

tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')

# Print number of words
word_index = tokensizer.word_index
print(f'~&gt; Number of Unique words: {bg(len(word_index))}')
</code></pre>


<h4>3. Create Embedding Matrix</h4>

<pre><code>num_words = len(word_index)+1
embedding_matrix = np.zeros((num_words, 100))

for word, i in tqdm(word_index.items()):
    if i &gt; num_words:
        continue

    emb_vec = embedding_dict.get(word)
    if emb_vec is not None:
        embedding_matrix[i] = emb_vec
</code></pre>


<h4>4. LSTM Model with Embedding</h4>

<pre><code>from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from keras.initializers import Constant
from keras.optimizers import Adam

model = Sequential([
    Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN),
    SpatialDropout1D(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='relu')
])
model.summary()
</code></pre>


<pre><code># Compile the model
model.compile(loss='binary_crossentropy',
              optimizer=Adam(learning_rate=1e-5),
              metrics=['acc', f1_m])
</code></pre>


<pre><code># Train Test Split
train = tweet_pad[:tweet.shape[0]]
test  = tweet_pad[tweet.shape[0]:]

X_train, X_test, y_train, y_test = train_test_split(train,
                                                    tweet['target'].values,
                                                    test_size=0.15)

shape(X_train, X_test)
</code></pre>


<pre><code># Fit the model.
history = model.fit(X_train, y_train,
                    batch_size=4,
                    epochs=20,
                    validation_data=(X_test, y_test),
                    verbose=2)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>ML Models for GloVe</b></summary><p>

<h4>5. Make GloVe suitable for ML Models</h4>

<pre><code># Convert each text in a row to a vector.
def sent2vec(s):
    words = str(s).lower()
    words = word_tokenize(words)
    words = [w for w in words if not w in stop and w.isalpha()]

    M = []
    for w in words:
        try:
            M.append(embedding_dict[w])
        except:
            continue

    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray: return np.zeros(100)
    return v / np.sqrt((v**2).sum())

# Apply the function
df['text'] = [sent2vec(x) for x in tqdm(df['text'], position=0)]
</code></pre>


<pre><code># Train Test Split the data
X_train, X_valid, y_train, y_valid = train_test_split(df[df.target.notnull()]['text'],
                                                      df[df.target.notnull()]['target'],
                                                      test_size=.2,
                                                      random_state=33)
</code></pre>


<pre><code># Convert the data into arrays
X_train, X_valid = X_train.apply(pd.Series), X_valid.apply(pd.Series)
</code></pre>

</p></details>
</p></details>

<details><summary><b style='font-size:20px'>Universal Sentence Encoding</b></summary><p>
<h4>1. Load the embeddings</h4>

<pre><code># Import the hubber
import tensorflow_hub as hub

# Load the embbedding
embed = hub.load('../../../Personal/Development/Courses Docs/Word Embeddings/Universal Sentence Encoder')
X_train_embedding = embed(train.text.values)
X_test_embedding  = embed(test.text.values)
</code></pre>


<h4>2. Concatenate with other features</h4>

<pre><code># Merge with TF-IDF
train_df    = np.concatenate([X_train_embedding['outputs'], tf_train], axis=1)
test_df     = np.concatenate([X_test_embedding['outputs'], tf_test], axis=1)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>Gensim [Word2Vec]</b></summary><p>
<h4>1. Load Word2Vec into Gensim</h4>

<pre><code># NOTE: you can limit = 200_000, if you have limited memory.
# GoogleNews: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
from gensim.models import KeyedVectors

word_vectors = KeyedVectors.load_word2vec_format(&quot;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&quot;, binary=True, limit=200_000)
</code></pre>


<h4>2. Most similar words "Synonems"</h4>

<pre><code>word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)

#Result.
[('cook', 0.6973530650138855),
('oven_roasting', 0.6754530668258667),
('Slow_cooker', 0.6742032170295715),
('sweet_potatoes', 0.6600279808044434),
('stir_fry_vegetables', 0.6548759341239929)]
</code></pre>


<h4>3. Adding & Subtracting words.</h4>

<pre><code>word_vectors.most_similar(positive=['king', 'women'], negative=['man'], topn=2)

#RESULT
[('queen', 0.7118192315101624), ('monarch', 0.6189674139022827)]
</code></pre>


<h4>4. Detect unrelated terms</h4>

<pre><code>word_vectors.doesnt_match(&quot;potatoes milk cake computer&quot;.split())

#RESULT
'computer'
</code></pre>


<h4>5. Cosine similarity b/w words.</h4>

<pre><code>word_vectors.similarity('princess', 'queen')

#RESULT
0.70705315983704509
</code></pre>


<h4>6. Numerical Dimension of the word.</h4>

<pre><code>word_vectors['phone']

#RESULT
array([-0.01446533, -0.12792969, -0.11572266, -0.22167969, -0.07373047,
-0.05981445, -0.10009766, -0.06884766, 0.14941406, 0.10107422,
-0.03076172, -0.03271484, -0.03125
 , -0.10791016, 0.12158203,
0.16015625, 0.19335938, 0.0065918 , -0.15429688, 0.03710938,
...
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>Build Custom Word2Vec</b></summary><p>
First you need to break your documents into sentences and the sentences into tokens. End up like this:

<pre><code>&gt;&gt;&gt; token_list
[
['to', 'provide', 'early', 'intervention/early', 'childhood', 'special',
'education', 'services', 'to', 'eligible', 'children', 'and', 'their',
'families'],
['essential', 'job', 'functions'],
['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to',
'complete', 'educational', 'assessments', 'for']
...
]
</code></pre>


<h4>Load the Model</h4>

<pre><code>from gensim.models.word2vec import Word2Vec

# Setup the parameters.
num_features = 300  # Number of vector elements to represent the word.
min_word_count = 3  # Min number of word count to be considered in the Word2vec model.
num_workers = 2   # Number of CPU cores used for the training.
window_size = 6  # Context window size.
subsampling = 1e-3      # Subsampling rate for frequent terms.
</code></pre>


<h4>Build the Model</h4>

<pre><code>model = Word2Vec(
            token_list,
            workers=num_workers,
            size=num_features,
            min_count=min_word_count,
            window=window_size,
            sample=subsampling)

</code></pre>


Word2vec models can consume quite a bit of memory. But remember that only the
weight matrix for the hidden layer is of interest. Once youâ€™ve trained your word
model, you can reduce the memory footprint by about half if you freeze your model
and discard the unnecessary information. The following command will discard the
unneeded output weights of your neural network:

<pre><code>model.init_sims(replace=True)
</code></pre>


<h4>Save the model</h4>

<pre><code>model_name = &quot;my_domain_specific_word2vec_model&quot;
model.save(model_name)
</code></pre>


<h4>Use the newly trained model</h4>

<pre><code>from gensim.models.word2vec import Word2Vec
model_name = &quot;my_domain_specific_word2vec_model&quot;
model = Word2Vec.load(model_name)
model.most_similar('radiology')
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>fastText</b></summary><p>
Head over to the fastText model repository and download the bin+text model for your language of choice.
After the download finishes, unzip the binary language file.27 With the following code, you can then load it into gensim:

<pre><code># If using gensim version before 3.2.0, use the following code: gensim.models.wrappers.fasttext import FastText
from gensim.models.fasttext import FastText
ft_model = FastText.load_fasttext_format(\
        model_file=MODEL_PATH)
ft_model.most_similar('soccer')
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>Similarity Measures</b></summary><p>

<pre><code># Calculate &quot;Eclidean distance&quot;
np.linalg.norm(wv[&quot;Illinois&quot;] - wv[&quot;Illini&quot;])

# Calculate Cosine Similatiry == Normalized dot product.
cos_similarity = np.dot(wv[&quot;Illinois&quot;], wv[&quot;Illini&quot;]) / (
    np.linalg.norm(wv[&quot;Illinois&quot;]) *\
    np.linalg.norm(wv[&quot;Illini&quot;]))
print(cos_similarity)

# Calculate Cosine Distance.
# 0 = Similar
# 
print(1 - cos_similarity)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>Embeddings for Sentence</b></summary><p>

<pre><code>def sentence_to_vec(sent, embedding_dict):
    # initialize emtpy list to store embeddings.
    M = []
    for w in words:
        # for every word, fetch the embedding from
        # the dictionary, and append to list of embeddings.
        if w in embedding_dict:
            M.append(embedding_dict[w])

    # if we don't have any vectors, returns zeros.
    if len(M) == 0:
        return np.zeros(300)

    # Convert list of embeddings to array.
    M = np.array(M)

    # Calculate sum over axis=0
    v = M.sum(axis=0)

    # Return normalized vector
    return v / np.sqrt((v ** 2).sum())
</code></pre>

</p></details>
</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>