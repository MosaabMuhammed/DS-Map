<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Modeling</h1>
<div style='width:1000px;margin:auto'>

<details><summary><b>CNN with Embedding</b></summary><p><ul>
<li><a href='./0_notebooks/CNN.html'>CNN with GloVe</a></li>
<li><a href='./0_notebooks/ch07.html'>CNN with GoogleNews <b>word2vec</b></a></li>
</ul></p></details>

<details><summary><b>LSTM & GRU & Bi-Directional</b></summary><p>
<li><a href='./0_notebooks/LSTM_Toxic.html'>LSTM with Text</a></li>
<h4>Note: For Bi-Directional, do the following:</h4>

<pre><code># you have 2 options:
# 1. return a sequence, then select the max features among them.
# 2. Don't return a sequence, just return the last value, and here there's no neet for GlobalMaxPool1D
x = Bidirectional(LSTM(15, return_sequences=True))(x)
x = GlobalMaxPool1D()(x)
</code></pre>

</p></details>

<details><summary><b>BERT</b></summary><p>
<li><a href='./0_notebooks/BERT for Humans.html#Comprehensive-BERT-Tutorial'>Tutorials on BERT</a></li>
<li><a href='./0_notebooks/BERT Keras.html#This-is-the-very-first-time-I-would-be-implementing-BERT.'>BERT Keras</a></li>
<li><a href='./0_notebooks/BERT using simple transformers.html'>BERT using simpleTransformers</a></li>
</p></details>

<details><summary><b>Sentiment Analysis</b> using <b>Rule-based</b></summary><p>

<pre><code># !pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sa = SentimentIntensityAnalyzer()
# sa.lexicon --&gt; Print the lexicons

sa.polarity_scores(text=&quot;Python is very readable and it's great for NLP.&quot;)
</code></pre>


<pre><code>corpus = [&quot;Absolutely perfect! Love it! :-) :-) :-)&quot;,
          &quot;Horrible! Completely useless. :(&quot;,
          &quot;It was OK. some good and some bad things.&quot;]

for doc in corpus:
    scores = sa.polarity_scores(doc)
    print(f&quot;{scores['compound']:+}: {doc}&quot;)
</code></pre>

</p></details>

<details><summary><b>Latent Discriminant Analysis</b></summary><p>
<p>NOTE: you can use it in sklearn. (sklearn.discriminant_analysis.LinearDiscriminantAnalysis), but here we show the manual calculations on spam filter.</p>
<p>LDA is very useful when we have more columns and less rows, specially in text analysis</p>


<pre><code># 1. Calculate the TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize.casual import casual_tokenize
tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)
tfidf_docs  = tfidf_model.fit_transform(sms.text).toarray()

# 2. Calculate the mean for spam and ham.
mask = sms.spam.astype(bool).values
spam_centriod = tfidf_docs[mask].mean(axis=0)
ham_centriod  = tfidf_docs[~mask].mean(axis=0)

# 3. Dot product with TF_IDF matrix.
spamminess_score = tfidf_docs.dot(spam_centriod - ham_centriod)
spamminess_score.round(2)

# 4. Normalize to predict.
from sklearn.preprocessing import MinMaxScaler
sms[&quot;lda_score&quot;]   = MinMaxScaler().fit_transform(spamminess_score.reshape(-1, 1))
sms[&quot;lda_predict&quot;] = (sms.lda_score &gt; .5).astype(int)
sms[&quot;spam lda_predict lda_score&quot;.split()].round(2).head(6)~~~~
</code></pre>

</p></details>

<details><summary><b>Language Model</b></summary><p>
<h4>1. Load the corpus</h4>

<pre><code>import nltk
nltk.download(&quot;gutenberg&quot;)
from nltk.corpus import gutenberg
gutenberg.fileids()

# Concatenate the samples into one corpus
text = ''
for txt in gutenberg.fileids():
    if 'shakespeare' in txt:
        text += gutenberg.raw(txt).lower()

chars = sorted(list(set(text)))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

print(f&quot;corpus length: {bg(len(text))}, total chars: {bg(len(chars))}&quot;)
</code></pre>


<h4>2. Prepare the input & output</h4>

<pre><code>maxlen = 40
step   = 3
sentences  = []
next_chars = []

for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i+maxlen])
    next_chars.append(text[i+maxlen])

print(f&quot;nb sequences: {bg(len(sentences))}, {bg(len(next_chars))}&quot;)
</code></pre>


<h4>3. Create One-Hot encoding</h4>

<pre><code>X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)

for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
</code></pre>


<h4>3. Create the model</h4>

<pre><code>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.layers import LSTM
from tensorflow.keras.optimizers import RMSprop

model = Sequential([
    LSTM(128, input_shape=(maxlen, len(chars))),
    Dense(len(chars), activation=&quot;softmax&quot;)
])

optimizer = RMSprop(lr=.01)
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=optimizer)
model.summary()

# Train the model.
epochs          = 6
batch_size      = 128
model_structure = model.to_json()
with open(&quot;shakes_lstm_model.json&quot;, &quot;w&quot;) as json_file:
    json_file.write(model_structure)

for i in range(5):
    model.fit(X, y,
              batch_size=batch_size,
              epochs=epochs)
    model.save_weights(f&quot;shakes_lstm_weights_{i+1}.h5&quot;)
</code></pre>


<h4>4. Predict the next n characters with temperature</h4>

<pre><code>import random
def sample(preds, temprature=1.0):
    preds     = np.asarray(preds).astype('float64')
    preds     = np.log(preds) / temprature
    exp_preds = np.exp(preds)
    preds     = exp_preds / np.sum(exp_preds)
    probas    = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

import sys
start_index = random.randint(0, len(text)-maxlen-1)
for diversity in [.2, .5, 1.]:
    print()
    print(f&quot;------------ Diversity: {diversity}&quot;)
    generated = ''
    sentence  = text[start_index: start_index+maxlen]
    generated += sentence
    print(f&quot;---------- Generating with seed: {sentence}&quot;)
    sys.stdout.write(generated)

    for i in range(400):
        x = np.zeros((1, maxlen, len(chars)))
        for t, char in enumerate(sentence):
            x[0, t, char_indices[char]] = 1.
        preds = model.predict(x, verbose=0)[0]
        next_index = sample(preds, diversity)
        next_char  = indices_char[next_index]
        generated += next_char
        sentence   = sentence[1:] + next_char
        sys.stdout.write(next_char)
        sys.stdout.flush()

    print()
</code></pre>

</p></details>
</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>