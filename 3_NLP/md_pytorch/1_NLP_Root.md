# NLP [PyTorch]

<div style='width:1000px;margin:auto'>

<details><summary><b>Perceptron</b></summary><ul>
<li> Create Vocabulary. [one-hot] </li>
<li> Create DatasetLoader </lli>
<li> Create Model </li>
<li> Train & Validate the Model</li>
</ul>

<a href="./0_notebooks/3_5_yelp_dataset_preprocessing_FULL.html"><b >1. Split Dataset</b></a><br>
<a href="./0_notebooks/3_5_Classifying_Yelp_Review_Sentiment.html"><b>2. Yelp Review notebook</b></a>

</details>

<details><summary><b>MLP</b></summary><ul>
<li> Create Vocabulary. [one-hot]</li>
<li> Create DatasetLoader </lli>
<li> Create Model </li>
<li> Train & Validate the Model</li>
</ul>

<a href="./0_notebooks/4_2_Classifying_Surnames_with_an_MLP.html"><b>notebook</b></a>

</details>

<details><summary><b>CNN1d</b></summary><ul>
<li> Create Vocabulary. [one-hot]</li>
<li> Create DatasetLoader </lli>
<li> Create Model </li>
<li> Train & Validate the Model</li>
</ul>

<a href="./0_notebooks/4_4_Classifying_Surnames_with_a_CNN.html"><b>notebook</b></a>

</details>

<details><summary>Read <b>Pre-Trained Embedding Class</b></summary>
find the closeset relationships between words.<br>
<a href="./0_notebooks/5_1_Pretrained_Embeddings.html"><b>notebook</b></a>
</details>

<details><summary><b>Train C-BOW</b> Embedding</summary>
Remember, sum(dim=1) after Embedding layer to sum up words vector into context vector, in order to be fed correctly to Linear layer.<br>

<a href="./0_notebooks/5_2_munging_frankenstein.html"><b>Prepare Data</b></a><br>
<a href="./0_notebooks/5_2_Continuous_Bag_of_Words_CBOW.html"><b>Train C-BOW</b></a>

</details>


</div>
