<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Text Representation</h1>
<div style='width:1000px;margin:auto'>
<details><summary><b style='font-size:20px'>1. Bag of Words</b></summary><p>

<pre><code>cnt_vectorizer = CountVectorizer(dtype=np.float32,
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3),min_df=3)


# we fit count vectorizer to get ngrams from both train and test data.
cnt_vectorizer.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))

xtrain_cntv =  cnt_vectorizer.transform(train_df.cleaned_text.values) 
xtest_cntv = cnt_vectorizer.transform(test_df.cleaned_text.values)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>2. TF-IDF</b></summary><p>
<p><b>NOTE:</b> You can build TF-IDF followed by search engine instead of ML models, and calculate cosine similarity between the quest and your documents using inverted index O(1) using Whoosh python library.</p>

<pre><code># Always start with these features. They work (almost) everytime!
tfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, 
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,
            stop_words = 'english')

# Fitting TF-IDF to both training and test sets (semi-supervised learning)
tfv.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))
xtrain_tfv =  tfv.transform(train_df.cleaned_text.values) 
xvalid_tfv = tfv.transform(test_df.cleaned_text.values)
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>3. Hashing Features</b></summary><p>

<pre><code># Always start with these features. They work (almost) everytime!
hv = HashingVectorizer(dtype=np.float32,
            strip_accents='unicode', analyzer='word',
            ngram_range=(1, 4),n_features=2**12,non_negative=True)
# Fitting Hash Vectorizer to both training and test sets (semi-supervised learning)
hv.fit(list(train_df.cleaned_text.values) + list(test_df.cleaned_text.values))
xtrain_hv =  hv.transform(train_df.cleaned_text.values) 
xvalid_hv = hv.transform(test_df.cleaned_text.values)
y_train = train_df.target.values
</code></pre>

</p></details>

<details><summary><b style='font-size:20px'>4. Latent Semantic Analysis</b></summary><p>

The idea here, is to gather all the words that have closer meaning together into a topic, to reduce the dimensionality and avoid overfitting and increase the generality of the models.<br><br>

You can do this by appling PCA/TruncatedSVD/LDiA/LDA/NNMF after building TF-IDF / BoW.<br><br>

Then feed this new data to the models.

</p></details>

<details><summary><b style='font-size:20px'>5. Other Schemes</b></summary><p>
<p><img src="imgs/20200621-174111.png" alt="" /></p>
</p></details>

<details><summary><b style='font-size:20px'>6. Semantic Hashing</b></summary><p>

<pre><code>def find_ngrams(input_list, n):
    return zip(*[input_list[i:] for i in range(n)])

def semhash_tokenizer(text):
    tokens       = text.lower().split(&quot; &quot;)
    final_tokens = []
    for unhashed_token in tokens:
        hashed_token = f&quot;#{unhashed_token}#&quot;
        final_tokens += [''.join(gram) for gram in list(find_ngrams(list(hashed_token), 3))]

    return final_tokens

# Use this one.
def semhash_corpus(sentence):
    tokens   = semhash_tokenizer(sentence)
    return &quot; &quot;.join(map(str, tokens))
</code></pre>

</p></details>
</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>