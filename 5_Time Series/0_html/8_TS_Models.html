<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        TS Models
    </title>
    <link rel="stylesheet" href="../../prism.css">
</head>

<body>
    <h1 id="2-forecasting">
        2. Forecasting
    </h1>
    <div style="width:1000px;margin:auto">

        <details>
            <summary><b>Intro to Forecasting</b></summary>
            <a href="./9_models/00-Introduction-to-Forecasting-Revised.html"><strong>Halt-Winter - Evaluation - Stationary</strong></a></p>
        </details>
        <!-- --------------------------- ----------------------------- -->
        <details>
            <summary><b>ARIMA Models</b></summary>
            <ul>
                <details>
                    <summary>AR Model</summary>
                    <p>An autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, etc.<br> The autoregressive model specifies that the output variable
                        depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation.</p>
                    <pre class="language-python"><code># Create a simulated process.
# AR(1) model
# Rt = μ + ϕRt-1 + εt
# As RHS has only one lagged value(Rt-1)this is called AR model of order 1 where μ is mean and ε is noise at time t
# AR(1) MA(1) model:AR parameter = +0.9
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_process import ArmaProcess
from statsmodels.tsa.arima_model import ARIMA
import math
from sklearn.metrics import mean_squared_error

rcParams['figure.figsize'] = 16, 12
plt.subplot(4,1,1)
ar1 = np.array([1, -0.9]) # We choose -0.9 as AR parameter is +0.9
ma1 = np.array([1])
AR1 = ArmaProcess(ar1, ma1)
sim1 = AR1.generate_sample(nsample=1000)
plt.title('AR(1) model: AR parameter = +0.9')
plt.plot(sim1)
</code></pre>
                    <img src="../0_html/imgs/ar.png" alt="">
                    <pre class="language-python"><code># Forecasting.
model = ARMA(sim1, order=(1,0))
result = model.fit()
print(result.summary())
print("μ={} ,ϕ={}".format(result.params[0],result.params[1]))

ARMA Model Results                              
==============================================================================
Dep. Variable:                      y   No. Observations:                 1000
Model:                     ARMA(1, 0)   Log Likelihood               -1415.701
Method:                       css-mle   S.D. of innovations              0.996
Date:                Thu, 02 Aug 2018   AIC                           2837.403
Time:                        14:43:19   BIC                           2852.126
Sample:                             0   HQIC                          2842.998
                                                                              
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.7072      0.288      2.454      0.014       0.142       1.272
ar.L1.y        0.8916      0.014     62.742      0.000       0.864       0.919
                                    Roots                                    
=============================================================================
                 Real           Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.1216           +0.0000j            1.1216            0.0000
-----------------------------------------------------------------------------
μ=0.7072025170552714 ,ϕ=0.8915815634822984
</code></pre>
                    <pre class="language-python"><code># Forecasting.
# Predicting simulated AR(1) model 
result.plot_predict(start=900, end=1010)
plt.show()
</code></pre>
                    <img src="../0_html/imgs/ar-forecast.png" alt="" width="600" height="400">
                    <pre class="language-python"><code>rmse = math.sqrt(mean_squared_error(sim1[900:1011], result.predict(start=900,end=999)))
print("The root mean squared error is {}.".format(rmse))
# The root mean squared error is 1.0408054544358292.
# y is predicted plot. Quite neat!
                </code></pre>
                </details>

                <details>
                    <summary>MA Model</summary>
                    <p>The moving-average (MA) model is a common approach for modeling univariate time series. <br>The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly
                        predictable) term.<br> MA(1) model <br> Rt = μ + ϵt1 + θϵt-1 <br>It translates to Today's returns = mean + today's noise + yesterday's noise</p>
                    <pre class="language-python"><code># Create a simulated process.
# AR(1) model
# Rt = μ + ϕRt-1 + εt
# As RHS has only one lagged value(Rt-1)this is called AR model of order 1 where μ is mean and ε is noise at time t
# AR(1) MA(1) model:AR parameter = +0.9
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_process import ArmaProcess
from statsmodels.tsa.arima_model import ARIMA
import math
from sklearn.metrics import mean_squared_error

rcParams['figure.figsize'] = 16, 6
ar1 = np.array([1])
ma1 = np.array([1, -0.5])
MA1 = ArmaProcess(ar1, ma1)
sim1 = MA1.generate_sample(nsample=1000)
plt.plot(sim1)
</code></pre>
                    <img src="../0_html/imgs/ma.png" alt="">
                    <pre class="language-python"><code># Forecasting and predicting montreal humidity
model = ARMA(humidity["Montreal"].diff().iloc[1:].values, order=(0,3))
result = model.fit()
print(result.summary())
print("μ={} ,θ={}".format(result.params[0],result.params[1]))
result.plot_predict(start=1000, end=1100)
plt.show()
                                ARMA Model Results                              
==============================================================================
Dep. Variable:                      y   No. Observations:                45251
Model:                     ARMA(0, 3)   Log Likelihood             -153516.982
Method:                       css-mle   S.D. of innovations              7.197
Date:                Thu, 02 Aug 2018   AIC                         307043.965
Time:                        14:43:32   BIC                         307087.564
Sample:                             0   HQIC                        307057.686
                                                                                
==============================================================================
                    coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0008      0.031     -0.025      0.980      -0.061       0.060
ma.L1.y       -0.1621      0.005    -34.507      0.000      -0.171      -0.153
ma.L2.y        0.0386      0.005      8.316      0.000       0.030       0.048
ma.L3.y        0.0357      0.005      7.446      0.000       0.026       0.045
                                    Roots                                    
=============================================================================
                    Real           Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
MA.1            1.4520           -2.2191j            2.6519           -0.1578
MA.2            1.4520           +2.2191j            2.6519            0.1578
MA.3           -3.9867           -0.0000j            3.9867           -0.5000
-----------------------------------------------------------------------------
μ=-0.0007772680242180366 ,θ=-0.16209499431431182
</code></pre>
                    <img src="../0_html/imgs/forcast_ma.png" alt="" width="600" height="400">
                    <pre class="language-python"><code>rmse = math.sqrt(mean_squared_error(humidity["Montreal"].diff().iloc[1000:1101].values, result.predict(start=1000,end=1100)))
print("The root mean squared error is {}.".format(rmse))
# The root mean squared error is 11.345129665763626.
</code></pre>
                </details>

                <details>
                    <summary>ARIMA Model</summary>
                    <p>Autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression and the second for the moving average.<br>It's the fusion
                        of AR and MA models.</p>
                    <p>ARMA(1,1) model <br> Rt = μ + ϕRt-1 + ϵt + θϵt-1 <br>Basically, Today's return = mean + Yesterday's return + noise + yesterday's noise.</p>
                    <pre class="language-python"><code>
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_process import ArmaProcess
from statsmodels.tsa.arima_model import ARIMA
import math
from sklearn.metrics import mean_squared_error

# Predicting the microsoft stocks volume
rcParams['figure.figsize'] = 16, 6
model = ARIMA(microsoft["Volume"].diff().iloc[1:].values, order=(2,1,0))
result = model.fit()
print(result.summary())
result.plot_predict(start=700, end=1000)
plt.show()

ARIMA Model Results                              
==============================================================================
Dep. Variable:                    D.y   No. Observations:                 3017
Model:                 ARIMA(2, 1, 0)   Log Likelihood              -56385.467
Method:                       css-mle   S.D. of innovations       31647215.014
Date:                Thu, 02 Aug 2018   AIC                         112778.933
Time:                        14:44:02   BIC                         112802.981
Sample:                             1   HQIC                        112787.581
                                                                              
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const       9984.0302   2.48e+05      0.040      0.968   -4.75e+05    4.95e+05
ar.L1.D.y     -0.8716      0.016    -53.758      0.000      -0.903      -0.840
ar.L2.D.y     -0.4551      0.016    -28.071      0.000      -0.487      -0.423
                                    Roots                                    
=============================================================================
                 Real           Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           -0.9575           -1.1315j            1.4823           -0.3618
AR.2           -0.9575           +1.1315j            1.4823            0.3618
-----------------------------------------------------------------------------

rmse = math.sqrt(mean_squared_error(microsoft["Volume"].diff().iloc[700:1001].values, result.predict(start=700,end=1000)))
print("The root mean squared error is {}.".format(rmse))
# The root mean squared error is 61937593.98493614.
</code></pre>
                    <img src="../0_html/imgs/ARIMA.png" alt="" width="600" height="400">
                </details>

                <details>
                    <summary>SARIMA Model</summary>
                    <p>Adding this letter to the four gives us the ARIMA model which can handle non-stationary data with the help of nonseasonal differences. Great, one more letter to go! <br> S(s) - this is responsible for seasonality and equals the season
                        period length of the series <br> With this, we have three parameters: (P,D,Q) <br> P - order of autoregression for the seasonal component of the model, which can be derived from PACF. But you need to look at the number of significant
                        lags, which are the multiples of the season period length. For example, if the period equals 24 and we see the 24-th and 48-th lags are significant in the PACF, that means the initial P should be 2. <br> Q - similar logic using
                        the ACF plot instead. <br> D - order of seasonal integration. This can be equal to 1 or 0, depending on whether seasonal differeces were applied or not.</p>
                    <pre class="language-python"><code># setting initial values and some bounds for them
ps = range(2, 5)
d=1 
qs = range(2, 5)
Ps = range(0, 2)
D=1 
Qs = range(0, 2)
s = 24 # season length is still 24

# creating list with all the possible combinations of parameters
parameters = product(ps, qs, Ps, Qs)
parameters_list = list(parameters)
len(parameters_list)
</code></pre>
                    <pre class="language-python"><code>def optimizeSARIMA(parameters_list, d, D, s):
    """
        Return dataframe with parameters and corresponding AIC
        
        parameters_list - list with (p, q, P, Q) tuples
        d - integration order in ARIMA model
        D - seasonal integration order 
        s - length of season
    """
    
    results = []
    best_aic = float("inf")

    for param in tqdm_notebook(parameters_list):
        # we need try-except because on some combinations model fails to converge
        try:
            model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(param[0], d, param[1]), 
                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)
        except:
            continue
        aic = model.aic
        # saving best model, AIC and parameters
        if aic < best_aic:
            best_model = model
            best_aic = aic
            best_param = param
        results.append([param, model.aic])

    result_table = pd.DataFrame(results)
    result_table.columns = ['parameters', 'aic']
    # sorting in ascending order, the lower AIC is - the better
    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)
    
    return result_table

result_table = optimizeSARIMA(parameters_list, d, D, s)
result_table.head()
# 	parameters	aic
# 0	(2, 3, 1, 1)	3888.642174
# 1	(3, 2, 1, 1)	3888.763568
# 2	(4, 2, 1, 1)	3890.279740
# 3	(3, 3, 1, 1)	3890.513196
# 4	(2, 4, 1, 1)	3892.302849
</code></pre>
                    <pre class="language-python"><code># set the parameters that give the lowest AIC
p, q, P, Q = result_table.parameters[0]

best_model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(p, d, q), 
                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)
print(best_model.summary())
#                                  Statespace Model Results                                 
# ==========================================================================================
# Dep. Variable:                                Ads   No. Observations:                  216
# Model:             SARIMAX(2, 1, 3)x(1, 1, 1, 24)   Log Likelihood               -1936.321
# Date:                            Mon, 04 Jan 2021   AIC                           3888.642
# Time:                                    00:06:39   BIC                           3914.660
# Sample:                                09-13-2017   HQIC                          3899.181
#                                      - 09-21-2017                                         
# Covariance Type:                              opg                                         
# ==============================================================================
#                  coef    std err          z      P>|z|      [0.025      0.975]
# ------------------------------------------------------------------------------
# ar.L1          0.7913      0.270      2.928      0.003       0.262       1.321
# ar.L2         -0.5503      0.306     -1.799      0.072      -1.150       0.049
# ma.L1         -0.7316      0.262     -2.793      0.005      -1.245      -0.218
# ma.L2          0.5651      0.282      2.005      0.045       0.013       1.118
# ma.L3         -0.1811      0.092     -1.964      0.049      -0.362      -0.000
# ar.S.L24       0.3312      0.076      4.351      0.000       0.182       0.480
# ma.S.L24      -0.7635      0.104     -7.361      0.000      -0.967      -0.560
# sigma2      4.574e+07   5.61e-09   8.15e+15      0.000    4.57e+07    4.57e+07
# ===================================================================================
# Ljung-Box (Q):                       43.70   Jarque-Bera (JB):                10.56
# Prob(Q):                              0.32   Prob(JB):                         0.01
# Heteroskedasticity (H):               0.65   Skew:                            -0.28
# Prob(H) (two-sided):                  0.09   Kurtosis:                         4.00
# ===================================================================================
</code></pre>
                    <pre class="language-python"><code>def plotSARIMA(series, model, n_steps):
    """
        Plots model vs predicted values
        
        series - dataset with timeseries
        model - fitted SARIMA model
        n_steps - number of steps to predict in the future
        
    """
    # adding model values
    data = series.copy()
    data.columns = ['actual']
    data['arima_model'] = model.fittedvalues
    # making a shift on s+d steps, because these values were unobserved by the model
    # due to the differentiating
    data['arima_model'][:s+d] = np.NaN
    
    # forecasting on n_steps forward 
    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)
    forecast = data.arima_model.append(forecast)
    # calculate error, again having shifted on s+d steps from the beginning
    error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])

    plt.figure(figsize=(15, 7))
    plt.title("Mean Absolute Percentage Error: {0:.2f}%".format(error))
    plt.plot(forecast, color='r', label="model")
    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')
    plt.plot(data.actual, label="actual")
    plt.legend()
    plt.grid(True);
plotSARIMA(ads, best_model, 50)</code></pre>

                </details>

                <details>
                    <summary>Auto-ARMA Model 1</summary>
                    <pre class="language-python"><code>!pip install pmdarima
from statsmodels.tsa.arima_model import ARIMA
import pmdarima as pm

model = pm.auto_arima(y_train, start_p=1, start_q=1,
                        test='adf',       # use adftest to find optimal 'd'
                        max_p=3, max_q=3, # maximum p and q
                        m=1,              # frequency of series
                        d=None,           # let model determine 'd'
                        seasonal=False,   # No Seasonality
                        start_P=0, 
                        D=0, 
                        trace=True,
                        error_action='ignore',  
                        suppress_warnings=True, 
                        stepwise=True)

print(model.summary())
# Performing stepwise search to minimize aic
#  ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=-631.136, Time=0.29 sec
#  ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=-242.692, Time=0.06 sec
#  ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=-574.047, Time=0.09 sec
#  ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=-427.347, Time=0.09 sec
#  ARIMA(0,1,0)(0,0,0)[0]             : AIC=-243.054, Time=0.02 sec
#  ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=-629.209, Time=0.48 sec
#  ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=-629.237, Time=0.77 sec
#  ARIMA(0,1,2)(0,0,0)[0] intercept   : AIC=-492.779, Time=0.27 sec
#  ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=-611.065, Time=0.19 sec
#  ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=-628.351, Time=1.11 sec
#  ARIMA(1,1,1)(0,0,0)[0]             : AIC=-632.995, Time=0.25 sec
#  ARIMA(0,1,1)(0,0,0)[0]             : AIC=-428.258, Time=0.04 sec
#  ARIMA(1,1,0)(0,0,0)[0]             : AIC=-575.735, Time=0.04 sec
#  ARIMA(2,1,1)(0,0,0)[0]             : AIC=-631.069, Time=0.15 sec
#  ARIMA(1,1,2)(0,0,0)[0]             : AIC=-631.097, Time=0.25 sec
#  ARIMA(0,1,2)(0,0,0)[0]             : AIC=-494.001, Time=0.11 sec
#  ARIMA(2,1,0)(0,0,0)[0]             : AIC=-612.866, Time=0.10 sec
#  ARIMA(2,1,2)(0,0,0)[0]             : AIC=-630.210, Time=0.47 sec

# Best model:  ARIMA(1,1,1)(0,0,0)[0]          
# Total fit time: 4.805 seconds
#                                SARIMAX Results                                
# ==============================================================================
# Dep. Variable:                      y   No. Observations:                  510
# Model:               SARIMAX(1, 1, 1)   Log Likelihood                 319.497
# Date:                Sat, 06 Mar 2021   AIC                           -632.995
# Time:                        00:23:18   BIC                           -620.297
# Sample:                             0   HQIC                          -628.016
#                                 - 510                                         
# Covariance Type:                  opg                                         
# ==============================================================================
#                  coef    std err          z      P>|z|      [0.025      0.975]
# ------------------------------------------------------------------------------
# ar.L1          0.9196      0.021     43.766      0.000       0.878       0.961
# ma.L1         -0.4885      0.037    -13.357      0.000      -0.560      -0.417
# sigma2         0.0167      0.001     24.809      0.000       0.015       0.018
# ===================================================================================
# Ljung-Box (L1) (Q):                   0.02   Jarque-Bera (JB):               185.01
# Prob(Q):                              0.90   Prob(JB):                         0.00
# Heteroskedasticity (H):               1.17   Skew:                             0.22
# Prob(H) (two-sided):                  0.32   Kurtosis:                         5.92
# ===================================================================================</code></pre>
                    <pre class="language-python"><code>model.plot_diagnostics(figsize=(16,8))
plt.show()
# So how to interpret the plot diagnostics?
# Top left: The residual errors seem to fluctuate around a mean of zero and have a uniform variance between (-4, 4).
# Top Right: The density plot suggest normal distribution with mean zero.
# Bottom left: The most part of the blue dots are over the red line, so it seems that the distribution in very low skewed (not skewed for me).
# Bottom Right: The Correlogram, aka, ACF plot shows the residual errors are not autocorrelated.
</code></pre>
                    <img src="../0_html/imgs/diag_autoarima.png" alt="">
                </details>
                <details>
                    <summary>Auto-ARMA Model 2</summary>
                    <pre class="language-python"><code>df.reset_index(drop=True, inplace=True)
lag_features = ["High", "Low", "Volume", "Turnover", "Trades"]
window1 = 3
window2 = 7
window3 = 30

df_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)
df_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)
df_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)

df_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)
df_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)
df_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)

df_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)
df_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)
df_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)

for feature in lag_features:
    df[f"{feature}_mean_lag{window1}"] = df_mean_3d[feature]
    df[f"{feature}_mean_lag{window2}"] = df_mean_7d[feature]
    df[f"{feature}_mean_lag{window3}"] = df_mean_30d[feature]
    
    df[f"{feature}_std_lag{window1}"] = df_std_3d[feature]
    df[f"{feature}_std_lag{window2}"] = df_std_7d[feature]
    df[f"{feature}_std_lag{window3}"] = df_std_30d[feature]

df.fillna(df.mean(), inplace=True)

df.set_index("Date", drop=False, inplace=True)
df.head()
</code></pre>

                    <pre class="language-python"><code>df_train = df[df.Date < "2019"]
df_valid = df[df.Date >= "2019"]

exogenous_features = ["High_mean_lag3", "High_std_lag3", "Low_mean_lag3", "Low_std_lag3",
                      "Volume_mean_lag3", "Volume_std_lag3", "Turnover_mean_lag3",
                      "Turnover_std_lag3", "Trades_mean_lag3", "Trades_std_lag3",
                      "High_mean_lag7", "High_std_lag7", "Low_mean_lag7", "Low_std_lag7",
                      "Volume_mean_lag7", "Volume_std_lag7", "Turnover_mean_lag7",
                      "Turnover_std_lag7", "Trades_mean_lag7", "Trades_std_lag7",
                      "High_mean_lag30", "High_std_lag30", "Low_mean_lag30", "Low_std_lag30",
                      "Volume_mean_lag30", "Volume_std_lag30", "Turnover_mean_lag30",
                      "Turnover_std_lag30", "Trades_mean_lag30", "Trades_std_lag30",
                      "month", "week", "day", "day_of_week"]
</code></pre>
                    <pre class="language-python"><code>!pip install pmdarima
from statsmodels.tsa.arima_model import ARIMA
import pmdarima as pm

model = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features], trace=True, error_action="ignore", suppress_warnings=True)
model.fit(df_train.VWAP, exogenous=df_train[exogenous_features])

forecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])
df_valid["Forecast_ARIMAX"] = forecast

df_valid[["VWAP", "Forecast_ARIMAX"]].plot(figsize=(14, 7))
</code></pre>
                </details>


            </ul>
        </details>

        <!-- --------------------------- ----------------------------- -->
        <details>
            <summary><b>FB Prophet</b></summary>
            <ul>
                <li><a href="../0_html/9_models/Facebook_Prophet.html">Prophet - Univariate forcasting</a></li>
                <li>
                    <details>
                        <summary><b>Multivariate Forcasting - Prophet</b></summary>

                        <pre class="language-python"><code>from fbprophet import Prophet

# Train the model
model = Prophet()
model.add_regressor('rainfall')
model.add_regressor('temperature')
model.add_regressor('drainage_volume')
model.add_regressor('river_hydrometry')

# Fit the model with train set
model.fit(train)

# Predict on valid set
y_pred = model.predict(x_valid)

# Calcuate metrics
score_mae = mean_absolute_error(y_valid, y_pred['yhat'])
score_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))

print(Fore.GREEN + 'RMSE: {}'.format(score_rmse))
# RMSE: 0.9940444133552163
                </code></pre>
                        <pre class="language-python"><code># Plot the forecast
f, ax = plt.subplots(1)
f.set_figheight(6)
f.set_figwidth(15)

model.plot(y_pred, ax=ax)
sns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite

ax.set_title(f'Prediction \n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)
ax.set_xlabel(xlabel='Date', fontsize=14)
ax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)

plt.show()
</code></pre>
                        <img src="../0_html/imgs/multivariate_prophet.png" alt="">
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Cross Validation</b></summary>
                        <a href="https://facebook.github.io/prophet/docs/diagnostics.html#:~:text=Cross%20validation,up%20to%20that%20cutoff%20point.">diagnostics docs</a>
                        <pre class="language-python"><code>from fbprophet import Prophet
from fbprophet.diagnostics import cross_validation, performance_metrics

m = Prophet(weekly_seasonality=False, interval_width = 0.95)

m.fit(df)
future = m.make_future_dataframe(periods= 24, freq = 'H')
forecast = m.predict(future)
m.plot_components(forecast, figsize=(CFG.img_dim1, CFG.img_dim2))

df_cv = cross_validation(m,initial = '3700 hours', period = '24 hours', horizon = '24 hours')
df_cv.head(10)

df_p1 = performance_metrics(df_cv)
df_p1.head(10)  
                </code></pre>

                    </details>
                </li>

            </ul>
        </details>

        <details>
            <summary><b>Neural Prophet </b></summary>
            <pre class="language-python"><code>!pip install neuralprophet
from neuralprophet import NeuralProphet

df = xdat2[['time', 'Tair']].rename(columns={"time": "ds", "Tair": "y"})

model = NeuralProphet(weekly_seasonality = False)

model.fit(df, freq="H")
# forecast
df_predict = model.make_future_dataframe(df, periods= 24)
df_predict = model.predict(df_predict)
fig = model.plot(df_predict)
            </code></pre>
        </details>

        <details>
            <summary><b>CNN-LSTM</b></summary>
            <a href="https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/">tutorials</a>
            <pre class="language-python"><code># univariate cnn-lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

subsequences = 2
timesteps = X_train_series.shape[1]//subsequences
X_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))
X_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))
print('Train set shape', X_train_series_sub.shape)
print('Validation set shape', X_valid_series_sub.shape)
# Train set shape (100746, 2, 15, 1)
# Validation set shape (67164, 2, 15, 1)

model_cnn_lstm = Sequential()
model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))
model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))
model_cnn_lstm.add(TimeDistributed(Flatten()))
model_cnn_lstm.add(LSTM(50, activation='relu'))
model_cnn_lstm.add(Dense(1))
model_cnn_lstm.compile(loss='mse', optimizer=adam)

cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, Y_train, validation_data=(X_valid_series_sub, Y_valid), epochs=epochs, verbose=2)

cnn_lstm_train_pred = model_cnn_lstm.predict(X_train_series_sub)
cnn_lstm_valid_pred = model_cnn_lstm.predict(X_valid_series_sub)
print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_lstm_train_pred)))
print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_lstm_valid_pred)))
# Train rmse: 19.204481417234568
# Validation rmse: 19.17420051024767
            </code></pre>
        </details>

    </div>
    <script src="../../prism.js"></script>
</body>

</html>