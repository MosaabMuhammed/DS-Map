<!doctype HTML>
<html>

<head>
    <meta charset="utf-8">
    <title>[TS]: EDA</title>
    <link rel="stylesheet" href="../../prism.css">
</head>

<body>
    <h1>4. Exploratory Data Analysis (EDA)</h1>
    <div style="width:1000px;margin:auto">

        <details>
            <summary><b>Tips & Tricks</b></summary>
            <p>

                <details>
                    <summary><b>Heatmap</b> for <b>Daily</b> usage</summary>
                    <p>
                        <a href='./5_EDA/Heatmap_daily_time_series.html#Sales-Heatmap-Calendar'>How to use (Notebook)</a>
                        <h4>Code</h4>

                        <pre class="language-python"><code class="python"># ----------------------------------------------------------------------------
# Author:  Nicolas P. Rougier
# License: BSD
# ----------------------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
from datetime import datetime
from dateutil.relativedelta import relativedelta


def calmap(ax, year, data):
    ax.tick_params('x', length=0, labelsize=&quot;medium&quot;, which='major')
    ax.tick_params('y', length=0, labelsize=&quot;x-small&quot;, which='major')

    # Month borders
    xticks, labels = [], []
    start = datetime(year,1,1).weekday()
    for month in range(1,13):
        first = datetime(year, month, 1)
        last = first + relativedelta(months=1, days=-1)

        y0 = first.weekday()
        y1 = last.weekday()
        x0 = (int(first.strftime(&quot;%j&quot;))+start-1)//7
        x1 = (int(last.strftime(&quot;%j&quot;))+start-1)//7

        P = [ (x0,   y0), (x0,    7),  (x1,   7),
              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),
              (x0+1,  0), (x0+1,  y0) ]
        xticks.append(x0 +(x1-x0+1)/2)
        labels.append(first.strftime(&quot;%b&quot;))
        poly = Polygon(P, edgecolor=&quot;black&quot;, facecolor=&quot;None&quot;,
                       linewidth=1, zorder=20, clip_on=False)
        ax.add_artist(poly)

    ax.set_xticks(xticks)
    ax.set_xticklabels(labels)
    ax.set_yticks(0.5 + np.arange(7))
    ax.set_yticklabels([&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;])
    ax.set_title(&quot;{}&quot;.format(year), weight=&quot;semibold&quot;)

    # Clearing first and last day from the data
    valid = datetime(year, 1, 1).weekday()
    data[:valid,0] = np.nan
    valid = datetime(year, 12, 31).weekday()
    # data[:,x1+1:] = np.nan
    data[valid+1:,x1] = np.nan

    # Showing data
    ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=-1, vmax=1,
              cmap=&quot;RdYlBu_r&quot;, origin=&quot;lower&quot;, alpha=.75)
</code></pre>

                    </p>
                </details>

                <details>
                    <summary><b>Upsampling</b></summary>
                    <p><b>Upsampling</b> Chaning the time from, for example, <b>minutes to seconds</b>.
                        <br><b>Upsampling</b> helps us to visualize and analyze data in more detail, and these fine-grained observations are calculated using <b>interpolation</b>.<br>
                        <h5>Pitfalls:</h5>
                        <b>Upsampling</b> leads to <b>NaN</b> values. the methods used in <b>interpolation</b> are linear or cubic splines for imputing <b>NaN</b> values. This might not represent the original data, so the analysis & visualization might
                        be misleading.</p>


                    <pre class="language-python"><code># Aggregate dataframe by year since we have a yearly time-series dataframe.
walmart_store_count = walmart_stores.groupby(&quot;YEAR&quot;)[[&quot;storenum&quot;]].agg(&quot;count&quot;).rename(columns={&quot;storenum&quot;: &quot;store_count&quot;})
</code></pre>



                    <pre class="language-python"><code># Convert the frequecy to 2 days (Upsampling):
walmart_store_count_series = walmart_store_count_series.asfreq('2D')
# And this will generated NaNs, and we will fill it with interpolation.
</code></pre>



                    <pre class="language-python"><code># Imputing NaNs with interpolation.
walmart_store_count_series = walmart_store_count_series.interpolate(method=&quot;spline&quot;, order=2)

# Then plot it.
walmart_store_count_series.plot(style=&quot;:&quot;)
</code></pre>

                    <img src="./imgs/upsampling.png">
                </details>

                <details>
                    <summary><b>Downsampling</b></summary>
                    <p><b>Downsampling</b> Chaning the time from, for example, <b>months to years</b>.
                        <br><b>Downsampling</b> helps summarize and get a general sense of trends in data.<br>
                        <h5>Pitfalls:</h5>
                        <b>Downsampling</b> aggregates the observation over sample frequency, where we provide a frequency to function as an argument, so we might lose information.</p>


                    <pre class="language-python"><code># After aggreation, we will smooth out the plot using downsampling with a frequency of BA(business year).
plt.figure(figsize=(12, 8))
plt.ylabel(&quot;Interpolated Values&quot;)
plt.plot(walmart_store_count_series)
walmart_store_count_series.resample('BA').mean().plot(style=':', title=&quot;Values Smoothen by Business Year Frequency&quot;) #BA stands for Business Year
</code></pre>

                    <img src="./imgs/downsampling_1.png">


                    <pre class="language-python"><code># Downsample with a frequency of BQ(business quarter) to observe higher granularity.
plt.figure(figsize=(12,8))
plt.ylabel(&quot;Interpolated Values&quot;)
walmart_store_count_series.plot(alpha=0.5, style='-')
walmart_store_count_series.resample('BQ').mean().plot(style=':', title=&quot;Values Smoothen by Business Quarter Frequency&quot;)#BQ stands for Business quarter
</code></pre>

                    <img src="./imgs/downsampling_2.png">
                </details>

                <details>
                    <summary><b>Autocorrelation, Partial Autocorrelation, other</b> in one plot</summary>
                    <pre class="language-python"><code>def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):
if not isinstance(y, pd.Series):
    y = pd.Series(y)
with plt.style.context(style):    
    fig = plt.figure(figsize=figsize)
    #mpl.rcParams['font.family'] = 'Ubuntu Mono'
    layout = (3, 2)
    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)
    acf_ax = plt.subplot2grid(layout, (1, 0))
    pacf_ax = plt.subplot2grid(layout, (1, 1))
    qq_ax = plt.subplot2grid(layout, (2, 0))
    pp_ax = plt.subplot2grid(layout, (2, 1))
    
    y.plot(ax=ts_ax)
    ts_ax.set_title(title)
    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)
    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)
    sm.qqplot(y, line='s', ax=qq_ax)
    qq_ax.set_title('QQ Plot')        
    scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)

    plt.tight_layout()
return </code></pre>

                    <pre class="language-python"><code># Simulate an AR(1) process with alpha = 0.6
np.random.seed(1)
n_samples = int(1000)
a = 0.6
x = w = np.random.normal(size=n_samples)

for t in range(n_samples):
    x[t] = a*x[t-1] + w[t]
limit=12    
_ = tsplot(x, lags=limit,title="AR(1)process")</code></pre>
                    <img src="../0_html/imgs/tsplot.png" alt="">
                    <h3>How to analyize ACF:</h3>
                    <img src="../0_html/imgs/acf_analysis.png" alt="">
                </details>


                <details>
                    <summary><b>Moving Average</summary>
                    <pre class="language-python"><code>def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):
        """
            series - dataframe with timeseries
            window - rolling window size 
            plot_intervals - show confidence intervals
            plot_anomalies - show anomalies 
        
        """
        rolling_mean = series.rolling(window=window).mean()
        
        plt.figure(figsize=(15,5))
        plt.title("Moving average\n window size = {}".format(window))
        plt.plot(rolling_mean, "g", label="Rolling mean trend")
        
        # Plot confidence intervals for smoothed values
        if plot_intervals:
            mae = mean_absolute_error(series[window:], rolling_mean[window:])
            deviation = np.std(series[window:] - rolling_mean[window:])
            lower_bond = rolling_mean - (mae + scale * deviation)
            upper_bond = rolling_mean + (mae + scale * deviation)
            plt.plot(upper_bond, "r--", label="Upper Bond / Lower Bond")
            plt.plot(lower_bond, "r--")
            
            # Having the intervals, find abnormal values
            if plot_anomalies:
                anomalies = pd.DataFrame(index=series.index, columns=series.columns)
                anomalies[series<lower_bond] = series[series<lower_bond]
                anomalies[series>upper_bond] = series[series>upper_bond]
                plt.plot(anomalies, "ro", markersize=10)
            
        plt.plot(series[window:], label="Actual values")
        plt.legend(loc="upper left")
        plt.grid(True)
        
        # plotMovingAverage(ads_anomaly, 4, plot_intervals=True, plot_anomalies=True)</code></pre>

                    <img src="../0_html/imgs/moving_avg.png" alt="">
                </details>
            </p>
        </details>

        </p>
        </details>

        <hr>
        <!-- - --------------------------------------- -->
        <details>
            <summary><b>Time-Series Analysis Course</b></summary>
                    <ul>
                        <!-- - --------------------------------------- -->
                        <li>
                            <details>
                                <summary><b>Time-Series with Pandas</b></summary>
                                <p>

                                    <ul>
                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/00-Datetime-Index.html#Python-Datetime-Review"><b>1.
                                    Datetime Index</b></a> </p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/01-Time-Resampling.html#Import-the-data"><b>2.
                                    Making Datetime as index and parse it as datetime64</b></a> </p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/01-Time-Resampling.html#resample()"><b>3.
                                    Resample</b> (making aggregation over time)</a></p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/02-Time-Shifting.html#Time-Shifting"><b>4.
                                    Time Shifting</b></a> </p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/03-Rolling-and-Expanding.html#Rolling-and-Expanding"><b>5.
                                    Rolling &amp; Expanding</b></a></p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/04-Visualizing-Time-Series-Data.html#Visualizing-Time-Series-Data"><b>6.
                                    Visualizing Time-Series Data</b></a> </p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/06-Pandas-Time-Series-Exercises-SET-ONE-Solutions.html"><b>From
                                    month as number to String</b></a> </p>
                                        </li>

                                        <li>
                                            <p><a href="./5_EDA/UDEMY_TSA_FINAL/04-Time-Series-with-Pandas/08-Time-Series-with-Pandas-Project-Exercise-SET-TWO-Solutions.html"><b>Extensive
                                    Time-Series EDA</b></a>
                                            </p>
                                </p>
                                </li>
                                </ul>
            </p>
            </details>
            </li>
            <!-- - --------------------------------------- -->
            <li>
                <details>
                    <summary><b>Statsmodel for Time-Series Analysis</b></summary>
                    <p>

                        <ul>
                            <li>
                                <p><a href="./5_EDA/UDEMY_TSA_FINAL/05-Time-Series-Analysis-with-Statsmodels/00-Introduction-to-Statsmodels.html"><b>1.
                                    Hodrick-Prescott filter</b></a> </p>
                            </li>

                            <li>
                                <p><a href="./5_EDA/UDEMY_TSA_FINAL/05-Time-Series-Analysis-with-Statsmodels/01-ETS-Decomposition.html"><b>2.
                                    ETS Decomposition</b></a> </p>
                            </li>

                            <li>
                                <p><a href="./5_EDA/UDEMY_TSA_FINAL/05-Time-Series-Analysis-with-Statsmodels/02-EWMA-Exponentially-Weighted-Moving-Average.html"><b>3.
                                    EWMA (Exponantially Weighted Moving Average)</b></a> </p>
                            </li>

                            <li>
                                <p><a href="./5_EDA/UDEMY_TSA_FINAL/05-Time-Series-Analysis-with-Statsmodels/03-Holt-Winters-Methods.html#Holt-Winters-Methods"><b>4.
                                    Holt-Winters Smoothing (Simple, Double, Triple)</b></a> </p>
                            </li>
                        </ul>
                    </p>
                </details>
            </li>

            </ul>
            </details>
            <!-- - --------------------------------------- -->
            <details>
                <summary><b>Finance</b></summary>
                <ul>
                    <li>
                        <details>
                            <summary>Change percent</summary>
                            <pre class="language-python"><code>google['Change'] = google.High.div(google.High.shift())
google['Change'].plot(figsize=(20,8))</code></pre>
                            <img src="./imgs/percent_change.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                    <li>
                        <details>
                            <summary>Stock Returns</summary>
                            <pre class="language-python"><code>google['Return'] = google.Change.sub(1).mul(100)
google['Return'].plot(figsize=(20,8))

google.High.pct_change().mul(100).plot(figsize=(20,6)) # Another way to calculate returns
</code></pre>
                            <img src="./imgs/percent_change.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                    <li>
                        <details>
                            <summary>Absolute Changes in successive days</summary>
                            <pre class="language-python"><code>google.High.diff().plot(figsize=(20,6))</code></pre>
                            <img src="./imgs/diff.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                    <li>
                        <details>
                            <summary>Comparing 2 or more time series</summary>
                            <p>We will compare 2 time series by normalizing them. This is achieved by dividing each time series element of all time series by the first element.<br> This way both series start at the same point and can be easily compared.</p>
                            <pre class="language-python"><code># Normalizing and comparison
# Both stocks start from 100
normalized_google = google.High.div(google.High.iloc[0]).mul(100)
normalized_microsoft = microsoft.High.div(microsoft.High.iloc[0]).mul(100)
normalized_google.plot()
normalized_microsoft.plot()
plt.legend(['Google','Microsoft'])
plt.show()

# You can clearly see how google outperforms microsoft over time.</code></pre>
                            <img src="./imgs/compare.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                    <li>
                        <details>
                            <summary>Window Functions <b>rolling</b>, <b>exapnding</b></summary>
                            <p>In rolling function the window size remain constant whereas in the expanding function it changes.</p>
                            <pre class="language-python"><code># Rolling window functions
rolling_google = google.High.rolling('90D').mean()
google.High.plot()
rolling_google.plot()
plt.legend(['High','Rolling Mean'])
# Plotting a rolling mean of 90 day window with original High attribute of google stocks
plt.show()</code></pre>
                            <img src="./imgs/rolling.png" alt="">
                            <pre class="language-python"><code># Expanding window functions
microsoft_mean = microsoft.High.expanding().mean()
microsoft_std = microsoft.High.expanding().std()
microsoft.High.plot()
microsoft_mean.plot()
microsoft_std.plot()
plt.legend(['High','Expanding Mean','Expanding Standard Deviation'])
plt.show()</code></pre>
                            <img src="./imgs/expanding.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                    <li>
                        <details>
                            <summary>OHLC charts</summary>
                            <pre class="language-python"><code>from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)
import plotly.graph_objs as go

# OHLC chart of June 2008
trace = go.Ohlc(x=google['06-2008'].index,
                open=google['06-2008'].Open,
                high=google['06-2008'].High,
                low=google['06-2008'].Low,
                close=google['06-2008'].Close)
data = [trace]
iplot(data, filename='simple_ohlc')</code></pre>
                            <img src="./imgs/OHLC.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                    <li>
                        <details>
                            <summary>Candle charts</summary>
                            <pre class="language-python"><code>from plotly.offline import init_notebook_mode, iplot
init_notebook_mode(connected=True)
import plotly.graph_objs as go

# Candlestick chart of march 2008
trace = go.Candlestick(x=google['03-2008'].index,
                open=google['03-2008'].Open,
                high=google['03-2008'].High,
                low=google['03-2008'].Low,
                close=google['03-2008'].Close)
data = [trace]
iplot(data, filename='simple_candlestick')</code></pre>
                            <img src="./imgs/candle.png" alt="">
                        </details>
                    </li>
                    <!-- =============================================== -->
                </ul>
            </details>
            <!-- - --------------------------------------- -->
            </ul>
        </details>

        <details>
            <summary><b>Tests</b></summary>
            <ul>
                <li>
                    <details>
                        <summary>Autocorrelation</summary>
                        <p>The autocorrelation function (ACF) measures how a series is correlated with itself at different lags.</p>
                        <p>In any case, given a lag, a large absolute autocorrelation means that the process tends to maintain an internal similarity, while a small value indicates that the evolution has removed most of the memory. In terms of prediction,
                            a large autocorrelation implies a simple prediction given the preceding instants, while a small autocorrelation (as in the case of the noise process) informs us that the value yt receives a very small influence from the previous
                            values (that is, in a white noise process the outputs are uncorrelated; therefore, the prediction is theoretically impossible).</p>
                        <details>
                            <summary>statsmodels</summary>
                            <pre class="language-python"><code>from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Autocorrelation of humidity of San Diego
plot_acf(humidity["San Diego"], lags=25, title="San Diego")
plt.show()

# As all lags are either close to 1 or at least greater than the confidence interval, they are statistically significant.</code></pre>
                            <img src="./imgs/autocorrelation.png" alt="">
                            <h3>How to analyize ACF:</h3>
                            <img src="../0_html/imgs/acf_analysis.png" alt="">
                        </details>
                        <details>
                            <summary>pandas</summary>
                            <pre class="language-python"><code>from random import seed
from random import random
from matplotlib import pyplot
from pandas.plotting import autocorrelation_plot
seed(1)
random_walk = list()
random_walk.append(-1 if random() < 0.5 else 1)
for i in range(1, 1000):
    movement = -1 if random() < 0.5 else 1
    value = random_walk[i-1] + movement
    random_walk.append(value)
autocorrelation_plot(random_walk)
pyplot.show()</code></pre>
                            <img src="./imgs/autocorrelation_2.png" alt="" width="400" height="400">
                            <h3>How to analyize ACF:</h3>
                            <img src="../0_html/imgs/acf_analysis.png" alt="">

                        </details><br>
                    </details>
                </li>
                <!-- =============================================== -->
                <li>
                    <details>
                        <summary>Partial Autocorrelation</summary>
                        <p>The partial autocorrelation function can be interpreted as a regression of the series against its past lags. <br>The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in
                            that particular lag while holding others constant.</p>
                        <pre class="language-python"><code>from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
                        
# Partial Autocorrelation of humidity of San Diego
plot_pacf(humidity["San Diego"],lags=25)
plt.show()
# Though it is statistically signficant, partial autocorrelation after first 2 lags is very low.
</code></pre>
                        <img src="./imgs/partial_autocorrelation.png" alt="">
                    </details>
                </li>
                <!-- =============================================== -->
                <li>
                    <details>
                        <summary>ETS Decomposition</summary>
                        <details>
                            <summary>Classic seasonal_decompose()</summary>
                            <pre>These are the components of a time series
        <b>Trend</b> - Consistent upwards or downwards slope of a time series
        <b>Seasonality</b> - Clear periodic pattern of a time series(like sine funtion)
        <b>Noise</b> - Outliers or missing values</pre>
                            <pre class="language-python"><code>import statsmodels.api as sm

    # Now, for decomposition...
    rcParams['figure.figsize'] = 11, 9
    decomposed_google_volume = sm.tsa.seasonal_decompose(google["High"],freq=360) # The frequncy is annual
    figure = decomposed_google_volume.plot()
    plt.show()
    # There is clearly an upward trend in the above plot.
    # You can also see the uniform seasonal change.
    # Non-uniform noise that represent outliers and missing values
    </code></pre>
                            <img src="./imgs/decomposition.png" alt="" weigth="200" height="400">
                        </details>

                        <details>
                            <summary>Additive seasonal_decompose()</summary>
                            <pre class="language-python"><code># Additive model
res = sm.tsa.seasonal_decompose(ts.values,freq=12,model="additive")
#plt.figure(figsize=(16,12))
fig = res.plot()
#fig.show()</code></pre>
                            <img src="./imgs/decompose.png" alt="" weigth="200" height="400">
                        </details>

                        <details>
                            <summary>Multiplictive seasonal_decompose()</summary>
                            <pre class="language-python"><code>import statsmodels.api as sm
# multiplicative
res = sm.tsa.seasonal_decompose(ts.values,freq=12,model="multiplicative")
#plt.figure(figsize=(16,12))
fig = res.plot()
#fig.show()</code></pre>
                            <img src="./imgs/decompose.png" alt="" weigth="200" height="400">
                        </details>

                        <details>
                            <summary>STL</summary>
                            <p>
                                The STL approach to time series decomposition has the following advantages over the X11 aproach: <br>It handles any type of seasonality. <br>The user can control the rate of change of the seasonal component.<br> It is robust
                                to outliers.
                            </p>
                            <p>We can change the smoothness of the trend-cycle and the seasonal components by passing an integer into the trend and seasonal arguments in the STL function. The seasonal argument is set to 7 by default (it is also recommended
                                that you use a seasonal smoother greater than or equal to 7).</p>
                            <p>The choice of the seasonal smoother is up to you. The larger is the integer, the more ‘smooth’ your seasonal component becomes. This causes less of the variation in your data to be attributed to its seasonal component. Thus,
                                you must decide how much variation in your data can be reasonably attributed to the seasonal component.</p>
                            <pre class="language-python"><code>from statsmodels.tsa.seasonal import STL
import matplotlib.pyplot as plt
import pandas as pd
df = df[:len(df) - 1] # Removes the last row in the data set
columns = ['Month', 'Passengers']
df.columns = columns
df.head()

df = df.set_index('Month') # Set the index to datetime object.
df = df.asfreq('MS') # Set frequency
# Set robust to True to handle outliers
res = STL(df, robust = True).fit() 
res.plot()
plt.show()</code></pre>

                            <img src="./imgs/stl_decompose.png" alt="" weigth="200" height="400">
                        </details>
                </li>
                </details>
                <!-- =============================================== -->
            </ul>
        </details>

    </div>
    <script src="../../prism.js"></script>
</body>

</html>