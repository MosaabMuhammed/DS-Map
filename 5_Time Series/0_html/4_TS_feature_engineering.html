<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        [TS]: Feature Engineering
    </title>
    <link rel="stylesheet" href="../../prism.css">
</head>

<body>
    <h1 id="4featureengineeringts">
        4. Feature Engineering (TS)
    </h1>
    <div style="width:1000px;margin:auto">
        <details>
            <summary>
                <b>
                    Lags
                </b> &amp;
                <b>
                    Rolling Window
                </b>
            </summary>
            <details>
                <summary>
                    Tutorials
                </summary>
                <p>
                    First off what each features mathematically does:
                </p>
                <ul>
                    <li>
                        <b>
                            lag_7:
                        </b> sales shifted 7 steps downwards for each group. The example above focuses on one group only as an example. That is why the first value appears on the 7th index.
                    </li>
                    <li>
                        <b>
                            lag_28:
                        </b> sales shifted 28 steps downwads. That is why the first value appears on the 28th index.
                    </li>
                    <li>
                        <b>
                            rmean_7_7:
                        </b> rolling mean sales of a window size of 7 over column lag_7. First value (0.2857) appears on the 13th index because means including nan are nan.
                    </li>
                    <li>
                        <b>
                            rmean_7_28:
                        </b> rolling mean sales of a window size of 7 over column lag_28. First value (0.357) appears on the 34th index because that is the first time the mean formula gets all 7 non-nan values.
                    </li>
                    <li>
                        <b>
                            rmean_28_7:
                        </b> rolling mean sales of a window size of 28 over column lag_7. First value (0.2857) appears on the 3th index because it is the first time the mean formula gets 28 non-nan values.
                    </li>
                    <li>
                        <b>
                            rmean_28_28:
                        </b> rolling mean sales of a window size of 28 over column lag_28. First value appears on 55th index because that is the first time the formula here all non-nan values.
                    </li>
                </ul>
                <br />
                <b>
                    The intuition as far as I can understand is the following:
                </b>
                <br />
                <ul>
                    <li>
                        1. Captures the week-on-week similarity and that too of just the past week. In other words, people are likely to shop this monday similar to the last monday (except it is some special occassion).
                    </li>
                    <li>
                        2. Captures the weekly similarity from a month-to-month perspective. Example: people in the 1st weekend of a month shop more so that weekend looks more similar to first weeks of other months than the previous weekend. (Though 28 is arguable here. A month
                        is generally 30. Interesting would be a variable window depending on when the comparative week starts. Dealing with edge cases like week divided into 2 months will be tricky).
                    </li>
                    <br />
                    <b>
                        Since individual data points are prone to erratic spikes or troughs, mean provides a more
                        "representative" picture.
                    </b>
                    <br />
                    <br />
                    <li>
                        3. Captures the information regarding the sales of the whole previous week ending 7 days in the past i.e. if we are at day 14, then the average is of sales from days 1-7 NOT days 7-14. This provides the information about the whole week and not just a
                        single day sale comparison like lag_7 to bring the lag_7 value into "better weekly context".
                    </li>
                    <li>
                        4. Captures the information regarding the sales of the entire previous 4 weeks ending 7 days in the past i.e. if we are at day 35, then the average is sales from days 1-28.
                    </li>
                    <li>
                        5. Captures the information regarding the sales of the whole week ending 4 weeks ago i.e. if we are on day 35, then the average is of sales from day 1-7. (Assuming for simplicity the month is 28 days), this provides the information of not just a month-to-month
                        comparison of the same day (day 7 of month one vs day 7 of month two), but the entire week leading up to day 7. Again the idea I believe is to capture the whole week and not just a single day sale comparison like lag_28 to bring
                        the lag_28 value into "better weekly context".
                    </li>
                    <li>
                        6. Captures the information regarding the sales of the entire previous 4 weeks ending 4 weeks in the past i.e. if we are at day 56, then the average is of days 1-28. (Assuming for simplicity the month is 28 days), the idea again is to bring the point
                        value of lag_28 into a better context (i.e. of day 28 when being compared to day 56) into a "better monthly context".
                    </li>
                </ul>
                <br />
                <b>
                    How would you "talk" about these features?
                </b>
                <br />
                <ul>
                    <li>
                        Hey let's see how the sales were last friday compared to this friday?
                    </li>
                    <li>
                        Hey let's see how the sales were first weekend of the last month compared to first weekend of this month?
                    </li>
                    <li>
                        May be comparing last saturday to this saturday is too specific. Week-on-week same day trends are more likely to be similar if the prior week went similar too. It would make sense to not just have the last saturday but also the mean of the whole week
                        leading upto that day to give the model the "hint" how normal the whole week was.
                    </li>
                </ul>
            </details>
            <details>
                <summary>
                    How to Choose the number of lags
                </summary>
                <ul>
                    <li>By just plotting the partial autocorrelation plot and picking previous time steps that have statistically significant correlation with the present.</li>
                    <li>Select a large number of lags and estimate a penalized model (e.g. using LASSO, ridge or elastic net regularization). The penalization should diminish the impact of irrelevant lags and this way effectively do the selection. There would
                        be some inconvenience in that cross validation is normally used for selecting penalty intensity, and cross validation is a bit tricky with time series. But this is still doable, no doubt about it.</li>
                    <li>Try a number of different lag combinations and either <br> (i) select the best of them according to an information criterion (AIC should do well in terms of forecasting as it is an efficient selector) or out-of-sample performance OR
                        <br> (ii) combine some or even all of them weighting the models based on their likelihood, information criteria or the like. Refer to model averaging and forecast combination literature for detailed recipes. <br> (ii) would often
                        do better than (i) in terms of forecasting, especially if you are selecting from a large number of alternatives.</li>
                </ul>
            </details>
            <details>
                <summary>
                    Code
                </summary>
                <pre class="language-python"><code>lags = [7, 28]
lag_cols = [f"lag_{lag}" for lag in lags]
for lag, lag_col in zip(lags, lag_cols):
df[lag_col] = dt[['id', 'sales']].groupby("id")['sales'].shift(lag)

wins = [7, 28]
for win in wins:
for lag, lag_col in zip(lags, lag_cols):
    df[f"rmean_{lag}_{win}"] = dt[['id', lag_col]].groupby("id")[lag_col].transform(lambda x: x.rolling(win).mean()
</code></pre>
            </details>
        </details>
        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>
                    Resampling
                </b>
            </summary>
            <p><b>Upsampling</b> - Time series is resampled from low frequency to high frequency(Monthly to daily frequency). It involves filling or interpolating missing data. </br>
                <b>Downsampling</b> - Time series is resampled from high frequency to low frequency(Weekly to monthly frequency). It involves aggregation of existing data.</p>
            <pre class="language-python"><code>#Resample and compute daily mean
daily = df['Chemical conc.'].resample('D')
daily_mean = daily.mean()
</code></pre>

            <pre class="language-python"><code>def grouped(df, key, freq, col):
    """ GROUP DATA WITH CERTAIN FREQUENCY """
    df_grouped = df.groupby([pd.Grouper(key=key, freq=freq)]).agg(mean = (col, 'mean'))
    df_grouped = df_grouped.reset_index()
    return df_grouped

# check grouped data
df_grouped_trans_w = grouped(df_trans, 'date', 'W', 'transactions')
# date	mean
# 0	2013-01-06	1883.203463
# 1	2013-01-13	1641.090062
# 2	2013-01-20	1639.024845
# 3	2013-01-27	1609.816770
# 4	2013-02-03	1685.263975</code></pre>
        </details>
        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>
                    Aggregation
                </b>
            </summary>
            <pre class="language-python"><code>#Calculate month wise statistics
monthly_stats = df.groupby(by='Month_Year')['Mean temparature'].aggregate([np.mean, np.median, np.std])
monthly_stats.reset_index(inplace=True)
monthly_stats.head(10)
</code></pre>
        </details>
        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>
                    Rolling
                </b>
            </summary>
            <pre class="language-python"><code>#Now we will calculate weekly moving average on the original time series of mean daily temparature
weekly_moving_average = df['Mean temparature'].rolling(7).mean()

#Now we will calculate monthly moving average on the original time series of mean daily temparature
monthly_moving_average = df['Mean temparature'].rolling(30).mean()

#Let's caluclate the weekly and monthly avergaes with a stride of length 2
weekly_moving_average_2stride = df['Mean temparature'].rolling(7).mean()[::2]
monthly_moving_average_2stride = df['Mean temparature'].rolling(30).mean()[::2]
</code></pre>
        </details>
        <!-- -------------------------------------------------------- -->

        <details>
            <summary>
                <b>Simple Moving Average</b>
            </summary>
            <pre class="language-python"><code>a = train.sort_values(["store_nbr", "family", "date"])
for i in [20, 30, 45, 60, 90, 120, 365, 730]:
    for j in [16, 30, 60]:
        a["SMA"+str(i)+"_sales_lag"+str(j)] = a.groupby(["store_nbr", "family"]).rolling(i).sales.mean().shift(j).values
</code></pre>
        </details>
        <!-- -------------------------------------------------------- -->

        <details>
            <summary>
                <b>Exponential Moving Average</b>
            </summary>
            <pre class="language-python"><code>keys = ['store_nbr']
val = 'sales'
lag = 1
alpha=0.95
df_temp.groupby(keys)[val].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())
</code></pre>
        </details>

        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>Date, Time features</b>
            </summary>
            <pre class="language-python"><code># Time Related Features
def create_date_features(df):
    df["hour"] = df.date.dt.hour.astype("int8")
    df["weekday"] = df.date.dt.weekday.astype("int8")
    df['day'] = df.date.dt.day.astype("int8")
    df['month'] = df.date.dt.month.astype("int8")
    df['day_of_month'] = df.date.dt.day.astype("int8")
    df['day_of_year'] = df.date.dt.dayofyear.astype("int16")
    df['week_of_month'] = (df.date.apply(lambda d: (d.day-1) // 7 + 1)).astype("int8")
    df['week_of_year'] = (df.date.dt.weekofyear).astype("int8")
    df['day_of_week'] = (df.date.dt.dayofweek + 1).astype("int8")
    df['year'] = df.date.dt.year.astype("int32")
    # the current "is_wknd" will treat Sat and Sun as holiday.
    # If you want it to map Fri and Sat, do df["is_wknd"] = df.date.dt.weekday.isin([4, 5]).astype("int8")
    df["is_wknd"] = (df.date.dt.weekday // 4).astype("int8") # df["is_wknd"] = df.date.dt.weekday.isin([5,6])*1
    df["quarter"] = df.date.dt.quarter.astype("int8")
    df['is_month_start'] = df.date.dt.is_month_start.astype("int8")
    df['is_month_end'] = df.date.dt.is_month_end.astype("int8")
    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype("int8")
    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype("int8")
    df['is_year_start'] = df.date.dt.is_year_start.astype("int8")
    df['is_year_end'] = df.date.dt.is_year_end.astype("int8")
    # 0: Winter - 1: Spring - 2: Summer - 3: Fall
    df['season'] = df['month'] % 12 // 3 + 1

    # df["season"] = np.where(df.month.isin([12,1,2]), 0, 1)
    # df["season"] = np.where(df.month.isin([6,7,8]), 2, df["season"])
    # df["season"] = pd.Series(np.where(df.month.isin([9, 10, 11]), 3, df["season"])).astype("int8")

    #### NOTE
    ## The new time features are cyclical. For example,the feature month cycles between 1 and 12 for every year. 
    ## While the difference between each month increments by 1 during the year, between two years the month feature jumps from 12 (December) to 1 (January). 
    ## This results in a -11 difference, which can confuse a lot of models.
    month_in_year = 12
    df['month_sin'] = np.sin(2*np.pi*df['month']/month_in_year)
    df['month_cos'] = np.cos(2*np.pi*df['month']/month_in_year)
    return df
d = create_date_features(d)
        </code></pre>
        </details>
        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>Target encoding for Date, Time features</b>
            </summary>
            <pre class="language-python"><code>def code_mean(data, cat_feature, real_feature):
    """
    Returns a dictionary where keys are unique categories of the cat_feature,
    and values are means over real_feature
    """
    return dict(data.groupby(cat_feature)[real_feature].mean())
</code></pre>
            <pre class="language-python"><code># calculate averages on train set only
test_index = int(len(data.dropna())*(1-test_size))
data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', "y").get, data.weekday))
data["hour_average"] = list(map(code_mean(data[:test_index], 'hour', "y").get, data.hour))
</code></pre>
        </details>
        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>Time-Series to Supervised Learning</b>
            </summary>
            <pre class="language-python"><code>from pandas import DataFrame
from pandas import concat

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    """
    Frame a time series as a supervised learning dataset.
    Arguments:
        data: Sequence of observations as a list or NumPy array.
        n_in: Number of lag observations as input (X).
        n_out: Number of observations as output (y).
        dropnan: Boolean whether or not to drop rows with NaN values.
    Returns:
        Pandas DataFrame of series framed for supervised learning.
    """
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg
</code></pre>
        </details>
        <!-- -------------------------------------------------------- -->
        <details>
            <summary>
                <b>DeterministicProcess with CalendarFourier</b>
            </summary>
            <pre class="language-python"><code>from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier

y = train.unstack(['store_nbr', 'family']).loc[sdate:edate]
fourier = CalendarFourier(freq = 'W', order = 4)
dp = DeterministicProcess(index = y.index, # dates from the training data
                            order = 1, # The order argument refers to polynomial order: 1 for linear, 2 for quadratic, 3 for cubic, and so on.
                            seasonal = False,
                            constant = False,  # dummy feature for the bias (y_intercept)
                            additional_terms = [fourier],
                            drop = True) # drop terms if necessary to avoid collinearity
x = dp.in_sample()
x = x.join(calendar)
x
</code></pre>
            <pre class="language-python"><code>xtest = dp.out_of_sample(steps = 16) # 16 because we are predicting next 16 days
xtest = xtest.join(calendar)
xtest
    </code></pre>
            <h3>Using LinearRegression to make a generalized line (It's usually called blending.)</h3>
            <pre class="language-python"><code>from joblib import Parallel, delayed
from tqdm.auto import tqdm
from sklearn.metrics import mean_squared_log_error as msle
from sklearn.model_selection import TimeSeriesSplit
from sklearn.svm import SVR
from sklearn.multioutput import MultiOutputRegressor

lnr = LinearRegression(fit_intercept = True, n_jobs = -1, normalize = True)
lnr.fit(x, y)

yfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)
ypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)

svr = MultiOutputRegressor(SVR(C = 0.2, kernel = 'rbf'), n_jobs = -1)
svr.fit(x, y)

yfit_svr = pd.DataFrame(svr.predict(x), index = x.index, columns = y.columns).clip(0.)
ypred_svr = pd.DataFrame(svr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)

yfit_mean = pd.DataFrame(np.mean([yfit_svr.values, yfit_lnr.values], axis = 0), index = x.index, columns = y.columns).clip(0.)
ypred_mean = pd.DataFrame(np.mean([ypred_lnr.values, ypred_svr.values], axis = 0), index = xtest.index, columns = y.columns).clip(0.)

y_ = y.stack(['store_nbr', 'family'])
y_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']
y_['svr'] = yfit_svr.stack(['store_nbr', 'family'])['sales']
y_['mean'] = yfit_mean.stack(['store_nbr', 'family'])['sales']

print('='*70, 'Linear Regression', '='*70)
print(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['lnr']))))
print('LNR RMSLE :', np.sqrt(msle(y, yfit_lnr)))
print('='*70, 'SVR', '='*70)
print(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['svr']))))
print('SVR RMSLE :', np.sqrt(msle(y, yfit_svr)))
print('='*70, 'Mean', '='*70)
print(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['mean']))))
print('Mean RMSLE :', np.sqrt(msle(y, yfit_mean)))
    </code></pre>
        </details>
    </div>
</body>
<script src="../../prism.js"></script>

</html>