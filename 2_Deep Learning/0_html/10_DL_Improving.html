<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1 id="improvingperformance">Improving Performance</h1>

<div style="width:1000px;margin:auto">
<details><summary><b>Partial Layers</b></summary><p>

<pre><code>from functools import partial

RegularizedDense = partial(tf.keras.layers.Dense,
                           activation=&quot;elu&quot;,
                           kernel_initializer=&quot;he_normal&quot;,
                           kernel_reqularizer=tf.keras.regularizers.l2(0.01))

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    RegularizedDense(300),
    RegularizedDense(100),
    RegularizedDense(10, activation=&quot;softmax&quot;,
                     kernel_initializer=&quot;glorot_uniform&quot;)
])
</code></pre>

</p></details>

<details><summary><b>Learning Curves</b></summary><p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/zero_to_deep_learning_video/course/9%20Improving%20performance.html#Learning-curves"><b style='color:#333'>1. Learning Curves</b></a> </p>

</p></details>

<details><summary><b>Depthwise Separable Convoluation</b></summary><p>
It performs a spatial convolution on each channel of its input, independently, before mising output channels via a pointwise convolution (1x1 convolution).<br><br>This is equivalent to separating the learning of spatial features and the learning of channel-wise features, which makes a lot of sense if you assume that spatial locations in the input are highly correlated, but different channels are fairly independent.<br><br>It requires significantly fewer parameters and involves fewer computations, thus resulting in smaller, speedier models.<br><br>These advantanges become especially important when you're training small models from scratch on limited data.


<pre><code>from tensorflow.keras.models import Sequential, Model
from tensorflow.keras import layers 

height = 64
width = 64
channels = 3
num_classes = 10

model = Sequential()
model.add(layers.SeparableConv2D(32, 3, activation=&quot;relu&quot;, input_shape=(height, width, channels,)))
model.add(layers.SeparableConv2D(64, 3, activation=&quot;relu&quot;))
model.add(layers.MaxPooling2D(2))

model.add(layers.SeparableConv2D(64, 3, activation='relu'))
model.add(layers.SeparableConv2D(128, 3, activation='relu'))
model.add(layers.MaxPooling2D(2))

model.add(layers.SeparableConv2D(64, 3, activation='relu'))
model.add(layers.SeparableConv2D(128, 3, activation='relu'))
model.add(layers.GlobalAveragePooling2D())

model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(num_classes, activation='softmax'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

</code></pre>

</p></details>

<details><summary><b>Batch Normalization</b></summary><p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/zero_to_deep_learning_video/course/9%20Improving%20performance.html#Batch-Normalization"><b style='color:#333'>Batch Normalization</b></a> </p>

<p>You can try both of the following, and see which one works best for you:<br>You can use it after Convolutional layer too.</p>

<pre><code># Adding BatchNormalization after activation.
input1      = tf.keras.layers.Input(shape=[2])
batch_norm1 = tf.keras.layers.BatchNormalization()(input1)
hidden1     = tf.keras.layers.Dense(50, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;)(batch_norm1)
batch_norm2 = tf.keras.layers.BatchNormalization()(hidden1)
hidden2     = tf.keras.layers.Dense(30, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;)(batch_norm2)
batch_norm3 = tf.keras.layers.BatchNormalization()(hidden2)
output1     = tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;)
</code></pre>



<pre><code># Adding BatchNormalization BEFORE activation.
input1      = tf.keras.layers.Input(shape=[2])
batch_norm1 = tf.keras.layers.BatchNormalization()(input1)
hidden1     = tf.keras.layers.Dense(50, kernel_initializer=&quot;he_normal&quot;)(batch_norm1)
batch_norm2 = tf.keras.layers.BatchNormalization()(hidden1)
elu1        = tf.keras.layers.Activation(&quot;elu&quot;)(batch_norm2)

hidden2     = tf.keras.layers.Dense(30, kernel_initializer=&quot;he_normal&quot;)(elu1)
batch_norm3 = tf.keras.layers.BatchNormalization()(hidden2)
elu2        = tf.keras.layers.Activation(&quot;elu&quot;)(batch_norm3)
output1     = tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;)
</code></pre>

</p></details>

<details><summary><b>Weight Normalization</b></summary><ul>
<li><a href="./notebooks/layers_weightnormalization.html">TF Docs</a></li>
</ul></details>

<details><summary><b>Dropout</b></summary><p>
<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/zero_to_deep_learning_video/course/9%20Improving%20performance.html#Weight-Regularization-&amp;-Dropout"><b style='color:#333'>3. Dropout</b></a> </p>
<ul>
<li>Since dropout is only active during training, the training loss is penalized compared to the validation loss, so computing the two can be misleading. In particular, a model maybe overfitting and yet have similar training and validation losses. So make sure to evalute the training loss without dropout.</li>
<li>Alternatively, you can call the fit() method inside a (with tf.keras.backend.learning_phase_scope(1)) block: this will force dropout to be active during both training and validation</li>
</ul>


<pre><code>model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(300, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation=&quot;softmax&quot;)
])
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;nadam&quot;, metrics=[&quot;accuracy&quot;])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))

</code></pre>

<li>If you want to regularize a self.normalizing network based on the SELU activation function, you should use <b>AlphaDropout</b>: this is a variant of dropout that preserves the mean and std of its inputs.</li>

<pre><code>model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(300, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(100, activation=&quot;selu&quot;, kernel_initializer=&quot;lecun_normal&quot;),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(10, activation=&quot;softmax&quot;)
])
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=optimizer, metrics=[&quot;accuracy&quot;])
n_epochs = 20
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))

</code></pre>

</p></details>

<details><summary><b>Monte-Carlo Dropout</b></summary><p>
<p><b>MC Dropout</b> simply is applying dropout on testing data, and taking n number of predictions for testing data, then averging them.</p>
<h4>1. If you have a model trained on a normal Droptout</h4>

<pre><code>with tf.keras.backend.learning_phase_scope(1):  # Force training mode = dropout on
    y_probas = np.stack([model.predict(X_test) for sample in range(500)])

# Take the mean along the first axis, which is number of samples.
y_proba = y_probas.mean(axis=0) 

# From probabilities to classes.
y_pred = np.round(y_proba)
# OR
y_pred = np.argmax(y_proba)
</code></pre>


<h4>2. MC Dropout Implementation</h4>

<pre><code>class MCDropout(tf.keras.layers.Dropout):
    def call(self, inputs):
        return super().call(inputs, training=True)

class MCAlphaDropout(tf.keras.layers.AlphaDropout):
    def call(self, inputs):
        return super().call(inputs, training=True)
</code></pre>


<pre><code># Build the model as normal
tf.random.set_seed(42)
np.random.seed(42)

mc_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(50, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, input_shape=[10]),
    MCDropout(0.2),
    tf.keras.layers.Dense(30, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;),
    MCDropout(.2),
    tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;)
])
</code></pre>


<pre><code># Then Predict.
y_probas = np.stack([mc_model.predict(X_test) for _ in range(100)])

y_pred = np.round(y_probas.mean(axis=0))
print(metrics.accuracy_score(y_test, y_pred))
</code></pre>

</p></details>

<details><summary><b>Max-Norm Regularization</b></summary><p>

<pre><code>layer = tf.keras.layers.Dense(100,
                              activation=&quot;selu&quot;,
                              kernel_initializer=&quot;lecun_normal&quot;,
                              kernel_constraint=tf.keras.constraints.max_norm(1., axis=0))

MaxNormDense = partial(tf.keras.layers.Dense,
                       activation=&quot;selu&quot;,
                       kernel_initializer=&quot;lecun_normal&quot;,
                       kernel_constraint=tf.keras.constraints.max_norm(1.))

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=[10]),
    MaxNormDense(50),
    MaxNormDense(30),
    tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;)
])
</code></pre>

</p></details>

<details><summary><b>Weight Regularization</b></summary>
<p>
<li><a href="https://keras.io/initializers/"><b style='color:#333'>1. Available initializers in Keras</b></a> </li>

<li><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Deep%20Learning%20Nanodegree/0_Data/deep-learning-master/weight-initialization/weight_initialization.html#Weight-Initialization"><b style='color:#333'>2. Which one is better (Experiment)</b></a> </p>

</p>
</details>

<details><summary><b>Data Augmentation</b></summary>
<p>

<p><a href="file:///media/mosaab/Volume/Courses/Computer%20Science/Advanced/Machine%20Learning/Udacity/Udacity%20-%20Deep%20Learning%20Nanodegree%20Program/Part%2003-Module%2001-Lesson%2002_Convolutional%20Neural%20Networks/20.%20Image%20Augmentation%20in%20Keras.html"><b style='color:#333'>1. Concept (Udacity)</b></a> </p>

<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/zero_to_deep_learning_video/course/9%20Improving%20performance.html#Data-augmentation"><b style='color:#333'>2. Code Example</b></a> </p>

</p></details>

<details><summary><b>Gradient Clipping</b> [solving exploding gradient]</summary><p>

<pre><code># Try to use &quot;clipvalue&quot; and &quot;clipnorm&quot;, and see which one works best for your data.
optimizer = keras.optimizers.SGD(clipvalue=1.0)
model.compile(loss=&quot;mse&quot;, optimizer=optimizer)

# now, every gradient will be between -1 and 1.
</code></pre>

</p></details>

<details><summary><b>Regularization</b></summary><p>
<h4>L1</h4>

<pre><code>layer = tf.keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_reqularizer=tf.keras.reqularizers.l1(0.01))
</code></pre>


<h4>L2</h4>

<pre><code>layer = tf.keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_reqularizer=tf.keras.reqularizers.l2(0.01))
</code></pre>


<h4>Elastic Net [L1 and L2]</h4>

<pre><code>layer = tf.keras.layers.Dense(100, activation=&quot;elu&quot;, kernel_initializer=&quot;he_normal&quot;, kernel_reqularizer=tf.keras.reqularizers.l1_l2(0.01))
</code></pre>

</p></details>

<details><summary><b>Residual Conncetions</b></summary><p>
- It helps with <b>vanishing gradient</b> and <b>representational bottlenecks</b> problems.<br>
- Adding it to any model that has more than <b>10 layers</b> is likely to be beneficial.

<pre><code>from tensorflow.keras import layers

x = ...
y = layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
y = layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(y)
y = layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(y)

y = layers.add([y, x])
</code></pre>


<h4>If they don't have the same shape</h4>

<pre><code># Note, if the 2 layers that are going to be added have different length/sizes, do the following: Dense layer without activation / convolutional feature maps 1x1 convolutions without activation, to insure they have the same shape.
from tensorflow.keras import layers

x = ...
y = layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(x)
y = layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(y)
y = layers.MaxPooling2D(2, stride=2)(y)

residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)
y = layers.add([y, residual])
</code></pre>

</p></details>

<details><summary><b>Hyperparamter Optimization</b> [Hyperas]</summary><p>

<pre><code>!pip install hyperas

from hyperopt import Trials, STATUS_OK, tpe
from keras.datasets import mnist
from keras.layers.core import Dense, Dropout, Activation
from keras.models import Sequential
from keras.utils import np_utils

from hyperas import optim
from hyperas.distributions import choice, uniform


def data():
    &quot;&quot;&quot;
    Data providing function:

    This function is separated from create_model() so that hyperopt
    won't reload data for each evaluation run.
    &quot;&quot;&quot;
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(60000, 784)
    x_test = x_test.reshape(10000, 784)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    nb_classes = 10
    y_train = np_utils.to_categorical(y_train, nb_classes)
    y_test = np_utils.to_categorical(y_test, nb_classes)
    return x_train, y_train, x_test, y_test


def create_model(x_train, y_train, x_test, y_test):
    &quot;&quot;&quot;
    Model providing function:

    Create Keras model with double curly brackets dropped-in as needed.
    Return value has to be a valid python dictionary with two customary keys:
        - loss: Specify a numeric evaluation metric to be minimized
        - status: Just use STATUS_OK and see hyperopt documentation if not feasible
    The last one is optional, though recommended, namely:
        - model: specify the model just created so that we can later use it again.
    &quot;&quot;&quot;
    model = Sequential()
    model.add(Dense(512, input_shape=(784,)))
    model.add(Activation('relu'))
    model.add(Dropout({{uniform(0, 1)}}))
    model.add(Dense({{choice([256, 512, 1024])}}))
    model.add(Activation({{choice(['relu', 'sigmoid'])}}))
    model.add(Dropout({{uniform(0, 1)}}))

    # If we choose 'four', add an additional fourth layer
    if {{choice(['three', 'four'])}} == 'four':
        model.add(Dense(100))

        # We can also choose between complete sets of layers

        model.add({{choice([Dropout(0.5), Activation('linear')])}})
        model.add(Activation('relu'))

    model.add(Dense(10))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],
                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})

    result = model.fit(x_train, y_train,
              batch_size={{choice([64, 128])}},
              epochs=2,
              verbose=2,
              validation_split=0.1)
    #get the highest validation accuracy of the training epochs
    validation_acc = np.amax(result.history['val_acc']) 
    print('Best validation acc of epoch:', validation_acc)
    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}


if __name__ == '__main__':
    best_run, best_model = optim.minimize(model=create_model,
                                          data=data,
                                          algo=tpe.suggest,
                                          max_evals=5,
                                          trials=Trials())
    X_train, Y_train, X_test, Y_test = data()
    print(&quot;Evalutation of best performing model:&quot;)
    print(best_model.evaluate(X_test, Y_test))
    print(&quot;Best performing model chosen hyper-parameters:&quot;)
    print(best_run)
</code></pre>

</p></details>


<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/zero_to_deep_learning_video/course/9%20Improving%20performance.html#Embeddings"><b style='color:#333'>5. Embedding</b></a> </p>

<p><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/zero_to_deep_learning_video/course/9%20Improving%20performance.html#Sentiment-prediction-on-movie-Reviews"><b style='color:#333'>Ex: Sentiment Analysis on Movie Review</b></a> </p>

</div><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>