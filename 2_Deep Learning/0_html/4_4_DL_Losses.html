<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 id="losses">
   Losses
  </h1>
  <div style="width:1000px;margin:auto">
   <details>
    <summary>
     <b>
      binary_crossentropy
     </b>
    </summary>
    <p>
    </p>
    <ul>
     <li>
      Used with
      <b>
       binary classification
      </b>
      problem.
     </li>
     <li>
      Can be used for
      <b>
       Multi-class, multilabel classification
      </b>
      problem
     </li>
     .
     <li>
      The last layer has only
      <b>
       1 neuron
      </b>
      [Binary Classification] and
      <b>
       n-1 neurons
      </b>
      [Multi-Label Classification] where n is the number of classes.
     </li>
     <li>
      The last layer has
      <b>
       sigmoid
      </b>
      activation function.
     </li>
     <li>
      The
      <b>
       y_labels
      </b>
      is an array of ONLY
      <b>
       zeros
      </b>
      and
      <b>
       ones
      </b>
     </li>
    </ul>
    <pre><code># Last layer.
    keras.layers.Dense(1, activation="sigmoid")

model.compile(optimizer="adam",
            loss="binary_crossentropy",
            metrics=["accuracy"])
</code></pre>
    <h4>
     To predict
    </h4>
    <pre><code>result = model.predict(X_valid)
# result is a class 0 or 1.
</code></pre>
   </details>
   <details>
    <summary>
     <b>
      categorical_crossentropy
     </b>
    </summary>
    <p>
    </p>
    <ul>
     <li>
      Used for
      <b>
       binary
      </b>
      or
      <b>
       multi-class
      </b>
      classification problems.
     </li>
     <li>
      The last layer has nuerons as the
      <b>
       number of classes
      </b>
      in y_labels.
     </li>
     <li>
      The last layer must have
      <b>
       "softmax"
      </b>
      activation function.
     </li>
     <li>
      The y_labels MUST be
      <b>
       one-hot encoded
      </b>
      .
     </li>
    </ul>
    <pre><code># To one-hot encode the y_labels.
from tensorflow.keras.utils import to_categorical

y_labels = to_categorical(y_labels, len(np.unique(y_labels)))
</code></pre>
    <pre><code># Last Layer.
    keras.layers.Dense(len(np.unique(y_labels)), activation="softmax")

model.compile(loss="categorical_crossentropy", ...)
</code></pre>
    <pre><code># To predict.
results = model.predict(X_valid)
class_idx = np.argmax(results)
</code></pre>
   </details>
   <details>
    <summary>
     <b>
      sparse_categorical_crossentropy
     </b>
    </summary>
    <p>
    </p>
    <ul>
     <li>
      Used for
      <b>
       multi-classification classification
      </b>
      problem.
     </li>
     <li>
      The last layer has a number of neurons as the
      <b>
       number of classes
      </b>
      in y_labels.
     </li>
     <li>
      The last layer has
      <b>
       "softmax"
      </b>
      activation function.
     </li>
     <li>
      <b>
       y_labels
      </b>
      MUST be an
      <b>
       array of intengers
      </b>
      .
     </li>
    </ul>
    <pre><code># Last Layer.
    keras.layers.Dense(len(np.unique(y_labels)), activation="softmax")

model.compile(loss="sparse_categorical_crossentropy", ...)
</code></pre>
    <pre><code># To predict.
results = model.predict(X_valid)
class_idx = np.argmax(results)
</code></pre>
   </details>
   <details>
    <summary>
     Create
     <b>
      Custom Loss Function
     </b>
    </summary>
    <pre><code># Define Huber Loss function
def huber_fn(y_true, y_pred):
    error = y_true - y_pred
    is_small_error = tf.abs(error) &lt; 1
    squared_loss = tf.square(error) / 2
    linear_loss = tf.abs(error) - 0.5
    return tf.where(is_small_error, squared_loss, linear_loss)

# Use the loss.
model.compile(loss=huber_fn, optimizer="nadam")
model.fit(X_train, y_train, [...])

# Load the loss when loading the saved model.
model = keras.models.load_model("my_model_with_a_custom_loss.h5",
                                custom_objects={"huber_fn": huber_fn})
</code></pre>
    <pre><code># If you want Keras to save the parameters of your loss.
class HuberLoss(keras.losses.Loss):
    def __init__(self, threshold=1.0, **kwargs):
        self.threshold = threshold
        super().__init__(**kwargs)

    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) &lt; self.threshold
        squared_loss = tf.square(error) / 2
        linear_loss = self.threshold * tf.abs(error) - self.threshold**2
        / 2
        return tf.where(is_small_error, squared_loss, linear_loss)

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}

model.compile(loss=HuberLoss(2.), optimizer="nadam")

model = keras.models.load_model("my_model_with_a_custom_loss_class.h5",
    custom_objects={"HuberLoss": HuberLoss})
</code></pre>
   </details>
   <details>
    <summary>
     Create
     <b>
      Custom Internal Loss
     </b>
     to be calculated at the end of each epoch
    </summary>
    <pre><code>
class ReconstructingRegressor(keras.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(30, activation="selu",
                kernel_initializer="lecun_normal") for _ in range(5)]
        self.out = keras.layers.Dense(output_dim)

    def build(self, batch_input_shape):
        n_inputs = batch_input_shape[-1]
        self.reconstruct = keras.layers.Dense(n_inputs)
        super().build(batch_input_shape)

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        reconstruction = self.reconstruct(Z)
        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        self.add_loss(0.05 * recon_loss)
        return self.out(Z)

''' 
Epoch 1/5
11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error:
1.7360
Epoch 2/5
11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error:0.8964
[...]
'''
</code></pre>
   </details>
  </div>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>