<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        Dataset
    </title>

    <link rel="stylesheet" href="../../../prism.css">
</head>

<body>
    <div style="width:1000px;margin:auto;">

        <h1 id="1startercode">
            Dataset
        </h1>
        
        <details>
            <summary><b style="font-size: 27px;">PyTorch Datasets</b></summary>
            <pre class="language-python"><code>import torch.utils.data as data</code></pre>
            <p>The data package defines two classes which are the standard interface for handling data in PyTorch: <code>data.Dataset</code>, and <code>data.DataLoader</code>. <br>The dataset class provides an uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training.</p>

            <details>
                <summary><b>DataLoader parameters</b></summary>
                <p>
                    <ul>
                        <li><code>batch_size</code>: Number of samples to stack per batch</li>
                        <li><code>shuffle</code>: If True, the data is returned in a random order. This is important during training for introducing stochasticity.</li>
                        <li><code>num_workers</code>: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.</li>
                        <li><code>pin_memory</code>: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. <br> This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.</li>
                        <li><code>drop_last</code>: If True, the last batch is dropped in case it is smaller than the specified batch size. <br> This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.</li>
                    </ul>
                </p>
            </details>
        </details>
        

        <hr>
        <h2>Custom Dataset</h2>

        <details>
            <summary><b style="font-size:20px">Tabular</b></summary>
            <ul>
                <li><details>
                    <summary><b>Classification</b></summary>
                    <ul>
                        <li><details>
                            <summary><b>XOR Dataset</b></summary>
                        <pre class="language-python"><code>import torch.utils.data as data
                            
class XORDataset(data.Dataset):

    def __init__(self, size, std=0.1):
        """
        Inputs:
            size - Number of data points we want to generate
            std - Standard deviation of the noise (see generate_continuous_xor function)
        """
        super().__init__()
        self.size = size
        self.std = std
        self.generate_continuous_xor()

    def generate_continuous_xor(self):
        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1
        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.
        # If x=y, the label is 0.
        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)
        label = (data.sum(dim=1) == 1).to(torch.long)
        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.
        data += self.std * torch.randn(data.shape)

        self.data = data
        self.label = label

    def __len__(self):
        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]
        return self.size

    def __getitem__(self, idx):
        # Return the idx-th data point of the dataset
        # If we have multiple things to return (data point and label), we can return them as tuple
        data_point = self.data[idx]
        data_label = self.label[idx]
        return data_point, data_label</code></pre>
        <pre class="language-python"><code>dataset = XORDataset(size=200)
print("Size of dataset:", len(dataset))
print("Data point 0:", dataset[0])
# Size of dataset: 200
# Data point 0: (tensor([0.9632, 0.1117]), tensor(1))</code></pre>
<h3>Visualize the dataset</h3>
<pre class="language-python"><code>def visualize_samples(data, label):
    if isinstance(data, torch.Tensor):
        data = data.cpu().numpy()
    if isinstance(label, torch.Tensor):
        label = label.cpu().numpy()
    data_0 = data[label == 0]
    data_1 = data[label == 1]

    plt.figure(figsize=(4,4))
    plt.scatter(data_0[:,0], data_0[:,1], edgecolor="#333", label="Class 0")
    plt.scatter(data_1[:,0], data_1[:,1], edgecolor="#333", label="Class 1")
    plt.title("Dataset samples")
    plt.ylabel(r"$x_2$")
    plt.xlabel(r"$x_1$")
    plt.legend()


visualize_samples(dataset.data, dataset.label)
plt.show()</code></pre>
                        </details></li>

                    </ul>
                </details></li>
            </ul>

        <br></details>

        <li><a style="font-size:20px;font-weight: bold;" href="./8_Torch_Images.html">Images</a></li>


    </div>
    <script src="../../../prism.js"></script>
</body>

</html>