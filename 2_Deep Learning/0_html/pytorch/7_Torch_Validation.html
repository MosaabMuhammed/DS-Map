<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Inference &amp; Validation</h1>
<li><a href="file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/deep-learning-v2-pytorch/convolutional-neural-networks/mnist-mlp/mnist_mlp_solution_with_validation.html#Load-and-Visualize-the-Data"><font color='#333'><b style='font-size:18px'>train_test_split [PyTorch] Version</b></font></a></li>

<hr>

<details><summary><b>Without</b> Dropout [Normal Case]</summary>
<p>

<h4>1. Design the model</h4>

<pre><code class="python">from torch import nn, optim
import torch.nn.functional as F

class Classifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 10)

    def forward(self, x):
        # make sure input tensor is flattened
        x = x.view(x.shape[0], -1)

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.log_softmax(self.fc4(x), dim=1)

        return x
</code></pre>


<h4>2. Inference & Validation</h4>

<pre><code class="python">model = Classifier()
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.parameters(), lr=0.003)

epochs = 30
steps = 0

train_losses, test_losses = [], []
for e in range(epochs):
    running_loss = 0
    for images, labels in trainloader:

        optimizer.zero_grad()

        log_ps = model(images)
        loss = criterion(log_ps, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    else:
        test_loss, accuracy = 0, 0
        with torch.no_grad():
            for images, labels in testloader:
                log_ps = model(images)
                test_loss += criterion(log_ps, labels)

                ps = torch.exp(log_ps)
                top_class = ps.topk(1, dim=1)[1]
                equals    = top_class == labels.view(*top_class.shape)
                accuracy  += torch.mean(equals.type(torch.FloatTensor))

        train_losses.append(running_loss/len(trainloader))
        test_losses.append(test_loss/len(testloader))

        print(f'&gt; {bg(&quot;Epoch&quot;, &quot;s&quot;, &quot;gray&quot;)}:{e+1}/{epochs}\t{bg(&quot;Train Loss&quot;, &quot;s&quot;, &quot;red&quot;)}:{(running_loss/len(trainloader)):.3f} \t{bg(&quot;Test Loss&quot;, &quot;s&quot;, &quot;red&quot;)}:{(test_loss/len(testloader)):.4f}\t{bg(&quot;Test Accuracy&quot;, &quot;s&quot;, &quot;green&quot;)}:{(accuracy/len(testloader)):.4f}')
</code></pre>


<h4>3. Plot Training & Testing Loss to notice Overfitting</h4>

<pre><code class="python">plt.plot(train_losses, label='Training losses')
plt.plot(test_losses, label='Validation Losses')
plt.legend(frameon=False)
</code></pre>

</p>
</details>

<details><summary><b>With</b> Dropout</summary>
<p>
<h4>1. Design the Model</h4>

<pre><code class="python">class Classifier2(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 10)

        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.dropout(F.relu(self.fc3(x)))
        x = F.log_softmax(self.fc4(x), dim=1)

        return x
</code></pre>


<h4>2. Validation</h4>

<pre><code class="python">## Train your model with dropout, and monitor the training progress with the validation loss and accuracy
model     = Classifier2()
criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=.003)

epochs = 20
steps  = 0

train_losses, test_losses = [], []
for e in range(epochs):
    running_loss = 0
    for images, labels in trainloader:
        log_ps = model(images)
        loss   = criterion(log_ps, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    else:
        test_loss, accuracy = 0, 0
        # Turn off gradient for validation, saves memory and computations.
        with torch.no_grad():
            model.eval() # Go to Evaluation Mode.
            for images, labels in testloader:
                log_ps           = model(images)
                test_loss       += criterion(log_ps, labels)

                ps               = torch.exp(log_ps)
                top_p, top_class = ps.topk(1, dim=1)
                equals           = top_class == labels.view(*top_class.shape)
                accuracy        += torch.mean(equals.type(torch.FloatTensor))

        model.train() # Go back to training mode.
        train_losses.append(running_loss/len(trainloader))
        test_losses.append(test_loss/len(testloader))

        print(f'~&gt; {bg(&quot;Epoch&quot;, &quot;s&quot;, &quot;grey&quot;)}:{e+1}/{epochs}', end='\t')
        print(f'{bg(&quot;Train Loss&quot;, &quot;s&quot;, &quot;red&quot;)}:{(running_loss/len(trainloader)):.3f}', end='\t')
        print(f'{bg(&quot;Test Loss&quot;, &quot;s&quot;, &quot;red&quot;)}:{(test_loss/len(testloader)):.3f}', end='\t')
        print(f'{bg(&quot;Acc&quot;, &quot;s&quot;, &quot;green&quot;)}:{(accuracy/len(testloader)):.3f}')
</code></pre>


<h4>3. Inference</h4>

<pre><code class="python"># Test out your network!
model.eval()

dataiter = iter(testloader)
images, labels = dataiter.next()
img = images[0]
# Convert 2D image to 1D vector
img = img.view(1, 784)

# Calculate the class probabilities (softmax) for img
with torch.no_grad():
    output = model.forward(img)

ps = torch.exp(output)

# Plot the image and probabilities
helper.view_classify(img.view(1, 28, 28), ps, version='Fashion')
</code></pre>

</p>
</details><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>