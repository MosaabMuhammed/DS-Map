<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 id="layers">
   <mark>
    Layers
   </mark>
  </h1>
  <p>
   <details>
    <summary>
     <strong>
      Dense
     </strong>
    </summary>
    <p>
     <br/>
     <a href="https://keras.io/layers/core/">
      <strong>
       Docs
      </strong>
     </a>
    </p>
    <ul>
     <li>
      <strong>
       Dense
      </strong>
      is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer. In the last layer we have to specify output dimensions/classes of the model.
     </li>
    </ul>
    <h4 id="1-class">
     1. Class.
    </h4>
    <pre><code class="python">keras.layers.Dense(
                units, 
                activation=None, 
                use_bias=True, 
                kernel_initializer='glorot_uniform', 
                bias_initializer='zeros', 
                kernel_regularizer=None, 
                bias_regularizer=None, 
                activity_regularizer=None, 
                kernel_constraint=None, 
                bias_constraint=None
                )
</code></pre>
    <h4 id="example">
     Example.
    </h4>
    <pre><code class="python">from keras.layers.core import  Dense

model = Sequential()
model.add(Lambda(standardize, input_shape=(28, 28, 1)))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))   &lt;----
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      Lambda
     </strong>
    </summary>
    <br/>
    <p>
     <br/>
     <a href="https://keras.io/layers/core/">
      <strong>
       Docs
      </strong>
     </a>
    </p>
    <ul>
     <li>
      <strong>
       Lambda layer
      </strong>
      performs simple arithmetic operations like sum, average, exponentiation etc.
     </li>
    </ul>
    <h4 id="1-class_1">
     1. Class.
    </h4>
    <pre><code class="python">keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)
</code></pre>
    <h4 id="example_1">
     Example.
    </h4>
    <pre><code class="python">from keras.layers.core import  Lambda

def standardize(x):
    return (x - mean_px)/std_px

model = Sequential()
model.add(Lambda(standardize, input_shape=(28, 28, 1)))  &lt;--
model.add(Flatten())
model.add(Dense(10, activation='softmax'))
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      Flatten
     </strong>
    </summary>
    <br/>
    <p>
     <br/>
     <a href="https://keras.io/layers/core/">
      <strong>
       Docs
      </strong>
     </a>
    </p>
    <ul>
     <li>
      <strong>
       Flatten
      </strong>
      will transform input into 1D array.
     </li>
    </ul>
    <h4 id="1-class_2">
     1. Class.
    </h4>
    <pre><code class="python">keras.layers.Flatten(data_format=None)
</code></pre>
    <h4 id="example_2">
     Example.
    </h4>
    <pre><code class="python">from keras.layers.core import  Flatten

model = Sequential()
model.add(Lambda(standardize, input_shape=(28, 28, 1)))
model.add(Flatten())   &lt;--
model.add(Dense(10, activation='softmax'))
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     <strong>
      Dropout
     </strong>
    </summary>
    <br/>
    <p>
     <br/>
     <a href="https://keras.io/layers/core/">
      <strong>
       Docs
      </strong>
     </a>
    </p>
    <ul>
     <li>
      <strong>
       Dropout
      </strong>
      consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.
     </li>
    </ul>
    <h4 id="1-class_3">
     1. Class.
    </h4>
    <pre><code class="python">keras.layers.Dropout(
                rate, 
                noise_shape=None, 
                seed=None
                )
</code></pre>
    <h4 id="example_3">
     Example.
    </h4>
    <pre><code class="python">from keras.layers.core import  Flatten

model = Sequential()
model.add(Lambda(standardize, input_shape=(28, 28, 1)))
model.add(Flatten())   &lt;--
model.add(Dense(10, activation='softmax'))
</code></pre>
    <p>
    </p>
    <br/>
   </details>
  </p>
  <p>
   <details>
    <summary>
     Create
     <strong>
      Custom
     </strong>
     Layer
    </summary>
    <p>
    </p>
    <pre><code># A layer with no weights.
exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))
</code></pre>
    <pre><code># A layer with weights using SubClass.
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, batch_input_shape):
        self.kernel = self.add_weight(name="kernel", shape=[batch_input_shape[-1], self.units], initializer="glorot_normal")
        self.bias = self.add_weight(name="bias", shape=[self.units], initializer="zeros")
        super().build(batch_input_shape) # must be at the end

    def call(self, X):
        return self.activation(X @ self.kernel + self.bias)

    def compute_output_shape(self, batch_input_shape):
        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "units": self.units, "activation":
                keras.activations.serialize(self.activation)}
</code></pre>
    <pre><code># Another example.
class MyGaussianNoise(keras.layers.Layer):
    def __init__(self, stddev, **kwargs):
        super().__init__(**kwargs)
        self.stddev = stddev

    def call(self, X, training=None):
        if training:
            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
            return X + noise
        else:
            return X

    def compute_output_shape(self, batch_input_shape):
        return batch_input_shape
</code></pre>
    <p>
    </p>
   </details>
  </p>
  <p>
   <details>
    <summary>
     Create
     <strong>
      Custom
     </strong>
     activation, weight initializer, regularizer, constraints
    </summary>
    <p>
    </p>
    <pre><code>def my_softplus(z): # return value is just tf.nn.softplus(z)
    return tf.math.log(tf.exp(z) + 1.0)

def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01 * weights))

def my_positive_weights(weights): # return value is just
tf.nn.relu(weights)
    return tf.where(weights &lt; 0., tf.zeros_like(weights), weights)

layer = keras.layers.Dense(30, activation=my_softplus,
                            kernel_initializer=my_glorot_initializer,
                            kernel_regularizer=my_l1_regularizer,
                            kernel_constraint=my_positive_weights)
</code></pre>
    <pre><code># If you want to make the parameters be saved, use SubClass API.
# Note that you must implement the call() method for losses, layers (including activation functions), and models, or the __call__() method 
# for regularizers, initializers, and constraints. For metrics, things are a bit different
class MyL1Regularizer(keras.regularizers.Regularizer):
    def __init__(self, factor):
        self.factor = factor
    def __call__(self, weights):
        return tf.reduce_sum(tf.abs(self.factor * weights))
    def get_config(self):
        return {"factor": self.factor}
</code></pre>
    <p>
    </p>
   </details>
  </p>
  <p>
   <details>
    <summary>
     Create
     <strong>
      Custom
     </strong>
     Model (Multiple Layers contained together)
    </summary>
    <p>
    </p>
    <pre><code># Create a Residual Block.
class ResidualBlock(keras.layers.Layer):
    def __init__(self, n_layers, n_neurons, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(n_neurons, activation="elu", kernel_initializer="he_normal")
                        for _ in range(n_layers)]

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        return inputs + Z
</code></pre>
    <pre><code>class ResidualRegressor(keras.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = keras.layers.Dense(30, activation="elu", kernel_initializer="he_normal")
        self.block1 = ResidualBlock(2, 30)
        self.block2 = ResidualBlock(2, 30)
        self.out = keras.layers.Dense(output_dim)

    def call(self, inputs):
        Z = self.hidden1(inputs)
        for _ in range(1 + 3):
            Z = self.block1(Z)
        Z = self.block2(Z)
        return self.out(Z)
</code></pre>
    <p>
    </p>
   </details>
  </p>
  <p>
   <details>
    <summary>
     Create
     <strong>
      Custom
     </strong>
     Training Loop
    </summary>
    <p>
    </p>
    <pre><code># &lt;h4&gt;Build your model&lt;/h4&gt;
l2_reg = keras.regularizers.l2(0.05)
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="elu",
                        kernel_initializer="he_normal",
                        kernel_regularizer=l2_reg),
    keras.layers.Dense(1, kernel_regularizer=l2_reg)
])
</code></pre>
    <pre><code># &lt;h4&gt;Define random_batch &amp; status_bar function&lt;/h4&gt;
def random_batch(X, y, batch_size=32):
    idx = np.random.randint(len(X), size=batch_size)
    return X[idx], y[idx]

def print_status_bar(iteration, total, loss, metrics=None):
    metrics = " - ".join(["{}: {:.4f}".format(m.name, m.result())
                        for m in [loss] + (metrics or [])])
    end = "" if iteration &lt; total else "\n"
    print("\r{}/{} - ".format(iteration, total) + metrics, end=end)
</code></pre>
    <pre><code># &lt;h4&gt;Define your hyperparameters&lt;/h4&gt;
n_epochs = 5
batch_size = 32
n_steps = len(X_train) // batch_size
optimizer = keras.optimizers.Nadam(lr=0.01)
loss_fn = keras.losses.mean_squared_error
mean_loss = keras.metrics.Mean()
metrics = [keras.metrics.MeanAbsoluteError()]
</code></pre>
    <pre><code># &lt;h4&gt;Write the training loop&lt;/h4&gt;
for epoch in range(1, n_epochs + 1):
    print("Epoch {}/{}".format(epoch, n_epochs))
    for step in range(1, n_steps + 1):
        X_batch, y_batch = random_batch(X_train_scaled, y_train)
        with tf.GradientTape() as tape:
            y_pred = model(X_batch, training=True)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        mean_loss(loss)

        for metric in metrics:
            metric(y_batch, y_pred)
        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)
    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)
    for metric in [mean_loss] + metrics:
        metric.reset_states()
</code></pre>
    <p>
    </p>
   </details>
  </p>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>