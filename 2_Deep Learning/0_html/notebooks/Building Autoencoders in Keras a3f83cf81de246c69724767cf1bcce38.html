<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Building Autoencoders in Keras</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 100%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 110%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 110%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style>
<link rel="stylesheet" href="../../../prism.css"></head><body><article id="a3f83cf8-1de2-46c6-9724-767cf1bcce38" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="https://blog.keras.io/favicon.ico"/></div><h1 class="page-title">Building Autoencoders in Keras</h1><table class="properties"><tbody><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Type</th><td></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Status</th><td><span class="selected-value select-value-color-default">Later</span></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>Category</th><td></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>Rate</th><td></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>Source/Author</th><td></td></tr><tr class="property-row property-row-text"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>Quantity</th><td>1</td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M3.73333,3.86667 L7.46667,3.86667 C8.49613,3.86667 9.33333,4.70387 9.33333,5.73333 C9.33333,6.7628 8.49613,7.6 7.46667,7.6 L6.53333,7.6 C6.01813,7.6 5.6,8.0186 5.6,8.53333 C5.6,9.04807 6.01813,9.46667 6.53333,9.46667 L7.46667,9.46667 C9.5284,9.46667 11.2,7.79507 11.2,5.73333 C11.2,3.6716 9.5284,2 7.46667,2 L3.73333,2 C1.6716,2 0,3.6716 0,5.73333 C0,7.124 0.762067,8.33453 1.88953,8.97713 C1.87553,8.83107 1.86667,8.6836 1.86667,8.53333 C1.86667,7.92013 1.98753,7.33447 2.2036,6.7978 C1.99267,6.4954 1.86667,6.12953 1.86667,5.73333 C1.86667,4.70387 2.70387,3.86667 3.73333,3.86667 Z M12.1095,5.28907 C12.1231,5.4356 12.1333,5.58307 12.1333,5.73333 C12.1333,6.34607 12.0101,6.9294 11.7931,7.46513 C12.0059,7.768 12.1333,8.13573 12.1333,8.53333 C12.1333,9.5628 11.2961,10.4 10.2667,10.4 L6.53333,10.4 C5.50387,10.4 4.66667,9.5628 4.66667,8.53333 C4.66667,7.50387 5.50387,6.66667 6.53333,6.66667 L7.46667,6.66667 C7.98187,6.66667 8.4,6.24807 8.4,5.73333 C8.4,5.2186 7.98187,4.8 7.46667,4.8 L6.53333,4.8 C4.4716,4.8 2.8,6.4716 2.8,8.53333 C2.8,10.59507 4.4716,12.2667 6.53333,12.2667 L10.2667,12.2667 C12.3284,12.2667 14,10.59507 14,8.53333 C14,7.14267 13.2375,5.93167 12.1095,5.28907 Z"></path></svg></span>Link</th><td><a href="https://blog.keras.io/building-autoencoders-in-keras.html" class="url-value">https://blog.keras.io/building-autoencoders-in-keras.html</a></td></tr><tr class="property-row property-row-relation"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesRelation"><polygon points="4.5 1 4.5 3 9.586 3 1 11.586 2.414 13 11 4.414 11 9.5 13 9.5 13 1"></polygon></svg></span>Projects (DB)</th><td></td></tr><tr class="property-row property-row-relation"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesRelation"><polygon points="4.5 1 4.5 3 9.586 3 1 11.586 2.414 13 11 4.414 11 9.5 13 9.5 13 1"></polygon></svg></span>Knowledge Hub</th><td></td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M3.73333,3.86667 L7.46667,3.86667 C8.49613,3.86667 9.33333,4.70387 9.33333,5.73333 C9.33333,6.7628 8.49613,7.6 7.46667,7.6 L6.53333,7.6 C6.01813,7.6 5.6,8.0186 5.6,8.53333 C5.6,9.04807 6.01813,9.46667 6.53333,9.46667 L7.46667,9.46667 C9.5284,9.46667 11.2,7.79507 11.2,5.73333 C11.2,3.6716 9.5284,2 7.46667,2 L3.73333,2 C1.6716,2 0,3.6716 0,5.73333 C0,7.124 0.762067,8.33453 1.88953,8.97713 C1.87553,8.83107 1.86667,8.6836 1.86667,8.53333 C1.86667,7.92013 1.98753,7.33447 2.2036,6.7978 C1.99267,6.4954 1.86667,6.12953 1.86667,5.73333 C1.86667,4.70387 2.70387,3.86667 3.73333,3.86667 Z M12.1095,5.28907 C12.1231,5.4356 12.1333,5.58307 12.1333,5.73333 C12.1333,6.34607 12.0101,6.9294 11.7931,7.46513 C12.0059,7.768 12.1333,8.13573 12.1333,8.53333 C12.1333,9.5628 11.2961,10.4 10.2667,10.4 L6.53333,10.4 C5.50387,10.4 4.66667,9.5628 4.66667,8.53333 C4.66667,7.50387 5.50387,6.66667 6.53333,6.66667 L7.46667,6.66667 C7.98187,6.66667 8.4,6.24807 8.4,5.73333 C8.4,5.2186 7.98187,4.8 7.46667,4.8 L6.53333,4.8 C4.4716,4.8 2.8,6.4716 2.8,8.53333 C2.8,10.59507 4.4716,12.2667 6.53333,12.2667 L10.2667,12.2667 C12.3284,12.2667 14,10.59507 14,8.53333 C14,7.14267 13.2375,5.93167 12.1095,5.28907 Z"></path></svg></span>URL</th><td><a href="https://blog.keras.io/building-autoencoders-in-keras.html" class="url-value">https://blog.keras.io/building-autoencoders-in-keras.html</a></td></tr></tbody></table></header><div class="page-body"><p id="3763cc7f-d3a1-4908-b4af-32fabbc6e0b0" class="">This post was written in early 2016. It is therefore badly outdated.</p><p id="fcb885a7-0fb1-4315-a39d-4e859ebc7170" class="">In this tutorial, we will answer some common questions about autoencoders, and we will cover code examples of the following models:</p><ul id="b9b93bc6-a6de-485b-ae0e-e36c807312c4" class="bulleted-list"><li style="list-style-type:disc">a simple autoencoder based on a fully-connected layer</li></ul><ul id="a6c5c2ad-0f17-44fb-9dad-eae4e655c7b1" class="bulleted-list"><li style="list-style-type:disc">a sparse autoencoder</li></ul><ul id="2b4eb5a1-4f79-4948-bed5-b0400ce26882" class="bulleted-list"><li style="list-style-type:disc">a deep fully-connected autoencoder</li></ul><ul id="d73e640b-06e4-4721-a121-b03eb3e64b2e" class="bulleted-list"><li style="list-style-type:disc">a deep convolutional autoencoder</li></ul><ul id="f95f65f5-2301-4b34-b98d-9e1af5d29ab3" class="bulleted-list"><li style="list-style-type:disc">an image denoising model</li></ul><ul id="0fd84642-c3e5-4c0b-9caa-cad0e24f9a33" class="bulleted-list"><li style="list-style-type:disc">a sequence-to-sequence autoencoder</li></ul><ul id="7c290067-d145-4cd9-8e6e-18b44d2338ba" class="bulleted-list"><li style="list-style-type:disc">a variational autoencoder</li></ul><p id="9405e9b1-7fd8-4454-a5d6-ed5c098ba308" class=""><strong>Note: all code examples have been updated to the Keras 2.0 API on March 14, 2017. You will need Keras version 2.0.0 or higher to run them.</strong></p><h2 id="582d45ec-e3e3-4719-a7ba-2a031f64c200" class="">What are autoencoders?</h2><figure id="8dfaeed5-0070-48a6-abec-a1c61c290f87" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/autoencoder_schema.jpg"><img style="width:700px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/autoencoder_schema.jpg"/></a></figure><p id="bfa21b16-b797-4824-a080-0b5ab1348b1a" class="">&quot;Autoencoding&quot; is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) <em>learned automatically from examples</em> rather than engineered by a human. Additionally, in almost all contexts where the term &quot;autoencoder&quot; is used, the compression and decompression functions are implemented with neural networks.</p><p id="c5dab162-c557-4be2-b5ed-3b8440e5d516" class="">1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about &quot;sound&quot; in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.</p><p id="b5e60520-52ee-4981-8a10-5b3e3399c2b7" class="">2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.</p><p id="eab8a930-8132-4f32-9310-ba31ba813357" class="">3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn&#x27;t require any new engineering, just appropriate training data.</p><p id="bd466861-a313-44d6-8540-bedc9a46186d" class="">To build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a &quot;loss&quot; function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent. It&#x27;s simple! And you don&#x27;t even need to understand any of these words to start using autoencoders in practice.</p><h2 id="21723677-03f4-4582-81fd-bfd58cb11548" class="">Are they good at data compression?</h2><p id="12407b76-2db4-4ace-9750-46f84e4a2e5b" class="">Usually, not really. In picture compression for instance, it is pretty difficult to train an autoencoder that does a better job than a basic algorithm like JPEG, and typically the only way it can be achieved is by restricting yourself to a very specific type of picture (e.g. one for which JPEG does not do a good job). The fact that autoencoders are data-specific makes them generally impractical for real-world data compression problems: you can only use them on data that is similar to what they were trained on, and making them more general thus requires <em>lots</em> of training data. But future advances might change this, who knows.</p><h2 id="d2819c2b-0089-48cc-9aa6-2a347acb7b46" class="">What are autoencoders good for?</h2><p id="cee59c87-ba6c-4de6-a7c7-5d0148de863e" class="">They are rarely used in practical applications. In 2012 they briefly found an application in greedy layer-wise pretraining for deep convolutional neural networks [1], but this quickly fell out of fashion as we started realizing that better random weight initialization schemes were sufficient for training deep networks from scratch. In 2014, batch normalization [2] started allowing for even deeper networks, and from late 2015 we could train arbitrarily deep networks from scratch using residual learning [3].</p><p id="e3847e88-2349-496a-b6cc-ecacc0d6f231" class="">Today two interesting practical applications of autoencoders are <strong>data denoising</strong> (which we feature later in this post), and <strong>dimensionality reduction for data visualization</strong>. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.</p><p id="11f9a7db-e004-463d-b1ec-2dd6a2063afb" class="">For 2D visualization specifically, <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a> (pronounced &quot;tee-snee&quot;) is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32-dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and <a href="https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb">is available on Github</a>. Otherwise <a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">scikit-learn</a> also has a simple and practical implementation.</p><h2 id="191dbafb-4ab2-45ca-a04e-9856ba0b1a48" class="">So what&#x27;s the big deal with autoencoders?</h2><p id="defb26ae-e851-4c8c-9b24-5a9435274a50" class="">Their main claim to fame comes from being featured in many introductory machine learning classes available online. As a result, a lot of newcomers to the field absolutely love autoencoders and can&#x27;t get enough of them. This is the reason why this tutorial exists!</p><p id="4df497ef-dd44-4133-9f4e-08e8be223c8c" class="">Otherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a <em>self-supervised</em> technique, a specific instance of <em>supervised learning</em> where the targets are generated from the input data. In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that&#x27;s where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here. At this point there is significant evidence that focusing on the reconstruction of a picture at the pixel level, for instance, is not conductive to learning interesting, abstract features of the kind that label-supervized learning induces (where targets are fairly abstract concepts &quot;invented&quot; by humans such as &quot;dog&quot;, &quot;car&quot;...). In fact, one may argue that the best features in this regard are those that are the <em>worst</em> at exact input reconstruction while achieving high performance on the main task that you are interested in (classification, localization, etc).</p><p id="b517631a-e99d-429c-8c52-8eb7fe5562de" class="">In self-supervized learning applied to vision, a potentially fruitful alternative to autoencoder-style input reconstruction is the use of toy tasks such as jigsaw puzzle solving, or detail-context matching (being able to match high-resolution but small patches of pictures with low-resolution versions of the pictures they are extracted from). The following paper investigates jigsaw puzzle solving and makes for a very interesting read: Noroozi and Favaro (2016) <a href="http://arxiv.org/abs/1603.09246">Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</a>. Such tasks are providing the model with built-in assumptions about the input data which are missing in traditional autoencoders, such as <em>&quot;visual macro-structure matters more than pixel-level details&quot;</em>.</p><figure id="da123832-9e23-41eb-9d1a-bb5469c2c4d7" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/jigsaw-puzzle.png"><img style="width:700px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/jigsaw-puzzle.png"/></a></figure><h2 id="c72585ab-1d19-43ed-919e-cccfc4b148f8" class="">Let&#x27;s build the simplest possible autoencoder</h2><p id="8aa2e75c-b285-4e45-9fc6-cb61b943ceef" class="">We&#x27;ll start simple, with a single fully-connected neural layer as encoder and as decoder:</p><pre class="language-python" id="abec57af-2608-47b5-918f-d2616f94c9c0" class="code code-wrap"><code>import keras
from keras import layers

# This is the size of our encoded representations
encoding_dim = 32  # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats

# This is our input image
input_img = keras.Input(shape=(784,))
# &quot;encoded&quot; is the encoded representation of the input
encoded = layers.Dense(encoding_dim, activation=&#x27;relu&#x27;)(input_img)
# &quot;decoded&quot; is the lossy reconstruction of the input
decoded = layers.Dense(784, activation=&#x27;sigmoid&#x27;)(encoded)

# This model maps an input to its reconstruction
autoencoder = keras.Model(input_img, decoded)
</code></pre><p id="55bb865d-c24f-439c-bde1-48120bde1eaf" class="">Let&#x27;s also create a separate encoder model:</p><pre class="language-python" id="c4958c86-5496-49cb-b213-429292379983" class="code code-wrap"><code># This model maps an input to its encoded representation
encoder = keras.Model(input_img, encoded)
</code></pre><p id="ac01c870-62c5-4f63-a1ff-ab5cce289bac" class="">As well as the decoder model:</p><pre class="language-python" id="d07598b7-0763-4b24-814a-9313cf0bcc65" class="code code-wrap"><code># This is our encoded (32-dimensional) input
encoded_input = keras.Input(shape=(encoding_dim,))
# Retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# Create the decoder model
decoder = keras.Model(encoded_input, decoder_layer(encoded_input))
</code></pre><p id="09bd474c-047d-4e37-a991-bc0943d16fb9" class="">Now let&#x27;s train our autoencoder to reconstruct MNIST digits.</p><p id="eadec876-633b-49e4-af9d-8844c4686b83" class="">First, we&#x27;ll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer:</p><pre class="language-python" id="693fea9f-29c3-4c55-b496-a0920d891652" class="code code-wrap"><code>autoencoder.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;)
</code></pre><p id="ce5ce76a-8f72-40b5-8498-f31aec800d01" class="">Let&#x27;s prepare our input data. We&#x27;re using MNIST digits, and we&#x27;re discarding the labels (since we&#x27;re only interested in encoding/decoding the input images).</p><pre class="language-python" id="2c508869-b9e7-4229-8409-2565a437a52d" class="code code-wrap"><code>from keras.datasets import mnist
import numpy as np
(x_train, _), (x_test, _) = mnist.load_data()
</code></pre><p id="8e5d3fba-33ae-4510-a2d3-98941e617aeb" class="">We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.</p><pre class="language-python" id="e07e369a-1cc7-4550-ae29-5314c850b007" class="code code-wrap"><code>x_train = x_train.astype(&#x27;float32&#x27;) / 255.
x_test = x_test.astype(&#x27;float32&#x27;) / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print(x_train.shape)
print(x_test.shape)
</code></pre><p id="368f7714-b535-4d7d-9353-be9d393b6f3c" class="">Now let&#x27;s train our autoencoder for 50 epochs:</p><pre class="language-python" id="4cdc3fb6-c1a3-4422-a390-b75629c6c2c9" class="code code-wrap"><code>autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))
</code></pre><p id="295e4b42-ea38-4463-b1be-7247445c10f0" class="">After 50 epochs, the autoencoder seems to reach a stable train/validation loss value of about <code>0.09</code>. We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib.</p><pre class="language-python" id="2b973f5a-5975-41af-bb73-f742b6085259" class="code code-wrap"><code># Encode and decode some digits
# Note that we take them from the *test* set
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)
</code></pre><pre class="language-python" id="d07e366d-5e72-4176-8f51-2676cbb81168" class="code code-wrap"><code># Use Matplotlib (don&#x27;t ask)
import matplotlib.pyplot as plt

n = 10  # How many digits we will display
plt.figure(figsize=(20, 4))
for i in range(n):
    # Display original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # Display reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
</code></pre><p id="a0b86bfd-c871-48f1-b16e-00f42cb5c2cf" class="">Here&#x27;s what we get. The top row is the original digits, and the bottom row is the reconstructed digits. We are losing quite a bit of detail with this basic approach.</p><figure id="d0c2052a-aa15-4cf2-8848-f687dc92ad73" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/basic_ae_32.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/basic_ae_32.png"/></a></figure><h2 id="fe2b86b3-3c3f-496f-a3e1-d989c0d56548" class="">Adding a sparsity constraint on the encoded representations</h2><p id="49bd38fd-d0d2-4d7b-a8e4-b918f9490d86" class="">In the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA (principal component analysis)</a>. But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would &quot;fire&quot; at a given time. In Keras, this can be done by adding an <code>activity_regularizer</code> to our <code>Dense</code> layer:</p><pre class="language-python" id="058eded2-3492-43a5-94ae-9f8ac1d8738e" class="code code-wrap"><code>from keras import regularizers

encoding_dim = 32

input_img = keras.Input(shape=(784,))
# Add a Dense layer with a L1 activity regularizer
encoded = layers.Dense(encoding_dim, activation=&#x27;relu&#x27;,
                activity_regularizer=regularizers.l1(10e-5))(input_img)
decoded = layers.Dense(784, activation=&#x27;sigmoid&#x27;)(encoded)

autoencoder = keras.Model(input_img, decoded)
</code></pre><p id="c9f79bc5-d79a-4a12-95bf-7c8c33fe53e8" class="">Let&#x27;s train this model for 100 epochs (with the added regularization the model is less likely to overfit and can be trained longer). The models ends with a train loss of <code>0.11</code> and test loss of <code>0.10</code>. The difference between the two is mostly due to the regularization term being added to the loss during training (worth about 0.01).</p><p id="728336c4-e7b8-45ed-bb34-516cdfd60015" class="">Here&#x27;s a visualization of our new results:</p><figure id="3c913e14-2fd0-4d32-b867-dc08fb196cf8" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/sparse_ae_32.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/sparse_ae_32.png"/></a></figure><p id="fc5f8163-496b-4792-9bc6-d5ea412656c3" class="">They look pretty similar to the previous model, the only significant difference being the sparsity of the encoded representations. <code>encoded_imgs.mean()</code> yields a value <code>3.33</code> (over our 10,000 test images), whereas with the previous model the same quantity was <code>7.30</code>. So our new model yields encoded representations that are twice sparser.</p><h2 id="1a4b7fcd-5fb1-4e32-b4ff-2fea428d5afe" class="">Deep autoencoder</h2><p id="118a689e-9526-4ddc-9f1a-03dbd86d8350" class="">We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:</p><pre class="language-python" id="efb1db0f-db54-41d5-853e-158c1734c4c2" class="code code-wrap"><code>input_img = keras.Input(shape=(784,))
encoded = layers.Dense(128, activation=&#x27;relu&#x27;)(input_img)
encoded = layers.Dense(64, activation=&#x27;relu&#x27;)(encoded)
encoded = layers.Dense(32, activation=&#x27;relu&#x27;)(encoded)

decoded = layers.Dense(64, activation=&#x27;relu&#x27;)(encoded)
decoded = layers.Dense(128, activation=&#x27;relu&#x27;)(decoded)
decoded = layers.Dense(784, activation=&#x27;sigmoid&#x27;)(decoded)
</code></pre><p id="e981f9ae-43b9-4894-be45-6af8da60364a" class="">Let&#x27;s try this:</p><pre class="language-python" id="2180290d-753b-4f4a-8186-471ee7995f8e" class="code code-wrap"><code>autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;)

autoencoder.fit(x_train, x_train,
                epochs=100,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))
</code></pre><p id="c724b226-c86e-4c8d-93b0-90fe7598ff85" class="">After 100 epochs, it reaches a train and validation loss of ~0.08, a bit better than our previous models. Our reconstructed digits look a bit better too:</p><figure id="a798872e-b82a-44f0-9f47-689b7fa32217" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/deep_ae_32.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/deep_ae_32.png"/></a></figure><h2 id="5e515c13-1221-430e-b66e-b3c53932e4c7" class="">Convolutional autoencoder</h2><p id="0d005cef-2146-43a0-be8a-1a27610cb3e0" class="">Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.</p><p id="16d1b565-443d-466a-af2b-8c527ca50187" class="">Let&#x27;s implement one. The encoder will consist in a stack of <code>Conv2D</code> and <code>MaxPooling2D</code> layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of <code>Conv2D</code> and <code>UpSampling2D</code> layers.</p><pre class="language-python" id="f5504c30-29b2-4b01-b6b4-f3ee2c2e7327" class="code code-wrap"><code>import keras
from keras import layers

input_img = keras.Input(shape=(28, 28, 1))

x = layers.Conv2D(16, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(input_img)
x = layers.MaxPooling2D((2, 2), padding=&#x27;same&#x27;)(x)
x = layers.Conv2D(8, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
x = layers.MaxPooling2D((2, 2), padding=&#x27;same&#x27;)(x)
x = layers.Conv2D(8, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
encoded = layers.MaxPooling2D((2, 2), padding=&#x27;same&#x27;)(x)

# at this point the representation is (4, 4, 8) i.e. 128-dimensional

x = layers.Conv2D(8, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(encoded)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(8, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(16, (3, 3), activation=&#x27;relu&#x27;)(x)
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, (3, 3), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(x)

autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;)
</code></pre><p id="844fedb1-2659-4f4a-a076-98bab6bdc8f0" class="">To train it, we will use the original MNIST digits with shape <code>(samples, 3, 28, 28)</code>, and we will just normalize pixel values between 0 and 1.</p><pre class="language-python" id="d604bf13-2d2b-4b5f-973e-5fc25d529f5c" class="code code-wrap"><code>from keras.datasets import mnist
import numpy as np

(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype(&#x27;float32&#x27;) / 255.
x_test = x_test.astype(&#x27;float32&#x27;) / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))
</code></pre><p id="b9c65be3-fe7d-4b59-ad93-17a81286d4d1" class="">Let&#x27;s train this model for 50 epochs. For the sake of demonstrating how to visualize the results of a model during training, we will be using the TensorFlow backend and the TensorBoard callback.</p><p id="615528e4-892e-40dd-be0c-baf1edc00430" class="">First, let&#x27;s open up a terminal and start a TensorBoard server that will read logs stored at <code>/tmp/autoencoder</code>.</p><pre class="language-python" id="6d895909-088d-499c-9417-7219620a2723" class="code code-wrap"><code>tensorboard --logdir=/tmp/autoencoder
</code></pre><p id="1f6b0032-3597-4ce7-b342-0f4f0b01a69c" class="">Then let&#x27;s train our model. In the <code>callbacks</code> list we pass an instance of the <code>TensorBoard</code> callback. After every epoch, this callback will write logs to <code>/tmp/autoencoder</code>, which can be read by our TensorBoard server.</p><pre class="language-python" id="e6067faa-66d6-4e56-ba67-3fc93f527f88" class="code code-wrap"><code>from keras.callbacks import TensorBoard

autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=128,
                shuffle=True,
                validation_data=(x_test, x_test),
                callbacks=[TensorBoard(log_dir=&#x27;/tmp/autoencoder&#x27;)])
</code></pre><p id="d05dfc62-fbca-4b0f-be49-bd0d09d5d067" class="">This allows us to monitor training in the TensorBoard web interface (by navighating to <code>http://0.0.0.0:6006</code>):</p><figure id="03553d12-cfc9-4510-bd7d-ec728f5ec50e" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/tb_curves.png"><img style="width:700px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/tb_curves.png"/></a></figure><p id="c429de05-0dec-453e-aee2-1429b71c2f37" class="">The model converges to a loss of 0.094, significantly better than our previous models (this is in large part due to the higher entropic capacity of the encoded representation, 128 dimensions vs. 32 previously). Let&#x27;s take a look at the reconstructed digits:</p><pre class="language-python" id="5971c543-dc34-4a75-ac32-bee7288369fc" class="code code-wrap"><code>decoded_imgs = autoencoder.predict(x_test)

n = 10
plt.figure(figsize=(20, 4))
for i in range(1, n + 1):
    # Display original
    ax = plt.subplot(2, n, i)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # Display reconstruction
    ax = plt.subplot(2, n, i + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
</code></pre><figure id="e002c7ef-cdbc-46c0-ae76-542f4d01fa7a" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/deep_conv_ae_128.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/deep_conv_ae_128.png"/></a></figure><p id="786297b7-1e5d-4a0f-865b-5d731da8d4c9" class="">We can also have a look at the 128-dimensional encoded representations. These representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images.</p><pre class="language-python" id="f91ac285-b266-4507-a4ab-3bc122456e82" class="code code-wrap"><code>encoder = keras.Model(input_img, encoded)
encoded_imgs = encoder.predict(x_test)

n = 10
plt.figure(figsize=(20, 8))
for i in range(1, n + 1):
    ax = plt.subplot(1, n, i)
    plt.imshow(encoded_imgs[i].reshape((4, 4 * 8)).T)
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
</code></pre><figure id="cdeb9d26-69e1-442b-911c-ecee5ffed43a" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/encoded_representations.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/encoded_representations.png"/></a></figure><h2 id="4fbdcbd2-cb1d-4706-890a-0f9061e7c12c" class="">Application to image denoising</h2><p id="7fe4cd30-6aa7-40a0-bc55-e6a5c169193e" class="">Let&#x27;s put our convolutional autoencoder to work on an image denoising problem. It&#x27;s simple: we will train the autoencoder to map noisy digits images to clean digits images.</p><p id="ca5d4e96-b05a-4925-a6e5-50b561bff285" class="">Here&#x27;s how we will generate synthetic noisy digits: we just apply a gaussian noise matrix and clip the images between 0 and 1.</p><pre class="language-python" id="2f8f8edb-b033-4b15-bcb6-f97bb7817523" class="code code-wrap"><code>from keras.datasets import mnist
import numpy as np

(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype(&#x27;float32&#x27;) / 255.
x_test = x_test.astype(&#x27;float32&#x27;) / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)
</code></pre><p id="f5269dd8-4a10-4e7f-bc8b-f973ba845b0a" class="">Here&#x27;s what the noisy digits look like:</p><pre class="language-python" id="dbe7ce47-d4bf-4072-bddb-2b30b544e5b7" class="code code-wrap"><code>n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n + 1):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test_noisy[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
</code></pre><p id="48aa731d-9f8e-4031-ba0e-24df4b0d12ed" class="">If you squint you can still recognize them, but barely. Can our autoencoder learn to recover the original digits? Let&#x27;s find out.</p><p id="38530fb6-36b7-4035-a019-a01ed0f2ad74" class="">Compared to the previous convolutional autoencoder, in order to improve the quality of the reconstructed, we&#x27;ll use a slightly different model with more filters per layer:</p><pre class="language-python" id="134fd262-959d-47a3-b21c-0eaf14985cb1" class="code code-wrap"><code>input_img = keras.Input(shape=(28, 28, 1))

x = layers.Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(input_img)
x = layers.MaxPooling2D((2, 2), padding=&#x27;same&#x27;)(x)
x = layers.Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
encoded = layers.MaxPooling2D((2, 2), padding=&#x27;same&#x27;)(x)

# At this point the representation is (7, 7, 32)

x = layers.Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(encoded)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(32, (3, 3), activation=&#x27;relu&#x27;, padding=&#x27;same&#x27;)(x)
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, (3, 3), activation=&#x27;sigmoid&#x27;, padding=&#x27;same&#x27;)(x)

autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;binary_crossentropy&#x27;)
</code></pre><p id="684827cb-510e-4f15-a5c0-7e8402a1942d" class="">Let&#x27;s train it for 100 epochs:</p><pre class="language-python" id="0755ea91-d964-4cac-8b76-1ec5bc0a742a" class="code code-wrap"><code>autoencoder.fit(x_train_noisy, x_train,
                epochs=100,
                batch_size=128,
                shuffle=True,
                validation_data=(x_test_noisy, x_test),
                callbacks=[TensorBoard(log_dir=&#x27;/tmp/tb&#x27;, histogram_freq=0, write_graph=False)])
</code></pre><p id="bf331062-0fb4-4d71-82aa-7ea793486427" class="">Now let&#x27;s take a look at the results. Top, the noisy digits fed to the network, and bottom, the digits are reconstructed by the network.</p><figure id="fc6d3a90-1fa7-42d9-be16-5ec4f889dd12" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/denoised_digits.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/denoised_digits.png"/></a></figure><p id="61ee1ced-6ae4-4128-b212-d68cca51ed19" class="">It seems to work pretty well. If you scale this process to a bigger convnet, you can start building document denoising or audio denoising models. <a href="https://www.kaggle.com/c/denoising-dirty-documents">Kaggle has an interesting dataset to get you started</a>.</p><h2 id="b4d468b2-dd8a-4134-bb78-07b70510543b" class="">Sequence-to-sequence autoencoder</h2><p id="147ddc84-ded1-4185-9440-6e43608e66a1" class="">If you inputs are sequences, rather than vectors or 2D images, then you may want to use as encoder and decoder a type of model that can capture temporal structure, such as a LSTM. To build a LSTM-based autoencoder, first use a LSTM encoder to turn your input sequences into a single vector that contains information about the entire sequence, then repeat this vector <code>n</code> times (where <code>n</code> is the number of timesteps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.</p><p id="87b8bbd1-2143-4840-b931-71151541380d" class="">We won&#x27;t be demonstrating that one on any specific dataset. We will just put a code example here for future reference for the reader!</p><pre class="language-python" id="5e522daa-5937-466b-a103-b68d8c9e964a" class="code code-wrap"><code>timesteps = ...  # Length of your sequences
input_dim = ...
latent_dim = ...

inputs = keras.Input(shape=(timesteps, input_dim))
encoded = layers.LSTM(latent_dim)(inputs)

decoded = layers.RepeatVector(timesteps)(encoded)
decoded = layers.LSTM(input_dim, return_sequences=True)(decoded)

sequence_autoencoder = keras.Model(inputs, decoded)
encoder = keras.Model(inputs, encoded)
</code></pre><h2 id="5fbbe393-dd0c-4fe3-839f-52e6838d0d62" class="">Variational autoencoder (VAE)</h2><p id="06dbebcc-f9aa-4b7f-95bc-43910b310658" class="">Variational autoencoders are a slightly more modern and interesting take on autoencoding.</p><p id="8f78d81d-79bd-46dc-96a9-008a406f8096" class="">What is a variational autoencoder, you ask? It&#x27;s a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a <a href="https://en.wikipedia.org/wiki/Latent_variable_model">latent variable model</a> for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a &quot;generative model&quot;.</p><p id="3dc3c9ad-99ca-4f32-a380-cef6d2a7fd2a" class="">How does a variational autoencoder work?</p><p id="7bab8b6c-a35f-445c-b12b-25b15a95219f" class="">First, an encoder network turns the input samples <code>x</code> into two parameters in a latent space, which we will note <code>z_mean</code> and <code>z_log_sigma</code>. Then, we randomly sample similar points <code>z</code> from the latent normal distribution that is assumed to generate the data, via <code>z = z_mean + exp(z_log_sigma) * epsilon</code>, where <code>epsilon</code> is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.</p><p id="ee5d1bea-d8cd-43f9-9095-77dc84b847a3" class="">The parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.</p><p id="23470a81-276a-4855-88df-d7feb21d11f4" class="">Because a VAE is a more complex example, we have made the code available on Github as <a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py">a standalone script</a>. Here we will review step by step how the model is created.</p><p id="26dab439-b745-441e-9151-c68655cc797a" class="">First, here&#x27;s our encoder network, mapping inputs to our latent distribution parameters:</p><pre class="language-python" id="c4226245-1110-4bfe-81e8-d053c368ba9e" class="code code-wrap"><code>original_dim = 28 * 28
intermediate_dim = 64
latent_dim = 2

inputs = keras.Input(shape=(original_dim,))
h = layers.Dense(intermediate_dim, activation=&#x27;relu&#x27;)(inputs)
z_mean = layers.Dense(latent_dim)(h)
z_log_sigma = layers.Dense(latent_dim)(h)
</code></pre><p id="9960ac2c-4490-4f17-adc7-d552f3029c56" class="">We can use these parameters to sample new similar points from the latent space:</p><pre class="language-python" id="39c115ee-3fc1-4dc7-a50c-da1e4f3329e9" class="code code-wrap"><code>from keras import backend as K

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                              mean=0., stddev=0.1)
    return z_mean + K.exp(z_log_sigma) * epsilon

z = layers.Lambda(sampling)([z_mean, z_log_sigma])
</code></pre><p id="5a2d4457-f3ce-4f48-9352-ca4c6a3026b7" class="">Finally, we can map these sampled latent points back to reconstructed inputs:</p><pre class="language-python" id="6a6673a4-2e41-416a-94af-7ed9ff24c97e" class="code code-wrap"><code># Create encoder
encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name=&#x27;encoder&#x27;)

# Create decoder
latent_inputs = keras.Input(shape=(latent_dim,), name=&#x27;z_sampling&#x27;)
x = layers.Dense(intermediate_dim, activation=&#x27;relu&#x27;)(latent_inputs)
outputs = layers.Dense(original_dim, activation=&#x27;sigmoid&#x27;)(x)
decoder = keras.Model(latent_inputs, outputs, name=&#x27;decoder&#x27;)

# instantiate VAE model
outputs = decoder(encoder(inputs)[2])
vae = keras.Model(inputs, outputs, name=&#x27;vae_mlp&#x27;)
</code></pre><p id="1cb37968-ff73-4576-9f2c-86480f080a4e" class="">What we&#x27;ve done so far allows us to instantiate 3 models:</p><ul id="60146bd0-ab66-4186-a123-37567313c152" class="bulleted-list"><li style="list-style-type:disc">an end-to-end autoencoder mapping inputs to reconstructions</li></ul><ul id="24971815-2e93-48b8-898f-d092b1f6c815" class="bulleted-list"><li style="list-style-type:disc">an encoder mapping inputs to the latent space</li></ul><ul id="02ecb059-3291-48c3-86ea-b6d68fb2b3cf" class="bulleted-list"><li style="list-style-type:disc">a generator that can take points on the latent space and will output the corresponding reconstructed samples.</li></ul><p id="fbe2dde8-e865-4c77-9b69-68cff7e2a0cc" class="">We train the model using the end-to-end model, with a custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.</p><pre class="language-python" id="76651b9b-5f6e-49a2-8c1c-8a8a09aff1f4" class="code code-wrap"><code>reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)
reconstruction_loss *= original_dim
kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -0.5
vae_loss = K.mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)
vae.compile(optimizer=&#x27;adam&#x27;)
</code></pre><p id="19d67f8c-65fb-4fc1-81ea-1af2e13c180c" class="">We train our VAE on MNIST digits:</p><pre class="language-python" id="9773f1c0-58b9-4864-ad70-3408a3761143" class="code code-wrap"><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype(&#x27;float32&#x27;) / 255.
x_test = x_test.astype(&#x27;float32&#x27;) / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

vae.fit(x_train, x_train,
        epochs=100,
        batch_size=32,
        validation_data=(x_test, x_test))
</code></pre><p id="94fed44a-5171-449d-bc06-57cc394cc626" class="">Because our latent space is two-dimensional, there are a few cool visualizations that can be done at this point. One is to look at the neighborhoods of different classes on the latent 2D plane:</p><pre class="language-python" id="bdf664e6-9b3d-4e24-9b57-8f4e25161477" class="code code-wrap"><code>x_test_encoded = encoder.predict(x_test, batch_size=batch_size)
plt.figure(figsize=(6, 6))
plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)
plt.colorbar()
plt.show()
</code></pre><figure id="7a9a378d-375c-4d4a-8f94-edb2f81e897e" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/vae_classes_plane.png"><img style="width:700px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/vae_classes_plane.png"/></a></figure><p id="cb681516-f8b7-4239-8e5c-9b7151ac30ca" class="">Each of these colored clusters is a type of digit. Close clusters are digits that are structurally similar (i.e. digits that share information in the latent space).</p><p id="3f96046a-c79a-46c3-9972-86c5c5c7a9b2" class="">Because the VAE is a generative model, we can also use it to generate new digits! Here we will scan the latent plane, sampling latent points at regular intervals, and generating the corresponding digit for each of these points. This gives us a visualization of the latent manifold that &quot;generates&quot; the MNIST digits.</p><pre class="language-python" id="15c68678-3106-4f0a-9699-9ed06b228fd6" class="code code-wrap"><code># Display a 2D manifold of the digits
n = 15  # figure with 15x15 digits
digit_size = 28
figure = np.zeros((digit_size * n, digit_size * n))
# We will sample n points within [-15, 15] standard deviations
grid_x = np.linspace(-15, 15, n)
grid_y = np.linspace(-15, 15, n)

for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]])
        x_decoded = decoder.predict(z_sample)
        digit = x_decoded[0].reshape(digit_size, digit_size)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.show()
</code></pre><figure id="160d2111-4621-42d0-9e91-cacef8dd9119" class="image"><a href="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/vae_digits_manifold.png"><img style="width:600px" src="Building%20Autoencoders%20in%20Keras%20a3f83cf81de246c69724767cf1bcce38/vae_digits_manifold.png"/></a></figure><p id="09a5c4e0-fe62-4bdc-9d2f-b38f1f517d6b" class="">That&#x27;s it! If you have suggestions for more topics to be covered in this post (or in future posts), you can contact me on Twitter at <a href="https://twitter.com/fchollet">@fchollet</a>.</p><h3 id="a0301e9a-998c-4695-a26a-99f1f8ff6b5c" class="">References</h3><p id="11720332-6f43-4bc8-9876-cee5b2cea5c9" class="">[1] <a href="http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf">Why does unsupervised pre-training help deep learning?</a></p><p id="68b739bd-2903-4288-a65c-1ff6b8029af6" class="">[2] <a href="http://arxiv.org/abs/1502.03167">Batch normalization: Accelerating deep network training by reducing internal covariate shift.</a></p><p id="88b166da-8424-4165-8d14-5c062882f48c" class="">[3] <a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p><p id="ca517efc-c35d-4452-b6d6-2d22b7201b2d" class="">[4] <a href="http://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></p></div></article>
<script src="../../../prism.js"></script>
</body></html>