<!doctype HTML><html><head><meta charset="utf-8"><title>Made with Remarkable!</title><link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css"><style type='text/css'>body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}</style></head><body><h1>Data Preprocessing</h1>
<details><summary><b>One-Hot Encoding (to_categorical)</b></summary><p>

<pre><code>from keras.utils.np_utils import to_categorical

## y_train (before).unique() = [0, 1, ... , 9]
y_train     = to_categorical(y_train)
num_classes = y_train.shape[1]
num_classes

#### Result ######
10
</code></pre>

</p></details>

<details><summary><b>Data Augmentation (Images)</b></summary><p>
[**Docs**](file:///media/mosaab/Volume/Personal/Development/Courses%20Docs/Sklearn/Image%20Preprocessing%20-%20Keras%20Documentation.html)

#### 1. Import the class ImageDataGenerator.
**NOTE:** you can specify its parameters. - see the docs -.

<pre><code>from keras.preprocessing import image

gen = image.ImageDataGenerator()
</code></pre>


#### 2. Test split before generating.

<pre><code>from sklearn.model_selection import train_test_split

X = X_train
y = y_train

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=42)
train_batches = gen.flow(X_train, y_train, batch_size=64)
val_batches   = gen.flow(X_val, y_val, batch_size=64)
</code></pre>


#### 3. After you defined your model using `Sequential`.

<pre><code>history=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=3, 
                    validation_data=val_batches, validation_steps=val_batches.n)
</code></pre>

</p></details>

<details><summary><b>Loading Large Dataset</b></summary><p>

<h4>1. Load the data</h4>

<pre><code>from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from utils import *
import os, numpy as np, pandas as pd

housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,
                                                              housing.target[..., None],
                                                              random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,
                                                      y_train_full,
                                                      random_state=42)
shape(X_train, X_valid, X_test, y_train, y_valid, y_test)
scaler = StandardScaler()
scaler.fit(X_train)
X_mean = scaler.mean_
X_std  = scaler.scale_
</code></pre>


<h4>2. Split the data into multiple csv files</h4>

<pre><code>import os, numpy as np, pandas as pd
def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):
    housing_dir = os.path.join(&quot;datasets&quot;, &quot;housing&quot;)  # Write where to save the new csv files.
    os.makedirs(housing_dir, exist_ok=True)
    path_format = os.path.join(housing_dir, &quot;my_{}_{:02d}.csv&quot;)

    filepaths = []
    m = len(data)
    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):
        part_csv = path_format.format(name_prefix, file_idx)
        filepaths.append(part_csv)

        with open(part_csv, &quot;wt&quot;, encoding=&quot;utf-8&quot;) as f:
            if header is not None:
                f.write(header)
                f.write(&quot;\n&quot;)
            for row_idx in row_indices:
                f.write(&quot;,&quot;.join([repr(col) for col in data[row_idx]]))
                f.write(&quot;\n&quot;)
    return filepaths
</code></pre>



<pre><code>train_data  = np.c_[X_train, y_train]
valid_data  = np.c_[X_valid, y_valid]
test_data   = np.c_[X_test, y_test]
header_cols = housing.feature_names + [&quot;MedianHouseValue&quot;]
header      = &quot;,&quot;.join(header_cols)

train_filepaths = save_to_multiple_csv_files(train_data, &quot;train&quot;, header, n_parts=20)
valid_filepaths = save_to_multiple_csv_files(valid_data, &quot;valid&quot;, header, n_parts=10)
test_filepaths  = save_to_multiple_csv_files(test_data, &quot;test&quot;, header, n_parts=10)
</code></pre>


<h4>3. Handle how to read the multiple files</h4>

<pre><code>n_inputs = 8

@tf.function
def preprocess(line):
    defs   = [0.]*n_inputs + [tf.constant([], dtype=tf.float32)]
    fields = tf.io.decode_csv(line, record_defaults=defs)
    X      = tf.stack(fields[:-1])
    y      = tf.stack(fields[-1:])
    return (X - X_mean) / X_std, y
</code></pre>



<pre><code>def csv_reader_dataset(filepaths, repeat=1, n_readers=5,
                       n_read_threads=tf.data.experimental.AUTOTUNE,
                       shuffle_buffer_size=10_000,
                       n_parse_threads=5, batch_size=32):
    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)
    dataset = dataset.interleave(
        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
        cycle_length=n_readers,
        num_parallel_calls=n_read_threads
    )
    dataset = dataset.shuffle(shuffle_buffer_size)
    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
    dataset = dataset.batch(batch_size)
    return dataset.prefetch(1)
</code></pre>


<pre><code>train_set = csv_reader_dataset(train_filepaths, repeat=None)
valid_set = csv_reader_dataset(valid_filepaths)
test_set  = csv_reader_dataset(test_filepaths)
</code></pre>


<h4>4. Modelling</h4>

<pre><code>tf.keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(30, activation=&quot;relu&quot;, input_shape=X_train.shape[1:]),
    tf.keras.layers.Dense(30, activation=&quot;relu&quot;),
    tf.keras.layers.Dense(1)
])
</code></pre>


<pre><code>model.compile(loss=&quot;mse&quot;,
              optimizer=tf.keras.optimizers.Adam(lr=1e-3))
batch_size = 16
model.fit(train_set,
          steps_per_epoch=len(X_train) // batch_size,
          epochs=10,
          validation_data=valid_set)
</code></pre>


<h4>5. Predict & Evaluate</h4>

<pre><code>model.evaluate(test_set, steps=len(X_test) // batch_size)
</code></pre>


<pre><code>new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels
X_new = X_test
model.predict(new_set, steps=len(X_new) // batch_size)
</code></pre>

</p></details><script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js"></script><script>hljs.initHighlightingOnLoad();</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript">MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});</script></body></html>