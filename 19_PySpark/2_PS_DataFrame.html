<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        PySpark DataFrame
    </title>
    <link rel="stylesheet" href="../prism.css">
</head>

<body>
    <h1 id="3-data-wrangling">
        PySpark DataFrame
    </h1>
    <div style="width:1000px;margin:auto">
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Load Data into DataFrame</b></summary>
            <ul>

                <li>
                    <details>
                        <summary><b>createDataFrame()</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
df.show()
+---------+----------+--------+----------+------+------+
# |firstname|middlename|lastname|dob       |gender|salary|
# +---------+----------+--------+----------+------+------+
# |James    |          |Smith   |1991-04-01|M     |3000  |
# |Michael  |Rose      |        |2000-05-19|M     |4000  |
# |Robert   |          |Williams|1978-09-05|M     |4000  |
# |Maria    |Anne      |Jones   |1967-12-01|F     |4000  |
# |Jen      |Mary      |Brown   |1980-02-17|F     |-1    |
# +---------+----------+--------+----------+------+------+
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>createDataFrame() with defined schema</b></summary>
                        <pre class="language-python"><code>from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
    ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
    ])
    
df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>From RDD</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
rdd = spark.sparkContext.parallelize(data)
                
columns = ["language","users_count"]
dfFromRDD1 = rdd.toDF(columns)
dfFromRDD1.printSchema()
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>from CSV file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
df = spark.read.csv("/tmp/resources/zipcodes.csv")
df.printSchema()
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>from TXT file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

df = spark.read.text("/src/resources/file.txt")
df.printSchema()
</code></pre>
                    </details>
                </li>


                <li>
                    <details>
                        <summary><b>from JSON file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

df = spark.read.json("/src/resources/file.json")
df.printSchema()
</code></pre>
                    </details>
                </li>
            </ul>
        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>DF Operations</b></summary>
            <ul>
                <li>
                    <details>
                        <summary><b>Rename columns</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql.functions import *

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

dataDF = [(('James','','Smith'),'1991-04-01','M',3000),
    (('Michael','Rose',''),'2000-05-19','M',4000),
    (('Robert','','Williams'),'1978-09-05','M',4000),
    (('Maria','Anne','Jones'),'1967-12-01','F',4000),
    (('Jen','Mary','Brown'),'1980-02-17','F',-1)
]

schema = StructType([
        StructField('name', StructType([
                StructField('firstname', StringType(), True),
                StructField('middlename', StringType(), True),
                StructField('lastname', StringType(), True)
                ])),
            StructField('dob', StringType(), True),
            StructField('gender', StringType(), True),
            StructField('salary', IntegerType(), True)
            ])

df = spark.createDataFrame(data = dataDF, schema = schema)
df.printSchema()

# Example 1
df.withColumnRenamed("dob","DateOfBirth").printSchema()
# Example 2   
df2 = df.withColumnRenamed("dob","DateOfBirth") \
    .withColumnRenamed("salary","salary_amount")
df2.printSchema()

# Example 3 
schema2 = StructType([
    StructField("fname",StringType()),
    StructField("middlename",StringType()),
    StructField("lname",StringType())])
    
df.select(col("name").cast(schema2),
    col("dob"),
    col("gender"),
    col("salary")) \
    .printSchema()    

# Example 4 
df.select(col("name.firstname").alias("fname"),
    col("name.middlename").alias("mname"),
    col("name.lastname").alias("lname"),
    col("dob"),col("gender"),col("salary")) \
    .printSchema()
    
# Example 5
df4 = df.withColumn("fname",col("name.firstname")) \
        .withColumn("mname",col("name.middlename")) \
        .withColumn("lname",col("name.lastname")) \
        .drop("name")
df4.printSchema()

#Example 7
newColumns = ["newCol1","newCol2","newCol3","newCol4"]
df.toDF(*newColumns).printSchema()
                        </code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Change DataType using PySpark withColumn()</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("salary", col("salary").cast("Integer")).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Update The Value of an Existing Column</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("salary", col("salary")*100).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Create a Column from an Existing Column</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("CopiedSalary", col("salary")*-1).show()
# +---------+----------+--------+----------+------+------+------------+
# |firstname|middlename|lastname|       dob|gender|salary|CopiedSalary|
# +---------+----------+--------+----------+------+------+------------+
# |    James|          |   Smith|1991-04-01|     M|  3000|       -3000|
# |  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|
# |   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|
# |    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|
# |      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|
# +---------+----------+--------+----------+------+------+------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Add a New Column using withColumn()</b></summary>
                        <pre class="language-python"><code>#  PySpark lit() function is used to add a constant value to a DataFrame column.
import pyspark
from pyspark.sql.functions import col, lit
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("Country", lit("USA")) \
  .withColumn("anotherCountry", lit("Egypt")) \
  .show()
# +---------+----------+--------+----------+------+------+-------+--------------+
# |firstname|middlename|lastname|       dob|gender|salary|Country|anotherCountry|
# +---------+----------+--------+----------+------+------+-------+--------------+
# |    James|          |   Smith|1991-04-01|     M|  3000|    USA|         Egypt|
# |  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|         Egypt|
# |   Robert|          |Williams|1978-09-05|     M|  4000|    USA|         Egypt|
# |    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|         Egypt|
# |      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|         Egypt|
# +---------+----------+--------+----------+------+------+-------+--------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Drop Column From PySpark DataFrame</b></summary>
                        <pre class="language-python"><code>#  PySpark lit() function is used to add a constant value to a DataFrame column.
import pyspark
from pyspark.sql.functions import col, lit
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.drop("salary").show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>filter() with Column Condition</b></summary>
                        <pre class="language-python"><code># Using equals condition
df.filter(df.state == "OH").show(truncate=False)

+----------------------+------------------+-----+------+
# |name                  |languages         |state|gender|
# +----------------------+------------------+-----+------+
# |[James, , Smith]      |[Java, Scala, C++]|OH   |M     |
# |[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |
# |[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |
# +----------------------+------------------+-----+------+

# not equals condition
df.filter(df.state != "OH") \
    .show(truncate=False) 
df.filter(~(df.state == "OH")) \
    .show(truncate=False)

#Using SQL col() function
from pyspark.sql.functions import col
df.filter(col("state") == "OH") \
    .show(truncate=False) 
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>filter() with SQL Expression</b></summary>
                        <pre class="language-python"><code>#Using SQL Expression
df.filter("gender == 'M'").show()
#For not equal
df.filter("gender != 'M'").show()
df.filter("gender <> 'M'").show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter Based on List Values</b></summary>
                        <pre class="language-python"><code>#Filter IS IN List values
li=["OH","CA","DE"]
df.filter(df.state.isin(li)).show()
+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|
| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|
|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|
+--------------------+------------------+-----+------+

# Filter NOT IS IN List values
#These show all records with NY (NY is not part of the list)
df.filter(~df.state.isin(li)).show()
df.filter(df.state.isin(li)==False).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter with Multiple Conditions</b></summary>
                        <pre class="language-python"><code>//Filter multiple condition
df.filter( (df.state  == "OH") & (df.gender  == "M") ) \
    .show(truncate=False)  
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter Based on Starts With, Ends With, Contains</b></summary>
                        <pre class="language-python"><code># Using startswith
df.filter(df.state.startswith("N")).show()
+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|
|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|
|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|
+--------------------+------------------+-----+------+

#using endswith
df.filter(df.state.endswith("H")).show()

#contains
df.filter(df.state.contains("H")).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter like and rlike</b></summary>
                        <pre class="language-python"><code>data2 = [(2,"Michael Rose"),(3,"Robert Williams"),
        (4,"Rames Rose"),(5,"Rames rose")
    ]
df2 = spark.createDataFrame(data = data2, schema = ["id","name"])

# like - SQL LIKE pattern
df2.filter(df2.name.like("%rose%")).show()
+---+----------+
| id|      name|
+---+----------+
|  5|Rames rose|
+---+----------+

# rlike - SQL RLIKE pattern (LIKE with Regex)
#This check case insensitive
df2.filter(df2.name.rlike("(?i)^*rose$")).show()
+---+------------+
| id|        name|
+---+------------+
|  2|Michael Rose|
|  4|  Rames Rose|
|  5|  Rames rose|               
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter on an Array column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import array_contains
df.filter(array_contains(df.languages,"Java")) \
    .show(truncate=False)     

+----------------+------------------+-----+------+
# |name            |languages         |state|gender|
# +----------------+------------------+-----+------+
# |[James, , Smith]|[Java, Scala, C++]|OH   |M     |
# |[Anna, Rose, ]  |[Spark, Java, C++]|NY   |F     |
# +----------------+------------------+-----+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filtering on Nested Struct columns</b></summary>
                        <pre class="language-python"><code>## Struct condition
df.filter(df.name.lastname == "Williams") \
    .show(truncate=False) 
+----------------------+------------+-----+------+
|name                  |languages   |state|gender|
+----------------------+------------+-----+------+
|[Julia, , Williams]   |[CSharp, VB]|OH   |F     |
|[Mike, Mary, Williams]|[Python, VB]|OH   |M     |
+----------------------+------------+-----+------+  
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>DataFrame sorting using the sort() function</b></summary>
                        <pre class="language-python"><code>df.sort("department","state").show(truncate=False)
df.sort(col("department"),col("state")).show(truncate=False)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>sorting using orderBy() function</b></summary>
                        <pre class="language-python"><code>df.orderBy("department","state").show(truncate=False)
df.orderBy(col("department"),col("state")).show(truncate=False)

# Sort by Ascending (ASC)
df.sort(df.department.asc(),df.state.asc()).show(truncate=False)
df.sort(col("department").asc(),col("state").asc()).show(truncate=False)
df.orderBy(col("department").asc(),col("state").asc()).show(truncate=False)

# Sort by Descending (DESC)
df.sort(df.department.asc(),df.state.desc()).show(truncate=False)
df.sort(col("department").asc(),col("state").desc()).show(truncate=False)
df.orderBy(col("department").asc(),col("state").desc()).show(truncate=False)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>sort using Raw SQL</b></summary>
                        <pre class="language-python"><code>df.createOrReplaceTempView("EMP")
spark.sql("select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc").show(truncate=False)      
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>explode array and map columns to rows</b></summary>
                        <pre class="language-python"><code># Create data
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()

arrayData = [
        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),
        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),
        ('Robert',['CSharp',''],{'hair':'red','eye':''}),
        ('Washington',None,None),
        ('Jefferson',['1','2'],{})

df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])
df.printSchema()
df.show()                      
</code></pre>
                        <h3>explode array or map column to rows</h3>
                        <p>used to explode or create array or map columns to rows. <br> When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. <br> When a map is passed, it creates two new columns
                            one for key and one for value and each element in map split into the rows.</p>
                        <pre class="language-python"><code># explode array or map column to rows
from pyspark.sql.functions import explode
df2 = df.select(df.name,explode(df.knownLanguages))
df3 = df.select(df.name,explode(df.properties))
df2.show()
+---------+------+
# |     name|   col|
# +---------+------+
# |    James|  Java|
# |    James| Scala|
# |  Michael| Spark|
# |  Michael|  Java|
# |  Michael|  null|
# |   Robert|CSharp|
# |   Robert|      |
# |Jefferson|     1|
# |Jefferson|     2|
# +---------+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>explode_outer – Create rows for each element in an array or map</b></summary>
                        <p>PySpark SQL explode_outer(e: Column) function is used to create a row for each element in the array or map column. Unlike explode, if the array or map is null or empty, explode_outer returns null.
                            <pre class="language-python"><code>from pyspark.sql.functions import explode_outer
""" with array """
df.select(df.name,explode_outer(df.knownLanguages)).show()
""" with map """
df.select(df.name,explode_outer(df.properties)).show()
# +----------+------+
# |      name|   col|
# +----------+------+
# |     James|  Java|
# |     James| Scala|
# |   Michael| Spark|
# |   Michael|  Java|
# |   Michael|  null|
# |    Robert|CSharp|
# |    Robert|      |
# |Washington|  null|
# | Jefferson|     1|
# | Jefferson|     2|
# +----------+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>posexplode – explode array or map elements to rows</b></summary>
                        <p>creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. And when the input column is a map, posexplode function creates 3
                            columns “pos” to hold the position of the map element, “key” and “value” columns.</p>
                        <p>This will ignore elements that have null or empty. Since the Washington and Jefferson have null or empty values in array and map, the following snippet out does not contain these.</p>
                        <pre class="language-python"><code>from pyspark.sql.functions import posexplode
""" with array """
df.select(df.name,posexplode(df.knownLanguages)).show()
""" with map """
df.select(df.name,posexplode(df.properties)).show()
# +---------+---+------+
# |     name|pos|   col|
# +---------+---+------+
# |    James|  0|  Java|
# |    James|  1| Scala|
# |  Michael|  0| Spark|
# |  Michael|  1|  Java|
# |  Michael|  2|  null|
# |   Robert|  0|CSharp|
# |   Robert|  1|      |
# |Jefferson|  0|     1|
# |Jefferson|  1|     2|
# +---------+---+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>posexplode_outer – explode array or map columns to rows</b></summary>
                        <p>creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. Unlike posexplode, if the array or map is null or empty, posexplode_outer
                            function returns null, null for pos and col columns. Similarly for the map, it returns rows with nulls.</p>
                        <pre class="language-python"><code>from pyspark.sql.functions import posexplode_outer
""" with array """
df.select($"name",posexplode_outer($"knownLanguages")).show()

""" with map """
df.select(df.name,posexplode_outer(df.properties)).show()
# +----------+----+----+-----+
# |      name| pos| key|value|
# +----------+----+----+-----+
# |     James|   0| eye|brown|
# |     James|   1|hair|black|
# |   Michael|   0| eye| null|
# |   Michael|   1|hair|brown|
# |    Robert|   0| eye|     |
# |    Robert|   1|hair|  red|
# |Washington|null|null| null|
# | Jefferson|null|null| null|
# +----------+----+----+-----+
</code></pre>

                    </details>
                </li>
            </ul>
        </details>
    </div>
    <script src="../prism.js"></script>
</body>

</html>