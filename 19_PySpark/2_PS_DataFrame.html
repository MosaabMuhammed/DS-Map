<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        PySpark DataFrame
    </title>
    <link rel="stylesheet" href="../prism.css">
</head>

<body>
    <h1 id="3-data-wrangling">
        PySpark DataFrame
    </h1>
    <div style="width:1000px;margin:auto">
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Load Data into DataFrame</b></summary>
            <ul>

                <li>
                    <details>
                        <summary><b>createDataFrame()</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
df.show()
+---------+----------+--------+----------+------+------+
# |firstname|middlename|lastname|dob       |gender|salary|
# +---------+----------+--------+----------+------+------+
# |James    |          |Smith   |1991-04-01|M     |3000  |
# |Michael  |Rose      |        |2000-05-19|M     |4000  |
# |Robert   |          |Williams|1978-09-05|M     |4000  |
# |Maria    |Anne      |Jones   |1967-12-01|F     |4000  |
# |Jen      |Mary      |Brown   |1980-02-17|F     |-1    |
# +---------+----------+--------+----------+------+------+
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>createDataFrame() with defined schema</b></summary>
                        <pre class="language-python"><code>from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
    ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
    ])
    
df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>From RDD</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
rdd = spark.sparkContext.parallelize(data)
                
columns = ["language","users_count"]
dfFromRDD1 = rdd.toDF(columns)
dfFromRDD1.printSchema()
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>from CSV file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

# Method 1
df = spark.read.csv("/tmp/resources/zipcodes.csv")

# Method 2
df = spark.read.format("csv")
                  .load("/tmp/resources/zipcodes.csv")

# Method 3
df = spark.read.format("org.apache.spark.sql.csv")
                  .load("/tmp/resources/zipcodes.csv")

# Read csv file with headers.
df = spark.read.csv("../input/movielens-20m-dataset/rating.csv", inferSchema=True, header=True)
df2 = spark.read.option("header",True).csv("/tmp/resources/zipcodes.csv")

# Read multiple csv file.
df = spark.read.csv("path1,path2,path3")

# Read all csv files in a directory.
df = spark.read.csv("Folder path")

# Read csv file with options.
df3 = spark.read.options(header='True', inferSchema='True', delimiter=',')\
                .csv("/tmp/resources/zipcodes.csv")
             

df.printSchema()
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>from TXT file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

df = spark.read.text("/src/resources/file.txt")
df.printSchema()
</code></pre>
                    </details>
                </li>


                <li>
                    <details>
                        <summary><b>from JSON file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

df = spark.read.json("/src/resources/file.json")
df.printSchema()
</code></pre>
                    </details>
                </li>
            </ul>
        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>DF Operations</b></summary>
            <ul>
                <li>
                    <details>
                        <summary><b>Rename columns</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql.functions import *

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

dataDF = [(('James','','Smith'),'1991-04-01','M',3000),
    (('Michael','Rose',''),'2000-05-19','M',4000),
    (('Robert','','Williams'),'1978-09-05','M',4000),
    (('Maria','Anne','Jones'),'1967-12-01','F',4000),
    (('Jen','Mary','Brown'),'1980-02-17','F',-1)
]

schema = StructType([
        StructField('name', StructType([
                StructField('firstname', StringType(), True),
                StructField('middlename', StringType(), True),
                StructField('lastname', StringType(), True)
                ])),
            StructField('dob', StringType(), True),
            StructField('gender', StringType(), True),
            StructField('salary', IntegerType(), True)
            ])

df = spark.createDataFrame(data = dataDF, schema = schema)
df.printSchema()

# Example 1
df.withColumnRenamed("dob","DateOfBirth").printSchema()
# Example 2   
df2 = df.withColumnRenamed("dob","DateOfBirth") \
    .withColumnRenamed("salary","salary_amount")
df2.printSchema()

# Example 3 
schema2 = StructType([
    StructField("fname",StringType()),
    StructField("middlename",StringType()),
    StructField("lname",StringType())])
    
df.select(col("name").cast(schema2),
    col("dob"),
    col("gender"),
    col("salary")) \
    .printSchema()    

# Example 4 
df.select(col("name.firstname").alias("fname"),
    col("name.middlename").alias("mname"),
    col("name.lastname").alias("lname"),
    col("dob"),col("gender"),col("salary")) \
    .printSchema()
    
# Example 5
df4 = df.withColumn("fname",col("name.firstname")) \
        .withColumn("mname",col("name.middlename")) \
        .withColumn("lname",col("name.lastname")) \
        .drop("name")
df4.printSchema()

#Example 7
newColumns = ["newCol1","newCol2","newCol3","newCol4"]
df.toDF(*newColumns).printSchema()
                        </code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary>get <b>shape of df</b> (n_rows, n_cols)</summary>
                        <pre class="language-python"><code>def shape(df):
    return (df.count(), len(df.columns))

train_data, test_data = scaled_df.randomSplit([.8, .2], seed=seed)

print(shape(train_data), shape(test_data))
# (16395, 10) (4038, 10)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary>Create a user-defined function <b>UDF</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

columns = ["Seqno","Name"]
data = [("1", "john jones"),
    ("2", "tracey smith"),
    ("3", "amy sanders")]

df = spark.createDataFrame(data=data,schema=columns)

df.show(truncate=False)

def convertCase(str):
    resStr=""
    arr = str.split(" ")
    for x in arr:
        resStr= resStr + x[0:1].upper() + x[1:len(x)] + " "
    return resStr 

""" Converting function to UDF """
convertUDF = udf(lambda z: convertCase(z))

df.select(col("Seqno"), \
    convertUDF(col("Name")).alias("Name") ) \
.show(truncate=False)

def upperCase(str):
    return str.upper()

upperCaseUDF = udf(lambda z:upperCase(z),StringType())    

df.withColumn("Cureated Name", upperCaseUDF(col("Name"))) \
.show(truncate=False)

""" Using UDF on SQL """
spark.udf.register("convertUDF", convertCase,StringType())
df.createOrReplaceTempView("NAME_TABLE")
spark.sql("select Seqno, convertUDF(Name) as Name from NAME_TABLE") \
        .show(truncate=False)
        
spark.sql("select Seqno, convertUDF(Name) as Name from NAME_TABLE " + \
            "where Name is not null and convertUDF(Name) like '%John%'") \
        .show(truncate=False)  
        
""" null check """

columns = ["Seqno","Name"]
data = [("1", "john jones"),
    ("2", "tracey smith"),
    ("3", "amy sanders"),
    ('4',None)]

df2 = spark.createDataFrame(data=data,schema=columns)
df2.show(truncate=False)
df2.createOrReplaceTempView("NAME_TABLE2")
    
spark.udf.register("_nullsafeUDF", lambda str: convertCase(str) if not str is None else "" , StringType())

spark.sql("select _nullsafeUDF(Name) from NAME_TABLE2") \
        .show(truncate=False)

spark.sql("select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 " + \
            " where Name is not null and _nullsafeUDF(Name) like '%John%'") \
        .show(truncate=False)                              
</code></pre>

                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Descriptive Statictics</b> describe()</summary>
                        <pre class="language-python"><code>(housing_df.describe().select(
"summary",
F.round("medage", 4).alias("medage"),
F.round("totrooms", 4).alias("totrooms"),
F.round("totbdrms", 4).alias("totbdrms"),
F.round("pop", 4).alias("pop"),
F.round("houshlds", 4).alias("houshlds"),
F.round("medinc", 4).alias("medinc"),
F.round("medhv", 4).alias("medhv"))
.show()
)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary>get <b>number of nan/null/missing</b> rows for each column</summary>
                        <p>isnan() is a function of the pysparq.sql.function package, we have to set which column we want to use as an argument of the function.
                            <br> isNull()" belongs to pyspark.sql.Column package, to check the null status of a column</p>
                        <pre class="language-python"><code>import pyspark.sql.functions as F
df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()
# +-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
# |PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|
# +-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
# |          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|
# +-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
</code></pre>
                        <pre class="language-python"><code># For string columns, we check for None and null
# For numeric columns, we check for zeroes and NaN
# For array type columns, we check if the array contain zeroes or NaN
                            
string_columns = ['gender', 'sport', 'url']
numeric_columns = ['id','userId']
array_columns = ['altitude', 'heart_rate', 'latitude', 'longitude', 'speed', 'timestamp']
missing_values = {} 

for index, column in enumerate(df.columns):
    if column in string_columns:    # check string columns with None and Null values
#         missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()
#         missing_values.update({column: missing_count})
        missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()
        missing_values.update({column:missing_count})
    if column in numeric_columns:  # check zeroes, None, NaN
        missing_count = df.where(col(column).isin([0,None,np.nan])).count()
        missing_values.update({column:missing_count})
    if column in array_columns:  # check zeros and NaN
        missing_count = df.filter(array_contains(df[column], 0) | array_contains(df[column], np.nan)).count()
        missing_values.update({column:missing_count})
missing_df = pd.DataFrame.from_dict([missing_values])
missing_df
# altitude	gender	heart_rate	id	latitude	longitude	speed	sport	timestamp	url	userId
# 40848	     0	       1280	     0	    113	     113	     7741	   0       0	     0	  0
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Drop nan/missing values</b></summary>
                        <pre class="language-python"><code>user_log_valid = user_log.dropna(how = "any", subset = ["userId", "sessionId"])
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary>Fill<b> nan/null/missing</b> values in a column</summary>
                        <pre class="language-python"><code>avg_age = df.groupby("Initial").avg("Age")
# +-------+------------------+
# |Initial|          avg(Age)|
# +-------+------------------+
# |   Miss|             21.86|
# |  Other|45.888888888888886|
# | Master| 4.574166666666667|
# |     Mr| 32.73960880195599|
# |    Mrs|35.981818181818184|
# +-------+------------------+

# Fill the missing values of Age with average of the previous initials.
for row in avg_age.collect():
    df = df.withColumn("Age", F.when((df["Initial"] == row['Initial']) & (df['Age'].isNull()), row['avg(Age)']).otherwise(df['Age']))
</code></pre>
                        <pre class="language-python"><code># As we can see, Majority Passengers boarded from "S". We can assign missing value with "S"
df = df.na.fill({"Embarked" : 'S'})    </code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary>From <b>Dataframe</b> to <b>Dictionary</b></summary>
                        <pre class="language-python"><code>page_views_timestamp_stats_df.rdd.collectAsMap()
# {'count': '2034275448',
#  'max': '1296000000',
#  'mean': '6.51464604736012E8',
#  'min': '0',
#  'stddev': '3.809745454636914E8'}
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary> <b>Drop Duplicates</b></summary>
                        <pre class="language-python"><code>user_log.select("page").dropDuplicates().sort("page").show()
</code></pre>

                    </details>
                </li>
                <li>
                    <details>
                        <summary>replace labels in a categorical column</summary>
                        <pre class="language-python"><code>df = df.replace(
   ['Mr', 'Sir', 'Mlle', 'Don', 'Miss', 'Capt', 'Mrs', 'Col', 'Rev','Ms', 'Master', 'Major', 'Jonkheer', 'Dr', 'Countess', 'Mme', 'Lady'],
    ['Mr', 'Mr', 'Miss', 'Mr', 'Miss', 'Mr', 'Mrs', 'Other', 'Other', 'Ms', 'Mr', 'Mr', 'Other', 'Mr', 'Other', 'Miss', 'Miss'])
</code></pre>

                    </details>
                </li>


                <li>
                    <details>
                        <summary>Extract a pattern from a text using regular expression</summary>
                        <pre class="language-python"><code>df = df.withColumn("Initial", F.regexp_extract(F.col("Name"), "([A-Za-z]+)\.", 1))
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Change DataType using PySpark withColumn()</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("salary", col("salary").cast("Integer")).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Update The Value of an Existing Column</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("salary", col("salary")*100).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Create a Column from an Existing Column</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("CopiedSalary", col("salary")*-1).show()
# +---------+----------+--------+----------+------+------+------------+
# |firstname|middlename|lastname|       dob|gender|salary|CopiedSalary|
# +---------+----------+--------+----------+------+------+------------+
# |    James|          |   Smith|1991-04-01|     M|  3000|       -3000|
# |  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|
# |   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|
# |    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|
# |      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|
# +---------+----------+--------+----------+------+------+------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Add a New Column using withColumn()</b></summary>
                        <pre class="language-python"><code>#  PySpark lit() function is used to add a constant value to a DataFrame column.
import pyspark
from pyspark.sql.functions import col, lit
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("Country", lit("USA")) \
  .withColumn("anotherCountry", lit("Egypt")) \
  .show()
# +---------+----------+--------+----------+------+------+-------+--------------+
# |firstname|middlename|lastname|       dob|gender|salary|Country|anotherCountry|
# +---------+----------+--------+----------+------+------+-------+--------------+
# |    James|          |   Smith|1991-04-01|     M|  3000|    USA|         Egypt|
# |  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|         Egypt|
# |   Robert|          |Williams|1978-09-05|     M|  4000|    USA|         Egypt|
# |    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|         Egypt|
# |      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|         Egypt|
# +---------+----------+--------+----------+------+------+-------+--------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Drop Column From PySpark DataFrame</b></summary>
                        <pre class="language-python"><code>#  PySpark lit() function is used to add a constant value to a DataFrame column.
import pyspark
from pyspark.sql.functions import col, lit
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.drop("salary").show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>filter() with Column Condition</b></summary>
                        <pre class="language-python"><code># Using equals condition
df.filter(df.state == "OH").show(truncate=False)

+----------------------+------------------+-----+------+
# |name                  |languages         |state|gender|
# +----------------------+------------------+-----+------+
# |[James, , Smith]      |[Java, Scala, C++]|OH   |M     |
# |[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |
# |[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |
# +----------------------+------------------+-----+------+

# not equals condition
df.filter(df.state != "OH") \
    .show(truncate=False) 
df.filter(~(df.state == "OH")) \
    .show(truncate=False)

#Using SQL col() function
from pyspark.sql.functions import col
df.filter(col("state") == "OH") \
    .show(truncate=False) 
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>filter() with SQL Expression</b></summary>
                        <pre class="language-python"><code>#Using SQL Expression
df.filter("gender == 'M'").show()
#For not equal
df.filter("gender != 'M'").show()
df.filter("gender <> 'M'").show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter Based on List Values</b></summary>
                        <pre class="language-python"><code>#Filter IS IN List values
li=["OH","CA","DE"]
df.filter(df.state.isin(li)).show()
+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|
| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|
|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|
+--------------------+------------------+-----+------+

# Filter NOT IS IN List values
#These show all records with NY (NY is not part of the list)
df.filter(~df.state.isin(li)).show()
df.filter(df.state.isin(li)==False).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter with Multiple Conditions</b></summary>
                        <pre class="language-python"><code>//Filter multiple condition
df.filter( (df.state  == "OH") & (df.gender  == "M") ) \
    .show(truncate=False)  
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter Based on Starts With, Ends With, Contains</b></summary>
                        <pre class="language-python"><code># Using startswith
df.filter(df.state.startswith("N")).show()
+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|
|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|
|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|
+--------------------+------------------+-----+------+

#using endswith
df.filter(df.state.endswith("H")).show()

#contains
df.filter(df.state.contains("H")).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter like and rlike</b></summary>
                        <pre class="language-python"><code>data2 = [(2,"Michael Rose"),(3,"Robert Williams"),
        (4,"Rames Rose"),(5,"Rames rose")
    ]
df2 = spark.createDataFrame(data = data2, schema = ["id","name"])

# like - SQL LIKE pattern
df2.filter(df2.name.like("%rose%")).show()
+---+----------+
| id|      name|
+---+----------+
|  5|Rames rose|
+---+----------+

# rlike - SQL RLIKE pattern (LIKE with Regex)
#This check case insensitive
df2.filter(df2.name.rlike("(?i)^*rose$")).show()
+---+------------+
| id|        name|
+---+------------+
|  2|Michael Rose|
|  4|  Rames Rose|
|  5|  Rames rose|               
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter on an Array column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import array_contains
df.filter(array_contains(df.languages,"Java")) \
    .show(truncate=False)     

+----------------+------------------+-----+------+
# |name            |languages         |state|gender|
# +----------------+------------------+-----+------+
# |[James, , Smith]|[Java, Scala, C++]|OH   |M     |
# |[Anna, Rose, ]  |[Spark, Java, C++]|NY   |F     |
# +----------------+------------------+-----+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filtering on Nested Struct columns</b></summary>
                        <pre class="language-python"><code>## Struct condition
df.filter(df.name.lastname == "Williams") \
    .show(truncate=False) 
+----------------------+------------+-----+------+
|name                  |languages   |state|gender|
+----------------------+------------+-----+------+
|[Julia, , Williams]   |[CSharp, VB]|OH   |F     |
|[Mike, Mary, Williams]|[Python, VB]|OH   |M     |
+----------------------+------------+-----+------+  
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>DataFrame sorting using the sort() function</b></summary>
                        <pre class="language-python"><code>df.sort("department","state").show(truncate=False)
df.sort(col("department"),col("state")).show(truncate=False)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>sorting using orderBy() function</b></summary>
                        <pre class="language-python"><code>df.orderBy("department","state").show(truncate=False)
df.orderBy(col("department"),col("state")).show(truncate=False)

# Sort by Ascending (ASC)
df.sort(df.department.asc(),df.state.asc()).show(truncate=False)
df.sort(col("department").asc(),col("state").asc()).show(truncate=False)
df.orderBy(col("department").asc(),col("state").asc()).show(truncate=False)

# Sort by Descending (DESC)
df.sort(df.department.asc(),df.state.desc()).show(truncate=False)
df.sort(col("department").asc(),col("state").desc()).show(truncate=False)
df.orderBy(col("department").asc(),col("state").desc()).show(truncate=False)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>sort using Raw SQL</b></summary>
                        <pre class="language-python"><code>df.createOrReplaceTempView("EMP")
spark.sql("select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc").show(truncate=False)      
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>explode array and map columns to rows</b></summary>
                        <pre class="language-python"><code># Create data
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()

arrayData = [
        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),
        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),
        ('Robert',['CSharp',''],{'hair':'red','eye':''}),
        ('Washington',None,None),
        ('Jefferson',['1','2'],{})

df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])
df.printSchema()
df.show()                      
</code></pre>
                        <h3>explode array or map column to rows</h3>
                        <p>used to explode or create array or map columns to rows. <br> When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. <br> When a map is passed, it creates two new columns
                            one for key and one for value and each element in map split into the rows.</p>
                        <pre class="language-python"><code># explode array or map column to rows
from pyspark.sql.functions import explode
df2 = df.select(df.name,explode(df.knownLanguages))
df3 = df.select(df.name,explode(df.properties))
df2.show()
+---------+------+
# |     name|   col|
# +---------+------+
# |    James|  Java|
# |    James| Scala|
# |  Michael| Spark|
# |  Michael|  Java|
# |  Michael|  null|
# |   Robert|CSharp|
# |   Robert|      |
# |Jefferson|     1|
# |Jefferson|     2|
# +---------+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>explode_outer – Create rows for each element in an array or map</b></summary>
                        <p>PySpark SQL explode_outer(e: Column) function is used to create a row for each element in the array or map column. Unlike explode, if the array or map is null or empty, explode_outer returns null.
                            <pre class="language-python"><code>from pyspark.sql.functions import explode_outer
""" with array """
df.select(df.name,explode_outer(df.knownLanguages)).show()
""" with map """
df.select(df.name,explode_outer(df.properties)).show()
# +----------+------+
# |      name|   col|
# +----------+------+
# |     James|  Java|
# |     James| Scala|
# |   Michael| Spark|
# |   Michael|  Java|
# |   Michael|  null|
# |    Robert|CSharp|
# |    Robert|      |
# |Washington|  null|
# | Jefferson|     1|
# | Jefferson|     2|
# +----------+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>posexplode – explode array or map elements to rows</b></summary>
                        <p>creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. And when the input column is a map, posexplode function creates 3
                            columns “pos” to hold the position of the map element, “key” and “value” columns.</p>
                        <p>This will ignore elements that have null or empty. Since the Washington and Jefferson have null or empty values in array and map, the following snippet out does not contain these.</p>
                        <pre class="language-python"><code>from pyspark.sql.functions import posexplode
""" with array """
df.select(df.name,posexplode(df.knownLanguages)).show()
""" with map """
df.select(df.name,posexplode(df.properties)).show()
# +---------+---+------+
# |     name|pos|   col|
# +---------+---+------+
# |    James|  0|  Java|
# |    James|  1| Scala|
# |  Michael|  0| Spark|
# |  Michael|  1|  Java|
# |  Michael|  2|  null|
# |   Robert|  0|CSharp|
# |   Robert|  1|      |
# |Jefferson|  0|     1|
# |Jefferson|  1|     2|
# +---------+---+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>posexplode_outer – explode array or map columns to rows</b></summary>
                        <p>creates a row for each element in the array and creates two columns “pos’ to hold the position of the array element and the ‘col’ to hold the actual array value. Unlike posexplode, if the array or map is null or empty, posexplode_outer
                            function returns null, null for pos and col columns. Similarly for the map, it returns rows with nulls.</p>
                        <pre class="language-python"><code>from pyspark.sql.functions import posexplode_outer
""" with array """
df.select($"name",posexplode_outer($"knownLanguages")).show()

""" with map """
df.select(df.name,posexplode_outer(df.properties)).show()
# +----------+----+----+-----+
# |      name| pos| key|value|
# +----------+----+----+-----+
# |     James|   0| eye|brown|
# |     James|   1|hair|black|
# |   Michael|   0| eye| null|
# |   Michael|   1|hair|brown|
# |    Robert|   0| eye|     |
# |    Robert|   1|hair|  red|
# |Washington|null|null| null|
# | Jefferson|null|null| null|
# +----------+----+----+-----+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>explode & flatten nested array (Array of Array) DataFrame columns into rows</b></summary>
                        <pre class="language-python"><code>import pyspark
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

arrayArrayData = [
    ("James",[["Java","Scala","C++"],["Spark","Java"]]),
    ("Michael",[["Spark","Java","C++"],["Spark","Java"]]),
    ("Robert",[["CSharp","VB"],["Spark","Python"]])
]

df = spark.createDataFrame(data=arrayArrayData, schema=["name", "subjects"])
df.printSchema()
df.show(truncate=False)

df.select("name", pyspark.sql.functions.explode("subjects")).show()
# +-------+------------------+
# |   name|               col|
# +-------+------------------+
# |  James|[Java, Scala, C++]|
# |  James|     [Spark, Java]|
# |Michael|[Spark, Java, C++]|
# |Michael|     [Spark, Java]|
# | Robert|      [CSharp, VB]|
# | Robert|   [Spark, Python]|
# +-------+------------------+

df.select("name", pyspark.sql.functions.flatten("subjects")).show(truncate=False)
# +-------+-------------------------------+
# |name   |flatten(subjects)              |
# +-------+-------------------------------+
# |James  |[Java, Scala, C++, Spark, Java]|
# |Michael|[Spark, Java, C++, Spark, Java]|
# |Robert |[CSharp, VB, Spark, Python]    |
# +-------+-------------------------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Save DF as csv file</b></summary>
                        <pre class="language-python"><code>df2.write.options(header='True', delimiter=',') \
         .csv("/tmp/spark_output/zipcodes")

### Saving modes.
# overwrite – mode is used to overwrite the existing file.
# append – To add the data to the existing file.
# ignore – Ignores write operation when the file already exists.
# error – This is a default option when the file already exists, it returns an error.

df2.write.mode('overwrite').csv("/tmp/spark_output/zipcodes")
## you can also use this
df2.write.format("csv").mode('overwrite').save("/tmp/spark_output/zipcodes")         
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Group by [min, max, avg, sum, mean, agg]</b></summary>
                        <p>
                            When we perform groupBy() on PySpark Dataframe, it returns GroupedData object which contains below aggregate functions. <br> count() - Returns the count of rows for each group. <br> mean() - Returns the mean of values for each
                            group. <br> max() - Returns the maximum of values for each group. <br> min() - Returns the minimum of values for each group. <br> sum() - Returns the total for values for each group. <br> avg() - Returns the average for values
                            for each group. <br> agg() - Using agg() function, we can calculate more than one aggregate at a time. <br>
                        </p>
                        <pre class="language-python"><code>df.groupBy("department").sum("salary").show(truncate=False)
df.groupBy("department").count()
df.groupBy("department").min("salary")
df.groupBy("department").max("salary")
df.groupBy("department").avg( "salary")
df.groupBy("department").mean( "salary") 

## GroupBy on multiple columns
df.groupBy("department","state") \
    .sum("salary","bonus") \
    .show(false)

# Running more aggregates at a time   
df.groupBy("department") \
    .agg(sum("salary").alias("sum_salary"), \
         avg("salary").alias("avg_salary"), \
         sum("bonus").alias("sum_bonus"), \
         max("bonus").alias("max_bonus") \
     ) \
    .show(truncate=False)

# using filter on grouped by data.
df.groupBy("department") \
    .agg(sum("salary").alias("sum_salary"), \
      avg("salary").alias("avg_salary"), \
      sum("bonus").alias("sum_bonus"), \
      max("bonus").alias("max_bonus")) \
    .where(col("sum_bonus") >= 50000) \
    .show(truncate=False)
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Get number of distinct labels in a column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import approx_count_distinct, countDistinct

print(str(df.select(approx_count_distinct("department")).collect()[0][0]))
# 3

# for a composite row.
df.select(countDistinct("department", "state")).collect()[0][0]
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get the average of numerical column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import avg

# avg
print("avg: " + str(df.select(avg("salary")).collect()[0][0]))
# Prints avg: 3400.0
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get all values from an input column with duplicates to list</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import collect_list
df.select(collect_list("salary")).collect()[0][0]
# [3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Returns the number of distinct elements in a columns</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import collect_set
df.select(collect_set("salary")).collect()[0][0]
# OR
df.select('salary').distinct().count() # distinct() will return a dataframe.
# [4600, 3000, 3900, 4100, 3300, 2000]
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get number of elements in a column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import count
print("count: "+str(df.select(count("salary")).collect()[0]))
# Prints county: 10
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get the first/last element in a column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import first, last
# first() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.
df.select(first("salary")).show(truncate=False)
# +--------------------+
# |first(salary, false)|
# +--------------------+
# |3000                |
# +--------------------+

df.select(last("salary")).show(truncate=False)
+-------------------+
|last(salary, false)|
+-------------------+
|4100               |
+-------------------+
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get kurtosis/max/min/mean/sum/skewness of a column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import kurtosis, max, min, mean, sum, skewness, sumDistinct

df.select(kurtosis("salary")).show(truncate=False)
df.select(max("salary")).show(truncate=False)
df.select(min("salary")).show(truncate=False)
df.select(mean("salary")).show(truncate=False)
df.select(skewness("salary")).show(truncate=False)
df.select(sum("salary")).show(truncate=False)
df.select(sumDistinct("salary")).show(truncate=False)
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get standard deviation std of a column</b></summary>
                        <p>
                            stddev() alias for stddev_samp. <br> stddev_samp() function returns the sample standard deviation of values in a column. <br> stddev_pop() function returns the population standard deviation of the values in a column. <br>
                        </p>
                        <pre class="language-python"><code>from pyspark.sql.functions import stddev, stddev_samp, stddev_pop
df.select(stddev("salary"), stddev_samp("salary"), \
stddev_pop("salary")).show(truncate=False)

# +-------------------+-------------------+------------------+
# |stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|
# +-------------------+-------------------+------------------+
# |765.9416862050705  |765.9416862050705  |726.636084983398  |
# +-------------------+-------------------+------------------+
</code></pre>
                    </details>
                </li>
                <li>
                    <details>
                        <summary><b>Get Variance of a column</b></summary>
                        <p>
                            variance() alias for var_samp <br> var_samp() function returns the unbiased variance of the values in a column. <br> var_pop() function returns the population variance of the values in a column. <br>
                        </p>
                        <pre class="language-python"><code>from pyspark.sql.functions import variance, var_samp, var_pop
df.select(variance("salary"),var_samp("salary"),var_pop("salary")) \
.show(truncate=False)

# +-----------------+-----------------+---------------+
# |var_samp(salary) |var_samp(salary) |var_pop(salary)|
# +-----------------+-----------------+---------------+
# |586666.6666666666|586666.6666666666|528000.0       |
# +-----------------+-----------------+---------------+
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Apply SQL queries on DF</b></summary>
                        <pre class="language-python"><code>empDF.createOrReplaceTempView("EMP")
deptDF.createOrReplaceTempView("DEPT")

joinDF = spark.sql("select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id") \
    .show(truncate=False)

joinDF2 = spark.sql("select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id") \
    .show(truncate=False)
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Join DFs together</b></summary>
                        <pre class="language-python"><code>df1.join(df2,df1.id1 == df2.id2,"inner") \
    .join(df3,df1.id1 == df3.id3,"inner")

empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"outer") \
    .show(truncate=False)
empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"full") \
    .show(truncate=False)
empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"fullouter") \
    .show(truncate=False)
    
empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"left") \
    .show(truncate=False)
empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"leftouter") \
    .show(truncate=False)

empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"right") \
    .show(truncate=False)
empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"rightouter") \
    .show(truncate=False)

empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"leftsemi") \
    .show(truncate=False)
    
empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,"leftanti") \
    .show(truncate=False)
</code></pre>
                    </details>
                </li>
            </ul>
        </details>
    </div>
    <script src="../prism.js"></script>
</body>

</html>