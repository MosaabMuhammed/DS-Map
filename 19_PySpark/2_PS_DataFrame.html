<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        PySpark DataFrame
    </title>
    <link rel="stylesheet" href="../prism.css">
</head>

<body>
    <h1 id="3-data-wrangling">
        PySpark DataFrame
    </h1>
    <div style="width:1000px;margin:auto">
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Load Data into DataFrame</b></summary>
            <ul>

                <li>
                    <details>
                        <summary><b>createDataFrame()</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
df.show()
+---------+----------+--------+----------+------+------+
# |firstname|middlename|lastname|dob       |gender|salary|
# +---------+----------+--------+----------+------+------+
# |James    |          |Smith   |1991-04-01|M     |3000  |
# |Michael  |Rose      |        |2000-05-19|M     |4000  |
# |Robert   |          |Williams|1978-09-05|M     |4000  |
# |Maria    |Anne      |Jones   |1967-12-01|F     |4000  |
# |Jen      |Mary      |Brown   |1980-02-17|F     |-1    |
# +---------+----------+--------+----------+------+------+
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>createDataFrame() with defined schema</b></summary>
                        <pre class="language-python"><code>from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
    ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
    ])
    
df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>From RDD</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
rdd = spark.sparkContext.parallelize(data)
                
columns = ["language","users_count"]
dfFromRDD1 = rdd.toDF(columns)
dfFromRDD1.printSchema()
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>from CSV file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()
                
df = spark.read.csv("/tmp/resources/zipcodes.csv")
df.printSchema()
</code></pre>
                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>from TXT file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

df = spark.read.text("/src/resources/file.txt")
df.printSchema()
</code></pre>
                    </details>
                </li>


                <li>
                    <details>
                        <summary><b>from JSON file</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

df = spark.read.json("/src/resources/file.json")
df.printSchema()
</code></pre>
                    </details>
                </li>
            </ul>
        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>DF Operations</b></summary>
            <ul>
                <li>
                    <details>
                        <summary><b>Rename columns</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql.functions import *

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

dataDF = [(('James','','Smith'),'1991-04-01','M',3000),
    (('Michael','Rose',''),'2000-05-19','M',4000),
    (('Robert','','Williams'),'1978-09-05','M',4000),
    (('Maria','Anne','Jones'),'1967-12-01','F',4000),
    (('Jen','Mary','Brown'),'1980-02-17','F',-1)
]

schema = StructType([
        StructField('name', StructType([
                StructField('firstname', StringType(), True),
                StructField('middlename', StringType(), True),
                StructField('lastname', StringType(), True)
                ])),
            StructField('dob', StringType(), True),
            StructField('gender', StringType(), True),
            StructField('salary', IntegerType(), True)
            ])

df = spark.createDataFrame(data = dataDF, schema = schema)
df.printSchema()

# Example 1
df.withColumnRenamed("dob","DateOfBirth").printSchema()
# Example 2   
df2 = df.withColumnRenamed("dob","DateOfBirth") \
    .withColumnRenamed("salary","salary_amount")
df2.printSchema()

# Example 3 
schema2 = StructType([
    StructField("fname",StringType()),
    StructField("middlename",StringType()),
    StructField("lname",StringType())])
    
df.select(col("name").cast(schema2),
    col("dob"),
    col("gender"),
    col("salary")) \
    .printSchema()    

# Example 4 
df.select(col("name.firstname").alias("fname"),
    col("name.middlename").alias("mname"),
    col("name.lastname").alias("lname"),
    col("dob"),col("gender"),col("salary")) \
    .printSchema()
    
# Example 5
df4 = df.withColumn("fname",col("name.firstname")) \
        .withColumn("mname",col("name.middlename")) \
        .withColumn("lname",col("name.lastname")) \
        .drop("name")
df4.printSchema()

#Example 7
newColumns = ["newCol1","newCol2","newCol3","newCol4"]
df.toDF(*newColumns).printSchema()
                        </code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Change DataType using PySpark withColumn()</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("salary", col("salary").cast("Integer")).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Update The Value of an Existing Column</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("salary", col("salary")*100).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Create a Column from an Existing Column</b></summary>
                        <pre class="language-python"><code>import pyspark
from pyspark.sql.functions import col
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("CopiedSalary", col("salary")*-1).show()
# +---------+----------+--------+----------+------+------+------------+
# |firstname|middlename|lastname|       dob|gender|salary|CopiedSalary|
# +---------+----------+--------+----------+------+------+------------+
# |    James|          |   Smith|1991-04-01|     M|  3000|       -3000|
# |  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|
# |   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|
# |    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|
# |      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|
# +---------+----------+--------+----------+------+------+------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Add a New Column using withColumn()</b></summary>
                        <pre class="language-python"><code>#  PySpark lit() function is used to add a constant value to a DataFrame column.
import pyspark
from pyspark.sql.functions import col, lit
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.withColumn("Country", lit("USA")) \
  .withColumn("anotherCountry", lit("Egypt")) \
  .show()
# +---------+----------+--------+----------+------+------+-------+--------------+
# |firstname|middlename|lastname|       dob|gender|salary|Country|anotherCountry|
# +---------+----------+--------+----------+------+------+-------+--------------+
# |    James|          |   Smith|1991-04-01|     M|  3000|    USA|         Egypt|
# |  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|         Egypt|
# |   Robert|          |Williams|1978-09-05|     M|  4000|    USA|         Egypt|
# |    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|         Egypt|
# |      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|         Egypt|
# +---------+----------+--------+----------+------+------+-------+--------------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Drop Column From PySpark DataFrame</b></summary>
                        <pre class="language-python"><code>#  PySpark lit() function is used to add a constant value to a DataFrame column.
import pyspark
from pyspark.sql.functions import col, lit
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
('Michael','Rose','','2000-05-19','M',4000),
('Robert','','Williams','1978-09-05','M',4000),
('Maria','Anne','Jones','1967-12-01','F',4000),
('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]
df = spark.createDataFrame(data=data, schema=columns)
df.printSchema()

df.drop("salary").show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>filter() with Column Condition</b></summary>
                        <pre class="language-python"><code># Using equals condition
df.filter(df.state == "OH").show(truncate=False)

+----------------------+------------------+-----+------+
# |name                  |languages         |state|gender|
# +----------------------+------------------+-----+------+
# |[James, , Smith]      |[Java, Scala, C++]|OH   |M     |
# |[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |
# |[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |
# +----------------------+------------------+-----+------+

# not equals condition
df.filter(df.state != "OH") \
    .show(truncate=False) 
df.filter(~(df.state == "OH")) \
    .show(truncate=False)

#Using SQL col() function
from pyspark.sql.functions import col
df.filter(col("state") == "OH") \
    .show(truncate=False) 
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>filter() with SQL Expression</b></summary>
                        <pre class="language-python"><code>#Using SQL Expression
df.filter("gender == 'M'").show()
#For not equal
df.filter("gender != 'M'").show()
df.filter("gender <> 'M'").show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter Based on List Values</b></summary>
                        <pre class="language-python"><code>#Filter IS IN List values
li=["OH","CA","DE"]
df.filter(df.state.isin(li)).show()
+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|    [James, , Smith]|[Java, Scala, C++]|   OH|     M|
| [Julia, , Williams]|      [CSharp, VB]|   OH|     F|
|[Mike, Mary, Will...|      [Python, VB]|   OH|     M|
+--------------------+------------------+-----+------+

# Filter NOT IS IN List values
#These show all records with NY (NY is not part of the list)
df.filter(~df.state.isin(li)).show()
df.filter(df.state.isin(li)==False).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter with Multiple Conditions</b></summary>
                        <pre class="language-python"><code>//Filter multiple condition
df.filter( (df.state  == "OH") & (df.gender  == "M") ) \
    .show(truncate=False)  
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter Based on Starts With, Ends With, Contains</b></summary>
                        <pre class="language-python"><code># Using startswith
df.filter(df.state.startswith("N")).show()
+--------------------+------------------+-----+------+
|                name|         languages|state|gender|
+--------------------+------------------+-----+------+
|      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|
|[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|
|  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|
+--------------------+------------------+-----+------+

#using endswith
df.filter(df.state.endswith("H")).show()

#contains
df.filter(df.state.contains("H")).show()
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter like and rlike</b></summary>
                        <pre class="language-python"><code>data2 = [(2,"Michael Rose"),(3,"Robert Williams"),
        (4,"Rames Rose"),(5,"Rames rose")
    ]
df2 = spark.createDataFrame(data = data2, schema = ["id","name"])

# like - SQL LIKE pattern
df2.filter(df2.name.like("%rose%")).show()
+---+----------+
| id|      name|
+---+----------+
|  5|Rames rose|
+---+----------+

# rlike - SQL RLIKE pattern (LIKE with Regex)
#This check case insensitive
df2.filter(df2.name.rlike("(?i)^*rose$")).show()
+---+------------+
| id|        name|
+---+------------+
|  2|Michael Rose|
|  4|  Rames Rose|
|  5|  Rames rose|               
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filter on an Array column</b></summary>
                        <pre class="language-python"><code>from pyspark.sql.functions import array_contains
df.filter(array_contains(df.languages,"Java")) \
    .show(truncate=False)     

+----------------+------------------+-----+------+
# |name            |languages         |state|gender|
# +----------------+------------------+-----+------+
# |[James, , Smith]|[Java, Scala, C++]|OH   |M     |
# |[Anna, Rose, ]  |[Spark, Java, C++]|NY   |F     |
# +----------------+------------------+-----+------+
</code></pre>

                    </details>
                </li>

                <li>
                    <details>
                        <summary><b>Filtering on Nested Struct columns</b></summary>
                        <pre class="language-python"><code>## Struct condition
df.filter(df.name.lastname == "Williams") \
    .show(truncate=False) 
+----------------------+------------+-----+------+
|name                  |languages   |state|gender|
+----------------------+------------+-----+------+
|[Julia, , Williams]   |[CSharp, VB]|OH   |F     |
|[Mike, Mary, Williams]|[Python, VB]|OH   |M     |
+----------------------+------------+-----+------+  
</code></pre>

                    </details>
                </li>
            </ul>
        </details>
    </div>
    <script src="../prism.js"></script>
</body>

</html>