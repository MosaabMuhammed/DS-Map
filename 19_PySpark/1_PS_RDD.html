<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        PySpark RDD
    </title>
    <link rel="stylesheet" href="../prism.css">
</head>

<body>
    <h1 id="3-data-wrangling">
        PySpark RDD
    </h1>
    <div style="width:1000px;margin:auto">
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary>Create a <b>SparkSession</b></summary>
            <p>SparkSession is an entry point to PySpark and creating a SparkSession instance would be the first statement you would write to program with RDD, DataFrame, and Dataset.</p>
            <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()</code></pre>
            <p>
                master() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn or mesos depends on your cluster setup. <br> Use local[x] when running in Standalone mode. x
                should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have. <br> appName() – Used to
                set your application name. <br> getOrCreate() – This returns a SparkSession object if already exists, creates new one if not exists. <br>
            </p>

            <details>
                <summary><b>SparkSession Commonly Used Methods:</b></summary>
                <ul>
                    <b>version()</b> – Returns Spark version where your application is running, probably the Spark version you cluster is configured with.
                    <b>createDataFrame()</b> – This creates a DataFrame from a collection and an RDD <br>
                    <b>getActiveSession()</b> – returns an active Spark session. <br>
                    <b>read()</b> – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro and more file formats into DataFrame. <br>
                    <b>readStream()</b> – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame. <br>
                    <b>sparkContext()</b> – Returns a SparkContext. <br>
                    <b>sql()</b> – Returns a DataFrame after executing the SQL mentioned. <br>
                    <b>sqlContext()</b> – Returns SQLContext. <br>
                    <b>stop()</b> – Stop the current SparkContext. <br>
                    <b>table()</b> – Returns a DataFrame of a table or view. <br>
                    <b>udf()</b> – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL. <br>
                </ul>
            </details>
        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Load Data in RDD</b></summary>
            <p>For production applications, we mostly create RDD by using external storage systems like HDFS, S3, HBase e.t.c. To make it simple for this PySpark RDD tutorial we are using files from the local system or loading it from the python list to
                create RDD.</p>

            <details>
                <summary><b>Read data from already loaded data</b></summary>
                <pre class="language-python"><code>#Create RDD from parallelize
import pyspark
rdd = pyspark.SparkContext().parallelize([1, 2, 3, 4, 5, 6, 7])</code></pre>

            </details>
            <details>
                <summary><b>Read text file into RDD</b></summary>
                <pre class="language-python"><code>import pyspark    
rdd = pyspark.SparkContext.getOrCreate().textFile(r"C:\Users\MosaabMuhammed\Desktop\text1.txt.txt")
print(rdd.collect())
# ['one,1', 'two,2', 'three,3']</code></pre>
            </details>

            <details>
                <summary><b>Read text files into RDD of Tuple into RDD</b></summary>
                <p>sparkContext.wholeTextFiles() reads a text file into PairedRDD of type RDD[(String,String)] with the key being the file path and value being contents of the file. This method also takes the path as an argument and optionally takes a number
                    of partitions as the second argument.</p>
                <pre class="language-python"><code>import pyspark    
rdd = pyspark.SparkContext.getOrCreate().wholeTextFiles(r"C:\Users\MosaabMuhammed\Desktop\text1.txt.txt")
print(rdd.collect())
# [('file:/C:/Users/MosaabMuhammed/Desktop/text1.txt.txt', 'one,1\r\ntwo,2\r\nthree,3')]
</code></pre>
            </details>

            <details>
                <summary><b>Reading multiple files at a time into RDD</b></summary>
                <p>When you know the names of the multiple files you would like to read, just input all file names with comma separator and just a folder if you want to read all files from a folder in order to create an RDD and both methods mentioned above
                    supports this.</p>
                <pre class="language-python"><code>import pyspark    
rdd = pyspark.SparkContext.getOrCreate().textFile(r"C:\Users\MosaabMuhammed\Desktop\text1.txt.txt" + r",C:\Users\MosaabMuhammed\Desktop\text2.txt.txt")
print(rdd.collect())
# ['one,1', 'two,2', 'three,3', 'four', 'fix', 'five', 'six']

rdd = pyspark.SparkContext.getOrCreate().wholeTextFiles(r"C:\Users\MosaabMuhammed\Desktop\text1.txt.txt" + r",C:\Users\MosaabMuhammed\Desktop\text2.txt.txt")
print(rdd.collect())
# [('file:/C:/Users/MosaabMuhammed/Desktop/text1.txt.txt',
# 'one,1\r\ntwo,2\r\nthree,3'),
# ('file:/C:/Users/MosaabMuhammed/Desktop/text2.txt.txt',
# 'four\r\nfix\r\nfive\r\nsix')]
</code></pre>
            </details>

            <details>
                <summary><b>Create empty RDD</b></summary>
                <pre class="language-python"><code>import pyspark    
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()
rdd   = spark.sparkContext.emptyRDD
</code></pre>
            </details>
        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Partitioning</b></summary>
            <p>When we use parallelize() or textFile() or wholeTextFiles() methods of SparkContxt to initiate RDD, it automatically splits the data into partitions based on resource availability. when you run it on a laptop it would create partitions as
                the same number of cores available on your system.</p>
            <pre class="language-python"><code># parallelize()
import pyspark
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()
# default value is 1 parition.
rdd   = spark.sparkContext.parallelize([], 10)
print(rdd.getNumPartitions())</code></pre>

            <p>Some times we may need to repartition the RDD, PySpark provides two ways to repartition; first using repartition() method which shuffles data from all nodes also called full shuffle and second coalesce() method which shuffle data from minimum
                nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes. <br> Both of the functions take the number of partitions to repartition rdd as shown below. Note that repartition() method is a very
                expensive operation as it shuffles data from all nodes in a cluster. </p>

            <pre class="language-python"><code>spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()
rdd   = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6], 2)
print(rdd.getNumPartitions())
rdd = rdd.repartition(4)
print(rdd.getNumPartitions())
# 2
# 4</code></pre>

        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Trasnformations</b></summary>
            <p>Transformations on PySpark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return
                new RDD instead of updating the current.</p>

            <details>
                <summary><b>Types of Transformations</b></summary>
                <ul>
                    <li><b>Narrow Transformation:</b> <br>
                        <p> Narrow transformations are the result of map() and filter() functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations.
                        </p>
                        <p>Functions such as map(), mapPartition(), flatMap(), filter(), union() are some examples of narrow transformation.</p>
                    </li>
                    <li><b>Wider Transformation:</b> <br>
                        <p>Wider transformations are the result of groupByKey() and reduceByKey() functions and these compute data that live on many partitions meaning there will be data movements between partitions to execute wider transformations. Since
                            these shuffles the data, they also called shuffle transformations.</p>
                        <p>Functions such as groupByKey(), aggregateByKey(), aggregate(), join(), repartition() are some examples of a wider transformations.</p>
                    </li>
                </ul>
            </details>

            <details>
                <summary><b>flatMap()</b></summary>
                <p>flatMap() transformation flattens the RDD after applying the function and returns a new RDD. On the below example, first, it splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each
                    record.
                </p>
                <pre class="language-python"><code>rdd = spark.sparkContext.textFile(r"C:\Users\MosaabMuhammed\Desktop\test.txt")
print(rdd.collect()[:5])
# ['Project Gutenberg’s', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll', 'This eBook is for the use', 'of anyone anywhere']
rdd2 = rdd.flatMap(lambda x: x.split())
print(rdd2.collect()[:5])
# ['Project', 'Gutenberg’s', 'Alice’s', 'Adventures', 'in']</code></pre>
            </details>


            <details>
                <summary><b>map()</b></summary>
                <p>map() transformation is used the apply any complex operations like adding a column, updating a column e.t.c, the output of map transformations would always have the same number of records as input.</p>
                <pre class="language-python"><code>rdd = spark.sparkContext.textFile(r"C:\Users\MosaabMuhammed\Desktop\test.txt")
print(rdd.collect()[:5])
# ['Project Gutenberg’s', 'Alice’s Adventures in Wonderland', 'by Lewis Carroll', 'This eBook is for the use', 'of anyone anywhere']
rdd3 = rdd.map(lambda x: x.split())
print(rdd3.collect()[:5])
# [['Project', 'Gutenberg’s'], ['Alice’s', 'Adventures', 'in', 'Wonderland'], ['by', 'Lewis', 'Carroll'], ['This', 'eBook', 'is', 'for', 'the', 'use'], ['of', 'anyone', 'anywhere']]
</code></pre>
            </details>

            <details>
                <summary><b>reduceByKey()</b></summary>
                <p>PySpark reduceByKey() transformation is used to merge the values of each key using an associative reduce function on PySpark RDD. <br><br> It is a wider transformation as it shuffles data across multiple partitions and It operates on pair
                    RDD (key/value pair). When reduceByKey() performs, the output will be partitioned by either numPartitions or the default parallelism level. The Default partitioner is hash-partition.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

data = [('Project', 1),
('Gutenberg’s', 1),
('Alice’s', 1),
('Adventures', 1),
('in', 1),
('Wonderland', 1),
('Project', 1),
('Gutenberg’s', 1),
('Adventures', 1),
('in', 1),
('Wonderland', 1),
('Project', 1),
('Gutenberg’s', 1)]

rdd=spark.sparkContext.parallelize(data)

rdd2=rdd.reduceByKey(lambda a,b: a+b)
for element in rdd2.collect():
    print(element)
# ('Project', 3)
# ('Gutenberg’s', 3)
# ('Alice’s', 1)
# ('Adventures', 2)
# ...
</code></pre>
            </details>

            <details>
                <summary><b>sortByKey()</b></summary>
                <p>sortByKey() transformation is used to sort RDD elements on key. In our example, first, we convert RDD[(String,Int]) to RDD[(Int, String]) using map transformation and apply sortByKey which ideally does sort on an integer value. And finally,
                    foreach with println statements returns all words in RDD and their count as key-value pair</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

data = [('Project', 1),
('Gutenberg’s', 1),
('Alice’s', 1),
('Adventures', 1),
('in', 1),
('Wonderland', 1),
('Project', 1),
('Gutenberg’s', 1),
('Adventures', 1),
('in', 1),
('Wonderland', 1),
('Project', 1),
('Gutenberg’s', 1)]

rdd=spark.sparkContext.parallelize(data)

rdd.map(lambda x: (x[1], x[0])).sortByKey(ascending=True).values().collect()
# ['Project',
#  'Gutenberg’s',
#  'Alice’s',
#  'Adventures',
#  'in',
#  'Wonderland',
#  'Project',
#  'Gutenberg’s',
#  'Adventures',
#  'in',
#  'Wonderland',
#  'Project',
#  'Gutenberg’s']
</code></pre>
            </details>

            <details>
                <summary><b>filter()</b></summary>
                <p>filter() transformation is used to filter the records in an RDD. In our example we are filtering all words starts with “a”.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

data = [('Project', 1),
('Gutenberg’s', 1),
('Alice’s', 1),
('Adventures', 1),
('in', 1),
('Wonderland', 1),
('Project', 1),
('Gutenberg’s', 1),
('Adventures', 1),
('in', 1),
('Wonderland', 1),
('Project', 1),
('Gutenberg’s', 1)]

rdd=spark.sparkContext.parallelize(data)

rdd.filter(lambda x: 'a' in x[0]).collect()
# [('Wonderland', 1), ('Wonderland', 1)]
</code></pre>
            </details>

        </details>

        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Actions</b></summary>
            <p>RDD actions are PySpark operations that return the values to the driver program. Any function on RDD that returns other than RDD is considered as an action in PySpark programming.</p>
            <p>Action functions trigger the transformations to execute. As mentioned in RDD Transformations, all transformations are lazy evaluation meaning they do not get executed right away, and action trigger them to execute.</p>

            <details>
                <summary><b>aggregate(zeroValue, seqOp, combOp)</b></summary>
                <p>Aggregate the elements of each partition, and then the results for all the partitions, using a given combine functions “combOp” and a neutral “zero value.” <br> The first function (seqOp) can return a different result type, U, than the
                    type of this RDD. Thus, we need one operation for merging a T into an U and one operation for merging two U
                </p>
                <pre class="language-python"><code>import pyspark
spark = pyspark.sql.SparkSession.builder.appName("hello_world").getOrCreate()
rdd   = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 2, 3, 8])

seqOp = lambda x, y: x + y
combOp = lambda x, y: x + y
print(rdd.aggregate(0, seqOp, combOp))
# 34

seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))
combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
agg2=rdd.aggregate((0, 0), seqOp2, combOp2)
print(agg2)
# (34, 9)</code></pre>
            </details>


            <details>
                <summary><b>treeAggregate(zeroValue, seqOp, combOp, depth=2)</b></summary>
                <p>treeAggregate() – Aggregates the elements of this RDD in a multi-level tree pattern. The output of this function will be similar to the aggregate function.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])
                    
#treeAggregate. This is similar to aggregate
seqOp = (lambda x, y: x + y)
combOp = (lambda x, y: x + y)
agg=listRdd.treeAggregate(0, seqOp, combOp)
print(agg) # output 20</code></pre>
            </details>

            <details>
                <summary><b>fold()</b></summary>
                <p>fold() – Aggregate the elements of each partition, and then the results for all the partitions.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])
                    
#fold
from operator import add
foldRes=listRdd.fold(0, add)
print(foldRes)
# 20
</code></pre>
            </details>

            <details>
                <summary><b>reduce()</b></summary>
                <p>reduce() – Reduces the elements of the dataset using the specified binary operator.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])
                    
#reduce
redRes=listRdd.reduce(add)
print(redRes) # output 20
</code></pre>
            </details>

            <details>
                <summary><b>treeReduce()</b></summary>
                <p>Reduces the elements of this RDD in a multi-level tree pattern.or.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])
                    
#treeReduce. This is similar to reduce
add = lambda x, y: x + y
redRes=listRdd.treeReduce(add)
print(redRes) # output 20
</code></pre>
            </details>

            <details>
                <summary><b>collect()</b></summary>
                <p>Return the complete dataset as an Array.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])
   
#Collect
data = listRdd.collect()
print(data)
</code></pre>
            </details>

            <details>
                <summary><b>count(), countApprox(), countApproxDistinct()</b></summary>
                <p>count() – Return the count of elements in the dataset.</p>
                <p>countApprox() – Return approximate count of elements in the dataset, this method returns incomplete when execution time meets timeout.</p>
                <p>countApproxDistinct() – Return an approximate number of distinct elements in the dataset.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#count, countApprox, countApproxDistinct
print("Count : "+str(listRdd.count()))
#Output: Count : 20
print("countApprox : "+str(listRdd.countApprox(1200)))
#Output: countApprox : (final: [7.000, 7.000])
print("countApproxDistinct : "+str(listRdd.countApproxDistinct()))
#Output: countApproxDistinct : 5
print("countApproxDistinct : "+str(inputRDD.countApproxDistinct()))
#Output: countApproxDistinct : 5
</code></pre>
            </details>

            <details>
                <summary><b>countByValue(), countByValueApprox()</b></summary>
                <p>Return Map[T,Long] key representing each unique value in dataset and value represents count each value present.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#countByValue, countByValueApprox
print("countByValue :  "+str(listRdd.countByValue()))
# countByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})
</code></pre>
            </details>

            <details>
                <summary><b>first()</b></summary>
                <p>Return the first element in the dataset.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#first
print("first :  "+str(listRdd.first()))
#Output: first :  1
print("first :  "+str(inputRDD.first()))
#Output: first :  (Z,1)
</code></pre>
            </details>

            <details>
                <summary><b>top()</b></summary>
                <p>Return top n elements from the dataset sorted ascendingly. <br> Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#top
print("top : "+str(listRdd.top(2)))
#Output: take : 5,4
print("top : "+str(inputRDD.top(2)))
#Output: take : (Z,1),(C,40)
</code></pre>
            </details>

            <details>
                <summary><b>min()</b></summary>
                <p>min() – Return the minimum value from the dataset.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#min
print("min :  "+str(listRdd.min()))
#Output: min :  1
print("min :  "+str(inputRDD.min()))
#Output: min :  (A,20)  
</code></pre>
            </details>

            <details>
                <summary><b>max()</b></summary>
                <p>max() – Return the maximum value from the dataset.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#max
print("max :  "+str(listRdd.max()))
#Output: max :  5
print("max :  "+str(inputRDD.max()))
#Output: max :  (Z,1)
</code></pre>
            </details>

            <details>
                <summary><b>take(), takeOrdered(), takeSample()</b></summary>
                <p>take() – Return the first num elements of the dataset.</p>
                <p>takeOrdered() – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action. Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory.</p>
                <p>takeSample() – Return the subset of the dataset in an Array. Note: Use this method only when the resulting array is small, as all the data is loaded into the driver’s memory.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

#take, takeOrdered, takeSample
print("take : "+str(listRdd.take(2)))
#Output: take : 1,2
print("takeOrdered : "+ str(listRdd.takeOrdered(2)))
#Output: takeOrdered : 1,2
print("take : "+str(listRdd.takeSample()))
</code></pre>
            </details>


            <details>
                <summary><b>saveAsTextFile()</b></summary>
                <p> Using saveAsTestFile action, we can write the RDD to a text file.</p>
                <pre class="language-python"><code>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()
data=[("Z", 1),("A", 20),("B", 30),("C", 40),("B", 30),("B", 60)]
inputRDD = spark.sparkContext.parallelize(data)
listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])

rdd6.saveAsTextFile("/tmp/wordCount")
</code></pre>
            </details>
        </details>

        <details>
            <summary><b>Types of RDD</b></summary>
            <ul>
                <li>PairRDDFunctions or PairRDD – Pair RDD is a key-value pair This is mostly used RDD type</li>
                <li>ShuffledRDD</li>
                <li>DoubleRDD</li>
                <li>SequenceFileRDD</li>
                <li>HadoopRDD</li>
                <li>ParallelCollectionRDD</li>
            </ul>
        </details>


        <details>
            <summary><b>Persistence</b></summary>
            <p>Though PySpark provides computation 100 x times faster than traditional Map Reduce jobs, If you have not designed the jobs to reuse the repeating computations you will see degrade in performance when you are dealing with billions or trillions
                of data. Hence, we need to look at the computations and use optimization techniques as one of the ways to improve performance. <br> Using cache() and persist() methods, PySpark provides an optimization mechanism to store the intermediate
                computation of an RDD so they can be reused in subsequent actions.</p>
            <details>
                <summary><b>Functions</b></summary>
                <ul>
                    <li>
                        <details>
                            <summary><b>cache()</b></summary>
                            <p>PySpark RDD cache() method by default saves RDD computation to storage level `MEMORY_ONLY` meaning it will store the data in the JVM heap as unserialized objects. <br> PySpark cache() method in RDD class internally calls persist()
                                method which in turn uses sparkSession.sharedState.cacheManager.cacheQuery to cache the result set of RDD. Let’s look at an example.</p>
                            <pre class="language-python"><code>cachedRdd = rdd.cache()</code></pre>
                        </details>
                    </li>
                    <li>
                        <details>
                            <summary><b>persist()</b></summary>
                            <p>PySpark persist() method is used to store the RDD to one of the storage levels MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2 and more. <br> PySpark persist has
                                two signature first signature doesn’t take any argument which by default saves it to <strong>MEMORY_ONLY</strong> storage level and the second signature which takes StorageLevel as an argument to store it to different storage
                                levels.
                            </p>
                            <pre class="language-python"><code>import pyspark
dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)
dfPersist.show(false)
                            </code></pre>
                        </details>
                    </li>
                    <li>
                        <details>
                            <summary><b>unpresist()</b></summary>
                            <p>PySpark automatically monitors every persist() and cache() calls you make and it checks usage on each node and drops persisted data if not used or by using least-recently-used (LRU) algorithm. You can also manually remove using
                                unpersist() method. unpersist() marks the RDD as non-persistent, and remove all blocks for it from memory and disk.</p>
                            <pre class="language-python"><code>rddPersist2 = rddPersist.unpersist()</code></pre>
                        </details>
                    </li>

                </ul>
            </details>
        </details>


        <details>
            <summary><b>Shared Variables</b></summary>
            <p>When PySpark executes transformation using map() or reduce() operations, It executes the transformations on a remote node by using the variables that are shipped with the tasks and these variables are not sent back to PySpark Driver hence
                there is no capability to reuse and sharing the variables across tasks. PySpark shared variables solve this problem using the below two techniques.</p>
            <details>
                <summary><b>Types</b></summary>
                <ul>
                    <li>
                        <details>
                            <summary><b>Broadcast read-only Variables</b></summary>
                            <p>In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every
                                task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.</p>
                            <pre class="language-python"><code># Let me explain with an example when to use broadcast variables, assume you are getting a two-letter country state code in a file and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code e.t.c).
# Instead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache this lookup info on each machine and tasks use this cached info while executing the transformations.


import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

states = {"NY":"New York", "CA":"California", "FL":"Florida"}
broadcastStates = spark.sparkContext.broadcast(states)

data = [("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  ]

rdd = spark.sparkContext.parallelize(data)

def state_convert(code):
    return broadcastStates.value[code]

result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()
print(result)
</code></pre>
                        </details>
                    </li>
                    <li>
                        <details>
                            <summary><b>Accumulator variable</b></summary>
                            <p>Spark Accumulators are shared variables which are only “added” through an associative and commutative operation and are used to perform counters (Similar to Map-reduce counters) or sum operations</p>
                            <p>Spark by default supports to create an accumulators of any numeric type and provide a capability to add custom accumulator types. </p>
                            <pre class="language-python"><code>accum = sc.longAccumulator("SumAccumulator")
sc.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))</code></pre>
                        </details>
                    </li>
                </ul>
            </details>
        </details>

        <details>
            <summary><b>Advanced APIs</b></summary>
            <details>
                <summary><b>Creating RDD from DataFrame and vice-versa</b></summary>
                <pre class="language-python"><code># Converts RDD to DataFrame
dfFromRDD1 = rdd.toDF()
# Converts RDD to DataFrame with column names
dfFromRDD2 = rdd.toDF("col1","col2")
# using createDataFrame() - Convert DataFrame to RDD
df = spark.createDataFrame(rdd).toDF("col1","col2")
# Convert DataFrame to RDD
rdd = df.rdd
</code></pre>
            </details>

        </details>
    </div>
    <script src="../prism.js"></script>
</body>

</html>