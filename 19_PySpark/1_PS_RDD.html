<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8" />
    <title>
        Time Series - Data Wrangling
    </title>
    <link rel="stylesheet" href="../prism.css">
</head>

<body>
    <h1 id="3-data-wrangling">
        3. TS - Data Wrangling
    </h1>
    <div style="width:1000px;margin:auto">
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary>Create a <b>SparkSession</b></summary>
            <p>SparkSession is an entry point to PySpark and creating a SparkSession instance would be the first statement you would write to program with RDD, DataFrame, and Dataset.</p>
            <pre class="language-python"><code>import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()</code></pre>
            <p>
                master() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either yarn or mesos depends on your cluster setup. <br> Use local[x] when running in Standalone mode. x
                should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have. <br> appName() – Used to
                set your application name. <br> getOrCreate() – This returns a SparkSession object if already exists, creates new one if not exists. <br>
            </p>

            <details>
                <summary><b>SparkSession Commonly Used Methods:</b></summary>
                <ul>
                    <b>version()</b> – Returns Spark version where your application is running, probably the Spark version you cluster is configured with.

                    <b>createDataFrame()</b> – This creates a DataFrame from a collection and an RDD <br>

                    <b>getActiveSession()</b> – returns an active Spark session. <br>

                    <b>read()</b> – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro and more file formats into DataFrame. <br>

                    <b>readStream()</b> – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame. <br>

                    <b>sparkContext()</b> – Returns a SparkContext. <br>

                    <b>sql()</b> – Returns a DataFrame after executing the SQL mentioned. <br>

                    <b>sqlContext()</b> – Returns SQLContext. <br>

                    <b>stop()</b> – Stop the current SparkContext. <br>

                    <b>table()</b> – Returns a DataFrame of a table or view. <br>

                    <b>udf()</b> – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL. <br>
                </ul>
            </details>
        </details>
        <!-- ---------------------------------------------------------------------------- -->
        <details>
            <summary><b>Load Data in RDD</b></summary>
            <p>For production applications, we mostly create RDD by using external storage systems like HDFS, S3, HBase e.t.c. To make it simple for this PySpark RDD tutorial we are using files from the local system or loading it from the python list to
                create RDD.</p>

            <details>
                <summary><b>Read data from already loaded data</b></summary>
                <pre class="language-python"><code>#Create RDD from parallelize
import pyspark
rdd = pyspark.SparkContext().parallelize([1, 2, 3, 4, 5, 6, 7])</code></pre>

            </details>
            <details>
                <summary><b>Read text file into RDD</b></summary>
                <pre class="language-python"><code>import pyspark    
rdd = pyspark.SparkContext().textFile("/path/file.txt")</code></pre>
            </details>

            <details>
                <summary><b>Read text files into RDD of Tuple into RDD</b></summary>
                <p>sparkContext.wholeTextFiles() reads a text file into PairedRDD of type RDD[(String,String)] with the key being the file path and value being contents of the file. This method also takes the path as an argument and optionally takes a number
                    of partitions as the second argument.</p>
                <pre class="language-python"><code>import pyspark    
rdd = pyspark.SparkContext().wholeTextFiles("/path/textFile.txt")</code></pre>
            </details>

            <details>
                <summary><b>Reading multiple files at a time into RDD</b></summary>
                <p>When you know the names of the multiple files you would like to read, just input all file names with comma separator and just a folder if you want to read all files from a folder in order to create an RDD and both methods mentioned above
                    supports this.</p>
                <pre class="language-python"><code>import pyspark    
rdd = pyspark.SparkContext().textFile("src/main/resources/csv/text01.txt," + "src/main/resources/csv/text02.txt")</code></pre>
            </details>
        </details>
    </div>
    <script src="../prism.js"></script>
</body>

</html>